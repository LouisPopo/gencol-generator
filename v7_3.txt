cuda is available
USING : cuda
Done loading data from cached files.
Training ...
     Batch 000 | Loss : 0.7032 | Acc : 0.4861
     Batch 005 | Loss : 0.7009 | Acc : 0.4869
     Batch 010 | Loss : 0.6983 | Acc : 0.4871
     Batch 015 | Loss : 0.6933 | Acc : 0.4885
     Batch 020 | Loss : 0.6933 | Acc : 0.5117
     Batch 025 | Loss : 0.6931 | Acc : 0.5830
     Batch 030 | Loss : 0.6928 | Acc : 0.5114
     Batch 035 | Loss : 0.6926 | Acc : 0.5122
     Batch 040 | Loss : 0.6916 | Acc : 0.5159
     Batch 045 | Loss : 0.6900 | Acc : 0.5130
     Batch 050 | Loss : 0.6773 | Acc : 0.5320
     Batch 055 | Loss : 0.6301 | Acc : 0.6801
     Batch 060 | Loss : 0.6011 | Acc : 0.7392
     Batch 065 | Loss : 0.5601 | Acc : 0.7571
     Batch 070 | Loss : 0.5851 | Acc : 0.7292
     Batch 075 | Loss : 0.5331 | Acc : 0.7821
     Batch 080 | Loss : 0.5634 | Acc : 0.7530
     Batch 085 | Loss : 0.5781 | Acc : 0.7180
     Batch 090 | Loss : 0.5987 | Acc : 0.7186
     Batch 095 | Loss : 0.4721 | Acc : 0.7761
     Batch 100 | Loss : 0.4444 | Acc : 0.7856
     Batch 105 | Loss : 0.5124 | Acc : 0.7360
     Batch 110 | Loss : 0.4917 | Acc : 0.7537
     Batch 115 | Loss : 0.4600 | Acc : 0.7698
     Batch 120 | Loss : 0.4761 | Acc : 0.7638
     Batch 125 | Loss : 0.4760 | Acc : 0.7679
     Batch 130 | Loss : 0.4889 | Acc : 0.7585
     Batch 135 | Loss : 0.4813 | Acc : 0.7569
     Batch 140 | Loss : 0.4684 | Acc : 0.7715
     Batch 145 | Loss : 0.4206 | Acc : 0.8018
     Batch 150 | Loss : 0.4407 | Acc : 0.7832
Epoch 00000 | Train Loss : 0.5676 | Eval Loss : 0.4320 | Train acc : 0.6762 | Eval Acc : 0.7906 | Eval Log. Respected : 0.9826
     Batch 000 | Loss : 0.4148 | Acc : 0.8082
     Batch 005 | Loss : 0.4134 | Acc : 0.8070
     Batch 010 | Loss : 0.3994 | Acc : 0.8159
     Batch 015 | Loss : 0.4217 | Acc : 0.7949
     Batch 020 | Loss : 0.3461 | Acc : 0.8466
     Batch 025 | Loss : 0.4188 | Acc : 0.7972
     Batch 030 | Loss : 0.4590 | Acc : 0.7775
     Batch 035 | Loss : 0.4581 | Acc : 0.7782
     Batch 040 | Loss : 0.3765 | Acc : 0.8254
     Batch 045 | Loss : 0.4244 | Acc : 0.8040
     Batch 050 | Loss : 0.3959 | Acc : 0.8099
     Batch 055 | Loss : 0.3901 | Acc : 0.8250
     Batch 060 | Loss : 0.4355 | Acc : 0.7864
     Batch 065 | Loss : 0.4026 | Acc : 0.8090
     Batch 070 | Loss : 0.3742 | Acc : 0.8305
     Batch 075 | Loss : 0.3543 | Acc : 0.8424
     Batch 080 | Loss : 0.4261 | Acc : 0.7966
     Batch 085 | Loss : 0.3991 | Acc : 0.8115
     Batch 090 | Loss : 0.3864 | Acc : 0.8203
     Batch 095 | Loss : 0.4878 | Acc : 0.7748
     Batch 100 | Loss : 0.4378 | Acc : 0.7871
     Batch 105 | Loss : 0.3517 | Acc : 0.8477
     Batch 110 | Loss : 0.3848 | Acc : 0.8233
     Batch 115 | Loss : 0.4175 | Acc : 0.7996
     Batch 120 | Loss : 0.4157 | Acc : 0.8003
     Batch 125 | Loss : 0.4372 | Acc : 0.7862
     Batch 130 | Loss : 0.4708 | Acc : 0.7788
     Batch 135 | Loss : 0.4333 | Acc : 0.7896
     Batch 140 | Loss : 0.4047 | Acc : 0.8073
     Batch 145 | Loss : 0.3872 | Acc : 0.8184
     Batch 150 | Loss : 0.3484 | Acc : 0.8418
Epoch 00001 | Train Loss : 0.4065 | Eval Loss : 0.3897 | Train acc : 0.8090 | Eval Acc : 0.8147 | Eval Log. Respected : 0.9417
     Batch 000 | Loss : 0.3991 | Acc : 0.8168
     Batch 005 | Loss : 0.4001 | Acc : 0.8085
     Batch 010 | Loss : 0.3316 | Acc : 0.8529
     Batch 015 | Loss : 0.4415 | Acc : 0.7898
     Batch 020 | Loss : 0.4373 | Acc : 0.7873
     Batch 025 | Loss : 0.3843 | Acc : 0.8205
     Batch 030 | Loss : 0.3998 | Acc : 0.8076
     Batch 035 | Loss : 0.4012 | Acc : 0.8058
     Batch 040 | Loss : 0.3562 | Acc : 0.8361
     Batch 045 | Loss : 0.3884 | Acc : 0.8189
     Batch 050 | Loss : 0.3830 | Acc : 0.8215
     Batch 055 | Loss : 0.3481 | Acc : 0.8433
     Batch 060 | Loss : 0.3881 | Acc : 0.8218
     Batch 065 | Loss : 0.4092 | Acc : 0.8010
     Batch 070 | Loss : 0.4170 | Acc : 0.7984
     Batch 075 | Loss : 0.3716 | Acc : 0.8203
     Batch 080 | Loss : 0.4372 | Acc : 0.7918
     Batch 085 | Loss : 0.3965 | Acc : 0.8104
     Batch 090 | Loss : 0.3617 | Acc : 0.8281
     Batch 095 | Loss : 0.4139 | Acc : 0.8007
     Batch 100 | Loss : 0.3551 | Acc : 0.8332
     Batch 105 | Loss : 0.3523 | Acc : 0.8369
     Batch 110 | Loss : 0.4010 | Acc : 0.8073
     Batch 115 | Loss : 0.4441 | Acc : 0.7815
     Batch 120 | Loss : 0.3383 | Acc : 0.8473
     Batch 125 | Loss : 0.3754 | Acc : 0.8230
     Batch 130 | Loss : 0.3796 | Acc : 0.8273
     Batch 135 | Loss : 0.3210 | Acc : 0.8534
     Batch 140 | Loss : 0.3530 | Acc : 0.8369
     Batch 145 | Loss : 0.4247 | Acc : 0.7942
     Batch 150 | Loss : 0.3502 | Acc : 0.8453
Epoch 00002 | Train Loss : 0.3896 | Eval Loss : 0.3840 | Train acc : 0.8174 | Eval Acc : 0.8169 | Eval Log. Respected : 0.9564
     Batch 000 | Loss : 0.3736 | Acc : 0.8242
     Batch 005 | Loss : 0.3481 | Acc : 0.8362
     Batch 010 | Loss : 0.4089 | Acc : 0.8025
     Batch 015 | Loss : 0.4112 | Acc : 0.7992
     Batch 020 | Loss : 0.3753 | Acc : 0.8221
     Batch 025 | Loss : 0.3471 | Acc : 0.8384
     Batch 030 | Loss : 0.3175 | Acc : 0.8681
     Batch 035 | Loss : 0.3584 | Acc : 0.8388
     Batch 040 | Loss : 0.3673 | Acc : 0.8312
     Batch 045 | Loss : 0.3917 | Acc : 0.8171
     Batch 050 | Loss : 0.3301 | Acc : 0.8464
     Batch 055 | Loss : 0.3889 | Acc : 0.8121
     Batch 060 | Loss : 0.3939 | Acc : 0.8131
     Batch 065 | Loss : 0.3686 | Acc : 0.8269
     Batch 070 | Loss : 0.3954 | Acc : 0.8108
     Batch 075 | Loss : 0.3502 | Acc : 0.8357
     Batch 080 | Loss : 0.3619 | Acc : 0.8372
     Batch 085 | Loss : 0.3547 | Acc : 0.8371
     Batch 090 | Loss : 0.4369 | Acc : 0.7836
     Batch 095 | Loss : 0.3527 | Acc : 0.8436
     Batch 100 | Loss : 0.3738 | Acc : 0.8256
     Batch 105 | Loss : 0.3719 | Acc : 0.8318
     Batch 110 | Loss : 0.3018 | Acc : 0.8707
     Batch 115 | Loss : 0.3953 | Acc : 0.8124
     Batch 120 | Loss : 0.3931 | Acc : 0.8175
     Batch 125 | Loss : 0.3317 | Acc : 0.8511
     Batch 130 | Loss : 0.3716 | Acc : 0.8232
     Batch 135 | Loss : 0.4280 | Acc : 0.8073
     Batch 140 | Loss : 0.3353 | Acc : 0.8476
     Batch 145 | Loss : 0.4018 | Acc : 0.8107
     Batch 150 | Loss : 0.3517 | Acc : 0.8380
Epoch 00003 | Train Loss : 0.3804 | Eval Loss : 0.3758 | Train acc : 0.8221 | Eval Acc : 0.8200 | Eval Log. Respected : 0.9703
     Batch 000 | Loss : 0.4547 | Acc : 0.7761
     Batch 005 | Loss : 0.4585 | Acc : 0.7819
     Batch 010 | Loss : 0.3566 | Acc : 0.8348
     Batch 015 | Loss : 0.3331 | Acc : 0.8502
     Batch 020 | Loss : 0.3463 | Acc : 0.8414
     Batch 025 | Loss : 0.3990 | Acc : 0.8055
     Batch 030 | Loss : 0.3359 | Acc : 0.8427
     Batch 035 | Loss : 0.3304 | Acc : 0.8488
     Batch 040 | Loss : 0.3346 | Acc : 0.8524
     Batch 045 | Loss : 0.3607 | Acc : 0.8307
     Batch 050 | Loss : 0.3786 | Acc : 0.8290
     Batch 055 | Loss : 0.4472 | Acc : 0.7900
     Batch 060 | Loss : 0.4052 | Acc : 0.8024
     Batch 065 | Loss : 0.3598 | Acc : 0.8264
     Batch 070 | Loss : 0.3744 | Acc : 0.8206
     Batch 075 | Loss : 0.3277 | Acc : 0.8502
     Batch 080 | Loss : 0.3994 | Acc : 0.8047
     Batch 085 | Loss : 0.3634 | Acc : 0.8287
     Batch 090 | Loss : 0.3269 | Acc : 0.8473
     Batch 095 | Loss : 0.3619 | Acc : 0.8352
     Batch 100 | Loss : 0.3748 | Acc : 0.8284
     Batch 105 | Loss : 0.3691 | Acc : 0.8278
     Batch 110 | Loss : 0.3630 | Acc : 0.8278
     Batch 115 | Loss : 0.3689 | Acc : 0.8271
     Batch 120 | Loss : 0.3305 | Acc : 0.8473
     Batch 125 | Loss : 0.3714 | Acc : 0.8231
     Batch 130 | Loss : 0.3939 | Acc : 0.8124
     Batch 135 | Loss : 0.3572 | Acc : 0.8336
     Batch 140 | Loss : 0.4025 | Acc : 0.8009
     Batch 145 | Loss : 0.3649 | Acc : 0.8276
     Batch 150 | Loss : 0.4334 | Acc : 0.7970
Epoch 00004 | Train Loss : 0.3697 | Eval Loss : 0.3648 | Train acc : 0.8266 | Eval Acc : 0.8256 | Eval Log. Respected : 0.9502
     Batch 000 | Loss : 0.3063 | Acc : 0.8722
     Batch 005 | Loss : 0.3613 | Acc : 0.8286
     Batch 010 | Loss : 0.4215 | Acc : 0.8024
     Batch 015 | Loss : 0.3862 | Acc : 0.8149
     Batch 020 | Loss : 0.4448 | Acc : 0.7978
     Batch 025 | Loss : 0.3306 | Acc : 0.8459
     Batch 030 | Loss : 0.3951 | Acc : 0.8138
     Batch 035 | Loss : 0.3628 | Acc : 0.8244
     Batch 040 | Loss : 0.4509 | Acc : 0.7918
     Batch 045 | Loss : 0.4275 | Acc : 0.7971
     Batch 050 | Loss : 0.3924 | Acc : 0.8108
     Batch 055 | Loss : 0.3997 | Acc : 0.8086
     Batch 060 | Loss : 0.3453 | Acc : 0.8408
     Batch 065 | Loss : 0.3821 | Acc : 0.8171
     Batch 070 | Loss : 0.3764 | Acc : 0.8212
     Batch 075 | Loss : 0.3902 | Acc : 0.8148
     Batch 080 | Loss : 0.3657 | Acc : 0.8292
     Batch 085 | Loss : 0.3654 | Acc : 0.8232
     Batch 090 | Loss : 0.4158 | Acc : 0.8020
     Batch 095 | Loss : 0.3425 | Acc : 0.8447
     Batch 100 | Loss : 0.3517 | Acc : 0.8330
     Batch 105 | Loss : 0.3304 | Acc : 0.8451
     Batch 110 | Loss : 0.3851 | Acc : 0.8222
     Batch 115 | Loss : 0.3209 | Acc : 0.8518
     Batch 120 | Loss : 0.3356 | Acc : 0.8442
     Batch 125 | Loss : 0.3777 | Acc : 0.8272
     Batch 130 | Loss : 0.3935 | Acc : 0.8114
     Batch 135 | Loss : 0.4458 | Acc : 0.7879
     Batch 140 | Loss : 0.3429 | Acc : 0.8452
     Batch 145 | Loss : 0.3521 | Acc : 0.8342
     Batch 150 | Loss : 0.3548 | Acc : 0.8363
Epoch 00005 | Train Loss : 0.3656 | Eval Loss : 0.3601 | Train acc : 0.8291 | Eval Acc : 0.8265 | Eval Log. Respected : 0.9282
     Batch 000 | Loss : 0.3384 | Acc : 0.8466
     Batch 005 | Loss : 0.3925 | Acc : 0.8129
     Batch 010 | Loss : 0.3540 | Acc : 0.8329
     Batch 015 | Loss : 0.3586 | Acc : 0.8297
     Batch 020 | Loss : 0.3537 | Acc : 0.8346
     Batch 025 | Loss : 0.3573 | Acc : 0.8312
     Batch 030 | Loss : 0.3327 | Acc : 0.8474
     Batch 035 | Loss : 0.3684 | Acc : 0.8251
     Batch 040 | Loss : 0.3448 | Acc : 0.8423
     Batch 045 | Loss : 0.3451 | Acc : 0.8391
     Batch 050 | Loss : 0.3833 | Acc : 0.8156
     Batch 055 | Loss : 0.3360 | Acc : 0.8485
     Batch 060 | Loss : 0.3868 | Acc : 0.8117
     Batch 065 | Loss : 0.4631 | Acc : 0.7842
     Batch 070 | Loss : 0.3483 | Acc : 0.8354
     Batch 075 | Loss : 0.3098 | Acc : 0.8638
     Batch 080 | Loss : 0.3649 | Acc : 0.8276
     Batch 085 | Loss : 0.3549 | Acc : 0.8351
     Batch 090 | Loss : 0.4016 | Acc : 0.8079
     Batch 095 | Loss : 0.4924 | Acc : 0.7739
     Batch 100 | Loss : 0.3594 | Acc : 0.8354
     Batch 105 | Loss : 0.3862 | Acc : 0.8223
     Batch 110 | Loss : 0.3252 | Acc : 0.8551
     Batch 115 | Loss : 0.2680 | Acc : 0.8820
     Batch 120 | Loss : 0.3606 | Acc : 0.8356
     Batch 125 | Loss : 0.3790 | Acc : 0.8273
     Batch 130 | Loss : 0.3888 | Acc : 0.8174
     Batch 135 | Loss : 0.2903 | Acc : 0.8677
     Batch 140 | Loss : 0.3777 | Acc : 0.8148
     Batch 145 | Loss : 0.3364 | Acc : 0.8491
     Batch 150 | Loss : 0.3764 | Acc : 0.8192
Epoch 00006 | Train Loss : 0.3600 | Eval Loss : 0.3669 | Train acc : 0.8324 | Eval Acc : 0.8254 | Eval Log. Respected : 0.9684
     Batch 000 | Loss : 0.3592 | Acc : 0.8331
     Batch 005 | Loss : 0.3498 | Acc : 0.8414
     Batch 010 | Loss : 0.3586 | Acc : 0.8304
     Batch 015 | Loss : 0.3292 | Acc : 0.8484
     Batch 020 | Loss : 0.3204 | Acc : 0.8507
     Batch 025 | Loss : 0.4042 | Acc : 0.8097
     Batch 030 | Loss : 0.3363 | Acc : 0.8503
     Batch 035 | Loss : 0.3639 | Acc : 0.8288
     Batch 040 | Loss : 0.3727 | Acc : 0.8243
     Batch 045 | Loss : 0.3713 | Acc : 0.8293
     Batch 050 | Loss : 0.3958 | Acc : 0.8147
     Batch 055 | Loss : 0.3659 | Acc : 0.8273
     Batch 060 | Loss : 0.3362 | Acc : 0.8454
     Batch 065 | Loss : 0.3783 | Acc : 0.8223
     Batch 070 | Loss : 0.3148 | Acc : 0.8556
     Batch 075 | Loss : 0.3603 | Acc : 0.8321
     Batch 080 | Loss : 0.3466 | Acc : 0.8360
     Batch 085 | Loss : 0.4132 | Acc : 0.8106
     Batch 090 | Loss : 0.3646 | Acc : 0.8277
     Batch 095 | Loss : 0.3326 | Acc : 0.8495
     Batch 100 | Loss : 0.3810 | Acc : 0.8181
     Batch 105 | Loss : 0.3042 | Acc : 0.8657
     Batch 110 | Loss : 0.3170 | Acc : 0.8537
     Batch 115 | Loss : 0.3885 | Acc : 0.8167
     Batch 120 | Loss : 0.3440 | Acc : 0.8398
     Batch 125 | Loss : 0.3484 | Acc : 0.8365
     Batch 130 | Loss : 0.3540 | Acc : 0.8335
     Batch 135 | Loss : 0.3238 | Acc : 0.8518
     Batch 140 | Loss : 0.3064 | Acc : 0.8658
     Batch 145 | Loss : 0.3645 | Acc : 0.8256
     Batch 150 | Loss : 0.3740 | Acc : 0.8245
Epoch 00007 | Train Loss : 0.3531 | Eval Loss : 0.3555 | Train acc : 0.8357 | Eval Acc : 0.8299 | Eval Log. Respected : 0.9189
     Batch 000 | Loss : 0.3627 | Acc : 0.8265
     Batch 005 | Loss : 0.3497 | Acc : 0.8313
     Batch 010 | Loss : 0.3178 | Acc : 0.8525
     Batch 015 | Loss : 0.3502 | Acc : 0.8336
     Batch 020 | Loss : 0.3210 | Acc : 0.8525
     Batch 025 | Loss : 0.3562 | Acc : 0.8344
     Batch 030 | Loss : 0.3428 | Acc : 0.8394
     Batch 035 | Loss : 0.3406 | Acc : 0.8435
     Batch 040 | Loss : 0.4156 | Acc : 0.7940
     Batch 045 | Loss : 0.3994 | Acc : 0.8037
     Batch 050 | Loss : 0.3176 | Acc : 0.8527
     Batch 055 | Loss : 0.3144 | Acc : 0.8555
     Batch 060 | Loss : 0.3546 | Acc : 0.8324
     Batch 065 | Loss : 0.3642 | Acc : 0.8273
     Batch 070 | Loss : 0.4840 | Acc : 0.7847
     Batch 075 | Loss : 0.3498 | Acc : 0.8373
     Batch 080 | Loss : 0.4041 | Acc : 0.8122
     Batch 085 | Loss : 0.3863 | Acc : 0.8229
     Batch 090 | Loss : 0.3614 | Acc : 0.8304
     Batch 095 | Loss : 0.3912 | Acc : 0.8192
     Batch 100 | Loss : 0.3145 | Acc : 0.8574
     Batch 105 | Loss : 0.3553 | Acc : 0.8312
     Batch 110 | Loss : 0.3200 | Acc : 0.8480
     Batch 115 | Loss : 0.3164 | Acc : 0.8540
     Batch 120 | Loss : 0.4433 | Acc : 0.7919
     Batch 125 | Loss : 0.3009 | Acc : 0.8645
     Batch 130 | Loss : 0.3069 | Acc : 0.8607
     Batch 135 | Loss : 0.3706 | Acc : 0.8226
     Batch 140 | Loss : 0.3756 | Acc : 0.8231
     Batch 145 | Loss : 0.3436 | Acc : 0.8430
     Batch 150 | Loss : 0.3197 | Acc : 0.8569
Epoch 00008 | Train Loss : 0.3502 | Eval Loss : 0.3510 | Train acc : 0.8371 | Eval Acc : 0.8333 | Eval Log. Respected : 0.9423
     Batch 000 | Loss : 0.3809 | Acc : 0.8149
     Batch 005 | Loss : 0.2985 | Acc : 0.8668
     Batch 010 | Loss : 0.3398 | Acc : 0.8446
     Batch 015 | Loss : 0.3518 | Acc : 0.8382
     Batch 020 | Loss : 0.3746 | Acc : 0.8273
     Batch 025 | Loss : 0.4208 | Acc : 0.8032
     Batch 030 | Loss : 0.3353 | Acc : 0.8440
     Batch 035 | Loss : 0.3200 | Acc : 0.8558
     Batch 040 | Loss : 0.3123 | Acc : 0.8550
     Batch 045 | Loss : 0.3420 | Acc : 0.8444
     Batch 050 | Loss : 0.2919 | Acc : 0.8705
     Batch 055 | Loss : 0.3133 | Acc : 0.8596
     Batch 060 | Loss : 0.3582 | Acc : 0.8332
     Batch 065 | Loss : 0.3326 | Acc : 0.8455
     Batch 070 | Loss : 0.3096 | Acc : 0.8544
     Batch 075 | Loss : 0.3836 | Acc : 0.8217
     Batch 080 | Loss : 0.3065 | Acc : 0.8631
     Batch 085 | Loss : 0.3138 | Acc : 0.8610
     Batch 090 | Loss : 0.3375 | Acc : 0.8459
     Batch 095 | Loss : 0.5115 | Acc : 0.7778
     Batch 100 | Loss : 0.3779 | Acc : 0.8183
     Batch 105 | Loss : 0.3452 | Acc : 0.8415
     Batch 110 | Loss : 0.3554 | Acc : 0.8306
     Batch 115 | Loss : 0.3767 | Acc : 0.8166
     Batch 120 | Loss : 0.2960 | Acc : 0.8694
     Batch 125 | Loss : 0.3679 | Acc : 0.8259
     Batch 130 | Loss : 0.3434 | Acc : 0.8380
     Batch 135 | Loss : 0.3745 | Acc : 0.8249
     Batch 140 | Loss : 0.4000 | Acc : 0.8157
     Batch 145 | Loss : 0.3346 | Acc : 0.8460
     Batch 150 | Loss : 0.3868 | Acc : 0.8157
Epoch 00009 | Train Loss : 0.3468 | Eval Loss : 0.3487 | Train acc : 0.8401 | Eval Acc : 0.8368 | Eval Log. Respected : 0.9435
     Batch 000 | Loss : 0.3706 | Acc : 0.8228
     Batch 005 | Loss : 0.3386 | Acc : 0.8461
     Batch 010 | Loss : 0.3458 | Acc : 0.8380
     Batch 015 | Loss : 0.3798 | Acc : 0.8206
     Batch 020 | Loss : 0.2978 | Acc : 0.8641
     Batch 025 | Loss : 0.3963 | Acc : 0.8220
     Batch 030 | Loss : 0.3148 | Acc : 0.8598
     Batch 035 | Loss : 0.3934 | Acc : 0.8154
     Batch 040 | Loss : 0.3365 | Acc : 0.8379
     Batch 045 | Loss : 0.4070 | Acc : 0.8074
     Batch 050 | Loss : 0.3464 | Acc : 0.8423
     Batch 055 | Loss : 0.3401 | Acc : 0.8403
     Batch 060 | Loss : 0.3300 | Acc : 0.8469
     Batch 065 | Loss : 0.3485 | Acc : 0.8366
     Batch 070 | Loss : 0.3560 | Acc : 0.8314
     Batch 075 | Loss : 0.4239 | Acc : 0.7949
     Batch 080 | Loss : 0.3283 | Acc : 0.8484
     Batch 085 | Loss : 0.3451 | Acc : 0.8398
     Batch 090 | Loss : 0.3733 | Acc : 0.8242
     Batch 095 | Loss : 0.3402 | Acc : 0.8416
     Batch 100 | Loss : 0.2950 | Acc : 0.8650
     Batch 105 | Loss : 0.4023 | Acc : 0.8083
     Batch 110 | Loss : 0.2875 | Acc : 0.8735
     Batch 115 | Loss : 0.4157 | Acc : 0.7967
     Batch 120 | Loss : 0.3413 | Acc : 0.8397
     Batch 125 | Loss : 0.3316 | Acc : 0.8479
     Batch 130 | Loss : 0.3151 | Acc : 0.8532
     Batch 135 | Loss : 0.4029 | Acc : 0.8084
     Batch 140 | Loss : 0.3459 | Acc : 0.8421
     Batch 145 | Loss : 0.3558 | Acc : 0.8322
     Batch 150 | Loss : 0.3327 | Acc : 0.8469
Epoch 00010 | Train Loss : 0.3403 | Eval Loss : 0.3357 | Train acc : 0.8427 | Eval Acc : 0.8422 | Eval Log. Respected : 0.9308
     Batch 000 | Loss : 0.3193 | Acc : 0.8523
     Batch 005 | Loss : 0.3602 | Acc : 0.8331
     Batch 010 | Loss : 0.3356 | Acc : 0.8432
     Batch 015 | Loss : 0.3713 | Acc : 0.8216
     Batch 020 | Loss : 0.3140 | Acc : 0.8639
     Batch 025 | Loss : 0.4183 | Acc : 0.7955
     Batch 030 | Loss : 0.3231 | Acc : 0.8535
     Batch 035 | Loss : 0.3668 | Acc : 0.8223
     Batch 040 | Loss : 0.3436 | Acc : 0.8372
     Batch 045 | Loss : 0.3990 | Acc : 0.8126
     Batch 050 | Loss : 0.3565 | Acc : 0.8279
     Batch 055 | Loss : 0.4179 | Acc : 0.7990
     Batch 060 | Loss : 0.3115 | Acc : 0.8620
     Batch 065 | Loss : 0.3070 | Acc : 0.8640
     Batch 070 | Loss : 0.3552 | Acc : 0.8303
     Batch 075 | Loss : 0.3502 | Acc : 0.8386
     Batch 080 | Loss : 0.3390 | Acc : 0.8421
     Batch 085 | Loss : 0.2912 | Acc : 0.8674
     Batch 090 | Loss : 0.2550 | Acc : 0.8892
     Batch 095 | Loss : 0.3560 | Acc : 0.8306
     Batch 100 | Loss : 0.3244 | Acc : 0.8523
     Batch 105 | Loss : 0.2938 | Acc : 0.8666
     Batch 110 | Loss : 0.3524 | Acc : 0.8292
     Batch 115 | Loss : 0.3492 | Acc : 0.8358
     Batch 120 | Loss : 0.3154 | Acc : 0.8545
     Batch 125 | Loss : 0.3232 | Acc : 0.8473
     Batch 130 | Loss : 0.3376 | Acc : 0.8417
     Batch 135 | Loss : 0.3106 | Acc : 0.8584
     Batch 140 | Loss : 0.3596 | Acc : 0.8374
     Batch 145 | Loss : 0.3165 | Acc : 0.8560
     Batch 150 | Loss : 0.3474 | Acc : 0.8346
Epoch 00011 | Train Loss : 0.3385 | Eval Loss : 0.3420 | Train acc : 0.8436 | Eval Acc : 0.8403 | Eval Log. Respected : 0.9258
     Batch 000 | Loss : 0.2968 | Acc : 0.8663
     Batch 005 | Loss : 0.3450 | Acc : 0.8422
     Batch 010 | Loss : 0.3580 | Acc : 0.8323
     Batch 015 | Loss : 0.3187 | Acc : 0.8571
     Batch 020 | Loss : 0.3848 | Acc : 0.8203
     Batch 025 | Loss : 0.3245 | Acc : 0.8514
     Batch 030 | Loss : 0.3562 | Acc : 0.8291
     Batch 035 | Loss : 0.2843 | Acc : 0.8718
     Batch 040 | Loss : 0.3402 | Acc : 0.8421
     Batch 045 | Loss : 0.4649 | Acc : 0.7893
     Batch 050 | Loss : 0.3127 | Acc : 0.8573
     Batch 055 | Loss : 0.3401 | Acc : 0.8432
     Batch 060 | Loss : 0.3637 | Acc : 0.8271
     Batch 065 | Loss : 0.3121 | Acc : 0.8535
     Batch 070 | Loss : 0.3135 | Acc : 0.8573
     Batch 075 | Loss : 0.3431 | Acc : 0.8375
     Batch 080 | Loss : 0.3535 | Acc : 0.8350
     Batch 085 | Loss : 0.3272 | Acc : 0.8545
     Batch 090 | Loss : 0.3548 | Acc : 0.8386
     Batch 095 | Loss : 0.3430 | Acc : 0.8450
     Batch 100 | Loss : 0.3509 | Acc : 0.8358
     Batch 105 | Loss : 0.3289 | Acc : 0.8487
     Batch 110 | Loss : 0.3130 | Acc : 0.8595
     Batch 115 | Loss : 0.3610 | Acc : 0.8283
     Batch 120 | Loss : 0.4075 | Acc : 0.8020
     Batch 125 | Loss : 0.3870 | Acc : 0.8155
     Batch 130 | Loss : 0.3551 | Acc : 0.8324
     Batch 135 | Loss : 0.2775 | Acc : 0.8783
     Batch 140 | Loss : 0.4625 | Acc : 0.7864
     Batch 145 | Loss : 0.3486 | Acc : 0.8460
     Batch 150 | Loss : 0.3262 | Acc : 0.8489
Epoch 00012 | Train Loss : 0.3355 | Eval Loss : 0.3412 | Train acc : 0.8455 | Eval Acc : 0.8434 | Eval Log. Respected : 0.9351
     Batch 000 | Loss : 0.3112 | Acc : 0.8588
     Batch 005 | Loss : 0.3458 | Acc : 0.8385
     Batch 010 | Loss : 0.3265 | Acc : 0.8553
     Batch 015 | Loss : 0.3336 | Acc : 0.8459
     Batch 020 | Loss : 0.2965 | Acc : 0.8710
     Batch 025 | Loss : 0.2886 | Acc : 0.8691
     Batch 030 | Loss : 0.2841 | Acc : 0.8714
     Batch 035 | Loss : 0.3220 | Acc : 0.8574
     Batch 040 | Loss : 0.3224 | Acc : 0.8532
     Batch 045 | Loss : 0.3501 | Acc : 0.8395
     Batch 050 | Loss : 0.3738 | Acc : 0.8280
     Batch 055 | Loss : 0.3355 | Acc : 0.8447
     Batch 060 | Loss : 0.2859 | Acc : 0.8709
     Batch 065 | Loss : 0.3345 | Acc : 0.8411
     Batch 070 | Loss : 0.2847 | Acc : 0.8723
     Batch 075 | Loss : 0.3326 | Acc : 0.8477
     Batch 080 | Loss : 0.3424 | Acc : 0.8448
     Batch 085 | Loss : 0.2735 | Acc : 0.8758
     Batch 090 | Loss : 0.3837 | Acc : 0.8133
     Batch 095 | Loss : 0.3762 | Acc : 0.8198
     Batch 100 | Loss : 0.3504 | Acc : 0.8350
     Batch 105 | Loss : 0.2814 | Acc : 0.8726
     Batch 110 | Loss : 0.4046 | Acc : 0.8094
     Batch 115 | Loss : 0.3194 | Acc : 0.8580
     Batch 120 | Loss : 0.3159 | Acc : 0.8573
     Batch 125 | Loss : 0.3595 | Acc : 0.8348
     Batch 130 | Loss : 0.3245 | Acc : 0.8467
     Batch 135 | Loss : 0.3698 | Acc : 0.8256
     Batch 140 | Loss : 0.3412 | Acc : 0.8415
     Batch 145 | Loss : 0.3485 | Acc : 0.8384
     Batch 150 | Loss : 0.3111 | Acc : 0.8594
Epoch 00013 | Train Loss : 0.3345 | Eval Loss : 0.3409 | Train acc : 0.8458 | Eval Acc : 0.8388 | Eval Log. Respected : 0.9220
     Batch 000 | Loss : 0.3804 | Acc : 0.8265
     Batch 005 | Loss : 0.2869 | Acc : 0.8732
     Batch 010 | Loss : 0.3185 | Acc : 0.8538
     Batch 015 | Loss : 0.2985 | Acc : 0.8620
     Batch 020 | Loss : 0.3033 | Acc : 0.8639
     Batch 025 | Loss : 0.3384 | Acc : 0.8403
     Batch 030 | Loss : 0.3485 | Acc : 0.8394
     Batch 035 | Loss : 0.3638 | Acc : 0.8316
     Batch 040 | Loss : 0.3336 | Acc : 0.8503
     Batch 045 | Loss : 0.3547 | Acc : 0.8299
     Batch 050 | Loss : 0.3133 | Acc : 0.8575
     Batch 055 | Loss : 0.3572 | Acc : 0.8339
     Batch 060 | Loss : 0.2888 | Acc : 0.8672
     Batch 065 | Loss : 0.4320 | Acc : 0.8017
     Batch 070 | Loss : 0.3785 | Acc : 0.8188
     Batch 075 | Loss : 0.3628 | Acc : 0.8265
     Batch 080 | Loss : 0.3539 | Acc : 0.8316
     Batch 085 | Loss : 0.3754 | Acc : 0.8180
     Batch 090 | Loss : 0.3545 | Acc : 0.8288
     Batch 095 | Loss : 0.2936 | Acc : 0.8673
     Batch 100 | Loss : 0.3543 | Acc : 0.8384
     Batch 105 | Loss : 0.3205 | Acc : 0.8513
     Batch 110 | Loss : 0.3163 | Acc : 0.8550
     Batch 115 | Loss : 0.3512 | Acc : 0.8332
     Batch 120 | Loss : 0.3008 | Acc : 0.8598
     Batch 125 | Loss : 0.3168 | Acc : 0.8562
     Batch 130 | Loss : 0.3276 | Acc : 0.8496
     Batch 135 | Loss : 0.3853 | Acc : 0.8181
     Batch 140 | Loss : 0.3158 | Acc : 0.8523
     Batch 145 | Loss : 0.3545 | Acc : 0.8325
     Batch 150 | Loss : 0.3022 | Acc : 0.8601
Epoch 00014 | Train Loss : 0.3336 | Eval Loss : 0.3358 | Train acc : 0.8463 | Eval Acc : 0.8429 | Eval Log. Respected : 0.9212
     Batch 000 | Loss : 0.3558 | Acc : 0.8303
     Batch 005 | Loss : 0.3057 | Acc : 0.8583
     Batch 010 | Loss : 0.3317 | Acc : 0.8463
     Batch 015 | Loss : 0.3382 | Acc : 0.8405
     Batch 020 | Loss : 0.3803 | Acc : 0.8275
     Batch 025 | Loss : 0.2923 | Acc : 0.8700
     Batch 030 | Loss : 0.3334 | Acc : 0.8441
     Batch 035 | Loss : 0.3261 | Acc : 0.8490
     Batch 040 | Loss : 0.3827 | Acc : 0.8166
     Batch 045 | Loss : 0.3335 | Acc : 0.8462
     Batch 050 | Loss : 0.3085 | Acc : 0.8600
     Batch 055 | Loss : 0.3524 | Acc : 0.8345
     Batch 060 | Loss : 0.2985 | Acc : 0.8682
     Batch 065 | Loss : 0.2941 | Acc : 0.8641
     Batch 070 | Loss : 0.3260 | Acc : 0.8507
     Batch 075 | Loss : 0.3527 | Acc : 0.8353
     Batch 080 | Loss : 0.4074 | Acc : 0.8102
     Batch 085 | Loss : 0.3869 | Acc : 0.8227
     Batch 090 | Loss : 0.2940 | Acc : 0.8659
     Batch 095 | Loss : 0.3137 | Acc : 0.8538
     Batch 100 | Loss : 0.3582 | Acc : 0.8309
     Batch 105 | Loss : 0.3363 | Acc : 0.8425
     Batch 110 | Loss : 0.2881 | Acc : 0.8722
     Batch 115 | Loss : 0.3832 | Acc : 0.8196
     Batch 120 | Loss : 0.3448 | Acc : 0.8395
     Batch 125 | Loss : 0.4096 | Acc : 0.8118
     Batch 130 | Loss : 0.3236 | Acc : 0.8545
     Batch 135 | Loss : 0.3749 | Acc : 0.8228
     Batch 140 | Loss : 0.2882 | Acc : 0.8755
     Batch 145 | Loss : 0.3272 | Acc : 0.8505
     Batch 150 | Loss : 0.3734 | Acc : 0.8245
Epoch 00015 | Train Loss : 0.3315 | Eval Loss : 0.3339 | Train acc : 0.8471 | Eval Acc : 0.8433 | Eval Log. Respected : 0.9345
     Batch 000 | Loss : 0.3301 | Acc : 0.8474
     Batch 005 | Loss : 0.3661 | Acc : 0.8272
     Batch 010 | Loss : 0.3251 | Acc : 0.8473
     Batch 015 | Loss : 0.3024 | Acc : 0.8593
     Batch 020 | Loss : 0.2828 | Acc : 0.8734
     Batch 025 | Loss : 0.3612 | Acc : 0.8296
     Batch 030 | Loss : 0.3619 | Acc : 0.8291
     Batch 035 | Loss : 0.3271 | Acc : 0.8547
     Batch 040 | Loss : 0.2763 | Acc : 0.8766
     Batch 045 | Loss : 0.3712 | Acc : 0.8258
     Batch 050 | Loss : 0.2984 | Acc : 0.8662
     Batch 055 | Loss : 0.3785 | Acc : 0.8199
     Batch 060 | Loss : 0.3407 | Acc : 0.8443
     Batch 065 | Loss : 0.3339 | Acc : 0.8415
     Batch 070 | Loss : 0.3718 | Acc : 0.8308
     Batch 075 | Loss : 0.3149 | Acc : 0.8543
     Batch 080 | Loss : 0.3018 | Acc : 0.8611
     Batch 085 | Loss : 0.3207 | Acc : 0.8504
     Batch 090 | Loss : 0.3110 | Acc : 0.8576
     Batch 095 | Loss : 0.3526 | Acc : 0.8394
     Batch 100 | Loss : 0.3389 | Acc : 0.8394
     Batch 105 | Loss : 0.3164 | Acc : 0.8516
     Batch 110 | Loss : 0.2928 | Acc : 0.8661
     Batch 115 | Loss : 0.3442 | Acc : 0.8408
     Batch 120 | Loss : 0.3352 | Acc : 0.8435
     Batch 125 | Loss : 0.3572 | Acc : 0.8317
     Batch 130 | Loss : 0.3366 | Acc : 0.8460
     Batch 135 | Loss : 0.3036 | Acc : 0.8645
     Batch 140 | Loss : 0.2793 | Acc : 0.8738
     Batch 145 | Loss : 0.3865 | Acc : 0.8274
     Batch 150 | Loss : 0.3326 | Acc : 0.8441
Epoch 00016 | Train Loss : 0.3303 | Eval Loss : 0.3369 | Train acc : 0.8477 | Eval Acc : 0.8411 | Eval Log. Respected : 0.9171
     Batch 000 | Loss : 0.3851 | Acc : 0.8164
     Batch 005 | Loss : 0.3264 | Acc : 0.8476
     Batch 010 | Loss : 0.2905 | Acc : 0.8676
     Batch 015 | Loss : 0.3515 | Acc : 0.8348
     Batch 020 | Loss : 0.3027 | Acc : 0.8585
     Batch 025 | Loss : 0.2898 | Acc : 0.8675
     Batch 030 | Loss : 0.3227 | Acc : 0.8496
     Batch 035 | Loss : 0.3137 | Acc : 0.8543
     Batch 040 | Loss : 0.3575 | Acc : 0.8333
     Batch 045 | Loss : 0.3096 | Acc : 0.8581
     Batch 050 | Loss : 0.3942 | Acc : 0.8113
     Batch 055 | Loss : 0.3143 | Acc : 0.8577
     Batch 060 | Loss : 0.2701 | Acc : 0.8781
     Batch 065 | Loss : 0.2977 | Acc : 0.8643
     Batch 070 | Loss : 0.3527 | Acc : 0.8317
     Batch 075 | Loss : 0.2849 | Acc : 0.8742
     Batch 080 | Loss : 0.3173 | Acc : 0.8599
     Batch 085 | Loss : 0.2935 | Acc : 0.8698
     Batch 090 | Loss : 0.3577 | Acc : 0.8342
     Batch 095 | Loss : 0.3098 | Acc : 0.8598
     Batch 100 | Loss : 0.4211 | Acc : 0.7991
     Batch 105 | Loss : 0.3073 | Acc : 0.8586
     Batch 110 | Loss : 0.3387 | Acc : 0.8481
     Batch 115 | Loss : 0.2937 | Acc : 0.8700
     Batch 120 | Loss : 0.3236 | Acc : 0.8499
     Batch 125 | Loss : 0.2462 | Acc : 0.8884
     Batch 130 | Loss : 0.4171 | Acc : 0.8087
     Batch 135 | Loss : 0.2863 | Acc : 0.8788
     Batch 140 | Loss : 0.4332 | Acc : 0.7971
     Batch 145 | Loss : 0.3211 | Acc : 0.8570
     Batch 150 | Loss : 0.3342 | Acc : 0.8459
Epoch 00017 | Train Loss : 0.3298 | Eval Loss : 0.3311 | Train acc : 0.8481 | Eval Acc : 0.8448 | Eval Log. Respected : 0.9263
     Batch 000 | Loss : 0.3305 | Acc : 0.8436
     Batch 005 | Loss : 0.2937 | Acc : 0.8655
     Batch 010 | Loss : 0.3373 | Acc : 0.8428
     Batch 015 | Loss : 0.2960 | Acc : 0.8730
     Batch 020 | Loss : 0.3158 | Acc : 0.8561
     Batch 025 | Loss : 0.3010 | Acc : 0.8667
     Batch 030 | Loss : 0.3555 | Acc : 0.8368
     Batch 035 | Loss : 0.3175 | Acc : 0.8582
     Batch 040 | Loss : 0.2943 | Acc : 0.8661
     Batch 045 | Loss : 0.3491 | Acc : 0.8423
     Batch 050 | Loss : 0.3411 | Acc : 0.8376
     Batch 055 | Loss : 0.3300 | Acc : 0.8440
     Batch 060 | Loss : 0.3283 | Acc : 0.8485
     Batch 065 | Loss : 0.3430 | Acc : 0.8435
     Batch 070 | Loss : 0.3079 | Acc : 0.8578
     Batch 075 | Loss : 0.3845 | Acc : 0.8295
     Batch 080 | Loss : 0.2980 | Acc : 0.8709
     Batch 085 | Loss : 0.2574 | Acc : 0.8907
     Batch 090 | Loss : 0.2988 | Acc : 0.8639
     Batch 095 | Loss : 0.3735 | Acc : 0.8317
     Batch 100 | Loss : 0.3497 | Acc : 0.8299
     Batch 105 | Loss : 0.3115 | Acc : 0.8545
     Batch 110 | Loss : 0.3145 | Acc : 0.8565
     Batch 115 | Loss : 0.3238 | Acc : 0.8518
     Batch 120 | Loss : 0.2769 | Acc : 0.8738
     Batch 125 | Loss : 0.2995 | Acc : 0.8636
     Batch 130 | Loss : 0.3074 | Acc : 0.8610
     Batch 135 | Loss : 0.3232 | Acc : 0.8529
     Batch 140 | Loss : 0.3418 | Acc : 0.8392
     Batch 145 | Loss : 0.2905 | Acc : 0.8646
     Batch 150 | Loss : 0.3579 | Acc : 0.8275
Epoch 00018 | Train Loss : 0.3274 | Eval Loss : 0.3273 | Train acc : 0.8495 | Eval Acc : 0.8462 | Eval Log. Respected : 0.9195
     Batch 000 | Loss : 0.3295 | Acc : 0.8493
     Batch 005 | Loss : 0.4293 | Acc : 0.8068
     Batch 010 | Loss : 0.3184 | Acc : 0.8523
     Batch 015 | Loss : 0.2677 | Acc : 0.8838
     Batch 020 | Loss : 0.3317 | Acc : 0.8482
     Batch 025 | Loss : 0.3120 | Acc : 0.8564
     Batch 030 | Loss : 0.2958 | Acc : 0.8660
     Batch 035 | Loss : 0.3194 | Acc : 0.8515
     Batch 040 | Loss : 0.3099 | Acc : 0.8577
     Batch 045 | Loss : 0.2766 | Acc : 0.8763
     Batch 050 | Loss : 0.3240 | Acc : 0.8483
     Batch 055 | Loss : 0.2729 | Acc : 0.8781
     Batch 060 | Loss : 0.3421 | Acc : 0.8365
     Batch 065 | Loss : 0.3550 | Acc : 0.8283
     Batch 070 | Loss : 0.3107 | Acc : 0.8556
     Batch 075 | Loss : 0.3500 | Acc : 0.8358
     Batch 080 | Loss : 0.3111 | Acc : 0.8589
     Batch 085 | Loss : 0.3329 | Acc : 0.8435
     Batch 090 | Loss : 0.3256 | Acc : 0.8546
     Batch 095 | Loss : 0.3057 | Acc : 0.8600
     Batch 100 | Loss : 0.3034 | Acc : 0.8639
     Batch 105 | Loss : 0.3768 | Acc : 0.8226
     Batch 110 | Loss : 0.3030 | Acc : 0.8607
     Batch 115 | Loss : 0.3905 | Acc : 0.8168
     Batch 120 | Loss : 0.2833 | Acc : 0.8771
     Batch 125 | Loss : 0.3132 | Acc : 0.8597
     Batch 130 | Loss : 0.2694 | Acc : 0.8763
     Batch 135 | Loss : 0.2797 | Acc : 0.8743
     Batch 140 | Loss : 0.3290 | Acc : 0.8446
     Batch 145 | Loss : 0.3306 | Acc : 0.8474
     Batch 150 | Loss : 0.3561 | Acc : 0.8353
Epoch 00019 | Train Loss : 0.3260 | Eval Loss : 0.3335 | Train acc : 0.8498 | Eval Acc : 0.8439 | Eval Log. Respected : 0.9404
     Batch 000 | Loss : 0.3167 | Acc : 0.8565
     Batch 005 | Loss : 0.3548 | Acc : 0.8372
     Batch 010 | Loss : 0.2787 | Acc : 0.8757
     Batch 015 | Loss : 0.2973 | Acc : 0.8671
     Batch 020 | Loss : 0.3718 | Acc : 0.8234
     Batch 025 | Loss : 0.3060 | Acc : 0.8635
     Batch 030 | Loss : 0.2867 | Acc : 0.8694
     Batch 035 | Loss : 0.3073 | Acc : 0.8603
     Batch 040 | Loss : 0.3180 | Acc : 0.8569
     Batch 045 | Loss : 0.3954 | Acc : 0.8098
     Batch 050 | Loss : 0.3965 | Acc : 0.8082
     Batch 055 | Loss : 0.3027 | Acc : 0.8614
     Batch 060 | Loss : 0.2742 | Acc : 0.8773
     Batch 065 | Loss : 0.3061 | Acc : 0.8558
     Batch 070 | Loss : 0.3558 | Acc : 0.8382
     Batch 075 | Loss : 0.4815 | Acc : 0.7719
     Batch 080 | Loss : 0.3241 | Acc : 0.8576
     Batch 085 | Loss : 0.3004 | Acc : 0.8588
     Batch 090 | Loss : 0.2705 | Acc : 0.8779
     Batch 095 | Loss : 0.3856 | Acc : 0.8249
     Batch 100 | Loss : 0.2970 | Acc : 0.8667
     Batch 105 | Loss : 0.3353 | Acc : 0.8450
     Batch 110 | Loss : 0.3098 | Acc : 0.8546
     Batch 115 | Loss : 0.3282 | Acc : 0.8525
     Batch 120 | Loss : 0.3075 | Acc : 0.8619
     Batch 125 | Loss : 0.3249 | Acc : 0.8511
     Batch 130 | Loss : 0.3085 | Acc : 0.8610
     Batch 135 | Loss : 0.3395 | Acc : 0.8382
     Batch 140 | Loss : 0.3157 | Acc : 0.8541
     Batch 145 | Loss : 0.3078 | Acc : 0.8611
     Batch 150 | Loss : 0.3615 | Acc : 0.8309
Epoch 00020 | Train Loss : 0.3262 | Eval Loss : 0.3307 | Train acc : 0.8497 | Eval Acc : 0.8445 | Eval Log. Respected : 0.9268
     Batch 000 | Loss : 0.3213 | Acc : 0.8450
     Batch 005 | Loss : 0.3094 | Acc : 0.8591
     Batch 010 | Loss : 0.3011 | Acc : 0.8595
     Batch 015 | Loss : 0.3019 | Acc : 0.8630
     Batch 020 | Loss : 0.3822 | Acc : 0.8206
     Batch 025 | Loss : 0.2963 | Acc : 0.8705
     Batch 030 | Loss : 0.3776 | Acc : 0.8223
     Batch 035 | Loss : 0.2734 | Acc : 0.8748
     Batch 040 | Loss : 0.2737 | Acc : 0.8796
     Batch 045 | Loss : 0.3851 | Acc : 0.8171
     Batch 050 | Loss : 0.3329 | Acc : 0.8475
     Batch 055 | Loss : 0.2888 | Acc : 0.8703
     Batch 060 | Loss : 0.3310 | Acc : 0.8427
     Batch 065 | Loss : 0.3038 | Acc : 0.8658
     Batch 070 | Loss : 0.3412 | Acc : 0.8425
     Batch 075 | Loss : 0.3868 | Acc : 0.8290
     Batch 080 | Loss : 0.3240 | Acc : 0.8514
     Batch 085 | Loss : 0.3089 | Acc : 0.8598
     Batch 090 | Loss : 0.3488 | Acc : 0.8411
     Batch 095 | Loss : 0.3129 | Acc : 0.8605
     Batch 100 | Loss : 0.3757 | Acc : 0.8197
     Batch 105 | Loss : 0.3044 | Acc : 0.8607
     Batch 110 | Loss : 0.3465 | Acc : 0.8380
     Batch 115 | Loss : 0.2889 | Acc : 0.8729
     Batch 120 | Loss : 0.3696 | Acc : 0.8210
     Batch 125 | Loss : 0.2767 | Acc : 0.8753
     Batch 130 | Loss : 0.3329 | Acc : 0.8452
     Batch 135 | Loss : 0.3168 | Acc : 0.8524
     Batch 140 | Loss : 0.3566 | Acc : 0.8293
     Batch 145 | Loss : 0.3275 | Acc : 0.8494
     Batch 150 | Loss : 0.3563 | Acc : 0.8336
Epoch 00021 | Train Loss : 0.3241 | Eval Loss : 0.3278 | Train acc : 0.8509 | Eval Acc : 0.8456 | Eval Log. Respected : 0.9303
     Batch 000 | Loss : 0.3010 | Acc : 0.8607
     Batch 005 | Loss : 0.3379 | Acc : 0.8409
     Batch 010 | Loss : 0.3120 | Acc : 0.8624
     Batch 015 | Loss : 0.2997 | Acc : 0.8634
     Batch 020 | Loss : 0.3219 | Acc : 0.8496
     Batch 025 | Loss : 0.3367 | Acc : 0.8412
     Batch 030 | Loss : 0.3039 | Acc : 0.8627
     Batch 035 | Loss : 0.3914 | Acc : 0.8227
     Batch 040 | Loss : 0.3915 | Acc : 0.8112
     Batch 045 | Loss : 0.3584 | Acc : 0.8326
     Batch 050 | Loss : 0.3389 | Acc : 0.8435
     Batch 055 | Loss : 0.3267 | Acc : 0.8460
     Batch 060 | Loss : 0.3377 | Acc : 0.8427
     Batch 065 | Loss : 0.2734 | Acc : 0.8771
     Batch 070 | Loss : 0.3274 | Acc : 0.8483
     Batch 075 | Loss : 0.2921 | Acc : 0.8676
     Batch 080 | Loss : 0.3509 | Acc : 0.8316
     Batch 085 | Loss : 0.3227 | Acc : 0.8466
     Batch 090 | Loss : 0.2908 | Acc : 0.8656
     Batch 095 | Loss : 0.3889 | Acc : 0.8247
     Batch 100 | Loss : 0.3310 | Acc : 0.8450
     Batch 105 | Loss : 0.3078 | Acc : 0.8597
     Batch 110 | Loss : 0.2844 | Acc : 0.8710
     Batch 115 | Loss : 0.3898 | Acc : 0.8155
     Batch 120 | Loss : 0.3599 | Acc : 0.8354
     Batch 125 | Loss : 0.3453 | Acc : 0.8339
     Batch 130 | Loss : 0.3117 | Acc : 0.8587
     Batch 135 | Loss : 0.4255 | Acc : 0.8053
     Batch 140 | Loss : 0.3563 | Acc : 0.8358
     Batch 145 | Loss : 0.3302 | Acc : 0.8458
     Batch 150 | Loss : 0.2789 | Acc : 0.8758
Epoch 00022 | Train Loss : 0.3255 | Eval Loss : 0.3322 | Train acc : 0.8499 | Eval Acc : 0.8466 | Eval Log. Respected : 0.9272
     Batch 000 | Loss : 0.2855 | Acc : 0.8714
     Batch 005 | Loss : 0.3277 | Acc : 0.8510
     Batch 010 | Loss : 0.3306 | Acc : 0.8489
     Batch 015 | Loss : 0.2803 | Acc : 0.8743
     Batch 020 | Loss : 0.2851 | Acc : 0.8707
     Batch 025 | Loss : 0.3191 | Acc : 0.8560
     Batch 030 | Loss : 0.3140 | Acc : 0.8560
     Batch 035 | Loss : 0.3741 | Acc : 0.8271
     Batch 040 | Loss : 0.3163 | Acc : 0.8559
     Batch 045 | Loss : 0.3044 | Acc : 0.8615
     Batch 050 | Loss : 0.3041 | Acc : 0.8632
     Batch 055 | Loss : 0.2943 | Acc : 0.8649
     Batch 060 | Loss : 0.3200 | Acc : 0.8526
     Batch 065 | Loss : 0.3121 | Acc : 0.8595
     Batch 070 | Loss : 0.4051 | Acc : 0.8137
     Batch 075 | Loss : 0.3794 | Acc : 0.8200
     Batch 080 | Loss : 0.3140 | Acc : 0.8586
     Batch 085 | Loss : 0.3672 | Acc : 0.8282
     Batch 090 | Loss : 0.3051 | Acc : 0.8567
     Batch 095 | Loss : 0.2802 | Acc : 0.8706
     Batch 100 | Loss : 0.3331 | Acc : 0.8431
     Batch 105 | Loss : 0.3319 | Acc : 0.8491
     Batch 110 | Loss : 0.3442 | Acc : 0.8338
     Batch 115 | Loss : 0.3541 | Acc : 0.8314
     Batch 120 | Loss : 0.2829 | Acc : 0.8750
     Batch 125 | Loss : 0.2770 | Acc : 0.8771
     Batch 130 | Loss : 0.3447 | Acc : 0.8420
     Batch 135 | Loss : 0.3064 | Acc : 0.8600
     Batch 140 | Loss : 0.3206 | Acc : 0.8570
     Batch 145 | Loss : 0.3355 | Acc : 0.8428
     Batch 150 | Loss : 0.2604 | Acc : 0.8862
Epoch 00023 | Train Loss : 0.3251 | Eval Loss : 0.3287 | Train acc : 0.8503 | Eval Acc : 0.8457 | Eval Log. Respected : 0.9429
     Batch 000 | Loss : 0.2852 | Acc : 0.8689
     Batch 005 | Loss : 0.2581 | Acc : 0.8841
     Batch 010 | Loss : 0.3260 | Acc : 0.8490
     Batch 015 | Loss : 0.2847 | Acc : 0.8748
     Batch 020 | Loss : 0.3173 | Acc : 0.8517
     Batch 025 | Loss : 0.3395 | Acc : 0.8416
     Batch 030 | Loss : 0.3643 | Acc : 0.8249
     Batch 035 | Loss : 0.3342 | Acc : 0.8454
     Batch 040 | Loss : 0.2789 | Acc : 0.8704
     Batch 045 | Loss : 0.2887 | Acc : 0.8671
     Batch 050 | Loss : 0.3825 | Acc : 0.8196
     Batch 055 | Loss : 0.3584 | Acc : 0.8321
     Batch 060 | Loss : 0.3753 | Acc : 0.8267
     Batch 065 | Loss : 0.3347 | Acc : 0.8438
     Batch 070 | Loss : 0.3389 | Acc : 0.8444
     Batch 075 | Loss : 0.3234 | Acc : 0.8514
     Batch 080 | Loss : 0.2839 | Acc : 0.8715
     Batch 085 | Loss : 0.3175 | Acc : 0.8532
     Batch 090 | Loss : 0.2761 | Acc : 0.8781
     Batch 095 | Loss : 0.3141 | Acc : 0.8538
     Batch 100 | Loss : 0.2729 | Acc : 0.8746
     Batch 105 | Loss : 0.3862 | Acc : 0.8184
     Batch 110 | Loss : 0.3143 | Acc : 0.8562
     Batch 115 | Loss : 0.3796 | Acc : 0.8214
     Batch 120 | Loss : 0.3143 | Acc : 0.8568
     Batch 125 | Loss : 0.2884 | Acc : 0.8698
     Batch 130 | Loss : 0.3496 | Acc : 0.8357
     Batch 135 | Loss : 0.2972 | Acc : 0.8651
     Batch 140 | Loss : 0.2789 | Acc : 0.8737
     Batch 145 | Loss : 0.4204 | Acc : 0.8068
     Batch 150 | Loss : 0.3444 | Acc : 0.8382
Epoch 00024 | Train Loss : 0.3245 | Eval Loss : 0.3372 | Train acc : 0.8505 | Eval Acc : 0.8423 | Eval Log. Respected : 0.9202
     Batch 000 | Loss : 0.2972 | Acc : 0.8669
     Batch 005 | Loss : 0.2903 | Acc : 0.8657
     Batch 010 | Loss : 0.2998 | Acc : 0.8637
     Batch 015 | Loss : 0.3261 | Acc : 0.8462
     Batch 020 | Loss : 0.3175 | Acc : 0.8529
     Batch 025 | Loss : 0.3377 | Acc : 0.8448
     Batch 030 | Loss : 0.2994 | Acc : 0.8611
     Batch 035 | Loss : 0.3192 | Acc : 0.8525
     Batch 040 | Loss : 0.3721 | Acc : 0.8245
     Batch 045 | Loss : 0.3329 | Acc : 0.8434
     Batch 050 | Loss : 0.3057 | Acc : 0.8593
     Batch 055 | Loss : 0.3393 | Acc : 0.8416
     Batch 060 | Loss : 0.3057 | Acc : 0.8574
     Batch 065 | Loss : 0.4010 | Acc : 0.8133
     Batch 070 | Loss : 0.3017 | Acc : 0.8661
     Batch 075 | Loss : 0.3049 | Acc : 0.8636
     Batch 080 | Loss : 0.4444 | Acc : 0.8045
     Batch 085 | Loss : 0.3186 | Acc : 0.8554
     Batch 090 | Loss : 0.2969 | Acc : 0.8724
     Batch 095 | Loss : 0.3553 | Acc : 0.8320
     Batch 100 | Loss : 0.3885 | Acc : 0.8262
     Batch 105 | Loss : 0.3389 | Acc : 0.8448
     Batch 110 | Loss : 0.3015 | Acc : 0.8595
     Batch 115 | Loss : 0.3540 | Acc : 0.8354
     Batch 120 | Loss : 0.2853 | Acc : 0.8697
     Batch 125 | Loss : 0.3122 | Acc : 0.8519
     Batch 130 | Loss : 0.3316 | Acc : 0.8455
     Batch 135 | Loss : 0.3159 | Acc : 0.8561
     Batch 140 | Loss : 0.2984 | Acc : 0.8645
     Batch 145 | Loss : 0.3016 | Acc : 0.8621
     Batch 150 | Loss : 0.3109 | Acc : 0.8563
Epoch 00025 | Train Loss : 0.3229 | Eval Loss : 0.3247 | Train acc : 0.8513 | Eval Acc : 0.8487 | Eval Log. Respected : 0.9244
     Batch 000 | Loss : 0.3383 | Acc : 0.8443
     Batch 005 | Loss : 0.2825 | Acc : 0.8722
     Batch 010 | Loss : 0.2814 | Acc : 0.8768
     Batch 015 | Loss : 0.3273 | Acc : 0.8524
     Batch 020 | Loss : 0.2924 | Acc : 0.8631
     Batch 025 | Loss : 0.3481 | Acc : 0.8310
     Batch 030 | Loss : 0.2580 | Acc : 0.8863
     Batch 035 | Loss : 0.2942 | Acc : 0.8645
     Batch 040 | Loss : 0.2959 | Acc : 0.8676
     Batch 045 | Loss : 0.3456 | Acc : 0.8359
     Batch 050 | Loss : 0.2917 | Acc : 0.8705
     Batch 055 | Loss : 0.2714 | Acc : 0.8772
     Batch 060 | Loss : 0.3409 | Acc : 0.8367
     Batch 065 | Loss : 0.3278 | Acc : 0.8480
     Batch 070 | Loss : 0.3534 | Acc : 0.8336
     Batch 075 | Loss : 0.3077 | Acc : 0.8597
     Batch 080 | Loss : 0.3412 | Acc : 0.8376
     Batch 085 | Loss : 0.3121 | Acc : 0.8555
     Batch 090 | Loss : 0.3591 | Acc : 0.8334
     Batch 095 | Loss : 0.3259 | Acc : 0.8489
     Batch 100 | Loss : 0.3089 | Acc : 0.8580
     Batch 105 | Loss : 0.3310 | Acc : 0.8472
     Batch 110 | Loss : 0.3380 | Acc : 0.8422
     Batch 115 | Loss : 0.3766 | Acc : 0.8234
     Batch 120 | Loss : 0.3103 | Acc : 0.8543
     Batch 125 | Loss : 0.3594 | Acc : 0.8374
     Batch 130 | Loss : 0.2761 | Acc : 0.8806
     Batch 135 | Loss : 0.3351 | Acc : 0.8424
     Batch 140 | Loss : 0.3135 | Acc : 0.8556
     Batch 145 | Loss : 0.2620 | Acc : 0.8873
     Batch 150 | Loss : 0.3449 | Acc : 0.8360
Epoch 00026 | Train Loss : 0.3230 | Eval Loss : 0.3300 | Train acc : 0.8511 | Eval Acc : 0.8455 | Eval Log. Respected : 0.9277
     Batch 000 | Loss : 0.2962 | Acc : 0.8693
     Batch 005 | Loss : 0.3629 | Acc : 0.8348
     Batch 010 | Loss : 0.2839 | Acc : 0.8719
     Batch 015 | Loss : 0.3102 | Acc : 0.8559
     Batch 020 | Loss : 0.3691 | Acc : 0.8322
     Batch 025 | Loss : 0.3078 | Acc : 0.8583
     Batch 030 | Loss : 0.2981 | Acc : 0.8633
     Batch 035 | Loss : 0.3018 | Acc : 0.8617
     Batch 040 | Loss : 0.3052 | Acc : 0.8614
     Batch 045 | Loss : 0.3299 | Acc : 0.8474
     Batch 050 | Loss : 0.3087 | Acc : 0.8573
     Batch 055 | Loss : 0.4158 | Acc : 0.8028
     Batch 060 | Loss : 0.3155 | Acc : 0.8560
     Batch 065 | Loss : 0.3195 | Acc : 0.8513
     Batch 070 | Loss : 0.2975 | Acc : 0.8637
     Batch 075 | Loss : 0.3432 | Acc : 0.8419
     Batch 080 | Loss : 0.2843 | Acc : 0.8789
     Batch 085 | Loss : 0.3311 | Acc : 0.8443
     Batch 090 | Loss : 0.2932 | Acc : 0.8628
     Batch 095 | Loss : 0.3106 | Acc : 0.8580
     Batch 100 | Loss : 0.2754 | Acc : 0.8782
     Batch 105 | Loss : 0.3058 | Acc : 0.8586
     Batch 110 | Loss : 0.2992 | Acc : 0.8659
     Batch 115 | Loss : 0.3933 | Acc : 0.8148
     Batch 120 | Loss : 0.3240 | Acc : 0.8544
     Batch 125 | Loss : 0.3069 | Acc : 0.8585
     Batch 130 | Loss : 0.3736 | Acc : 0.8186
     Batch 135 | Loss : 0.3643 | Acc : 0.8281
     Batch 140 | Loss : 0.3100 | Acc : 0.8634
     Batch 145 | Loss : 0.3181 | Acc : 0.8490
     Batch 150 | Loss : 0.3086 | Acc : 0.8581
Epoch 00027 | Train Loss : 0.3218 | Eval Loss : 0.3292 | Train acc : 0.8517 | Eval Acc : 0.8469 | Eval Log. Respected : 0.9209
     Batch 000 | Loss : 0.3219 | Acc : 0.8521
     Batch 005 | Loss : 0.3821 | Acc : 0.8150
     Batch 010 | Loss : 0.4264 | Acc : 0.8019
     Batch 015 | Loss : 0.3453 | Acc : 0.8391
     Batch 020 | Loss : 0.2954 | Acc : 0.8636
     Batch 025 | Loss : 0.4370 | Acc : 0.8002
     Batch 030 | Loss : 0.3857 | Acc : 0.8160
     Batch 035 | Loss : 0.3203 | Acc : 0.8526
     Batch 040 | Loss : 0.3242 | Acc : 0.8507
     Batch 045 | Loss : 0.2662 | Acc : 0.8804
     Batch 050 | Loss : 0.3197 | Acc : 0.8484
     Batch 055 | Loss : 0.2986 | Acc : 0.8635
     Batch 060 | Loss : 0.2906 | Acc : 0.8711
     Batch 065 | Loss : 0.3096 | Acc : 0.8570
     Batch 070 | Loss : 0.3067 | Acc : 0.8599
     Batch 075 | Loss : 0.2750 | Acc : 0.8786
     Batch 080 | Loss : 0.2822 | Acc : 0.8677
     Batch 085 | Loss : 0.3957 | Acc : 0.8181
     Batch 090 | Loss : 0.3308 | Acc : 0.8453
     Batch 095 | Loss : 0.3434 | Acc : 0.8417
     Batch 100 | Loss : 0.3218 | Acc : 0.8512
     Batch 105 | Loss : 0.2826 | Acc : 0.8706
     Batch 110 | Loss : 0.2799 | Acc : 0.8704
     Batch 115 | Loss : 0.3160 | Acc : 0.8548
     Batch 120 | Loss : 0.3400 | Acc : 0.8400
     Batch 125 | Loss : 0.3747 | Acc : 0.8326
     Batch 130 | Loss : 0.2857 | Acc : 0.8704
     Batch 135 | Loss : 0.3346 | Acc : 0.8459
     Batch 140 | Loss : 0.3225 | Acc : 0.8463
     Batch 145 | Loss : 0.2923 | Acc : 0.8717
     Batch 150 | Loss : 0.3136 | Acc : 0.8520
Epoch 00028 | Train Loss : 0.3212 | Eval Loss : 0.3244 | Train acc : 0.8518 | Eval Acc : 0.8487 | Eval Log. Respected : 0.9382
     Batch 000 | Loss : 0.3179 | Acc : 0.8528
     Batch 005 | Loss : 0.3081 | Acc : 0.8602
     Batch 010 | Loss : 0.3179 | Acc : 0.8491
     Batch 015 | Loss : 0.3066 | Acc : 0.8569
     Batch 020 | Loss : 0.2778 | Acc : 0.8743
     Batch 025 | Loss : 0.2848 | Acc : 0.8674
     Batch 030 | Loss : 0.3292 | Acc : 0.8460
     Batch 035 | Loss : 0.3498 | Acc : 0.8319
     Batch 040 | Loss : 0.3267 | Acc : 0.8446
     Batch 045 | Loss : 0.3339 | Acc : 0.8421
     Batch 050 | Loss : 0.3026 | Acc : 0.8595
     Batch 055 | Loss : 0.3206 | Acc : 0.8515
     Batch 060 | Loss : 0.2965 | Acc : 0.8654
     Batch 065 | Loss : 0.2724 | Acc : 0.8757
     Batch 070 | Loss : 0.3488 | Acc : 0.8339
     Batch 075 | Loss : 0.3363 | Acc : 0.8428
     Batch 080 | Loss : 0.2680 | Acc : 0.8824
     Batch 085 | Loss : 0.3734 | Acc : 0.8301
     Batch 090 | Loss : 0.3478 | Acc : 0.8435
     Batch 095 | Loss : 0.2682 | Acc : 0.8866
     Batch 100 | Loss : 0.2872 | Acc : 0.8705
     Batch 105 | Loss : 0.3704 | Acc : 0.8268
     Batch 110 | Loss : 0.2982 | Acc : 0.8701
     Batch 115 | Loss : 0.2893 | Acc : 0.8655
     Batch 120 | Loss : 0.2697 | Acc : 0.8761
     Batch 125 | Loss : 0.3390 | Acc : 0.8488
     Batch 130 | Loss : 0.3183 | Acc : 0.8550
     Batch 135 | Loss : 0.3528 | Acc : 0.8391
     Batch 140 | Loss : 0.2955 | Acc : 0.8665
     Batch 145 | Loss : 0.3759 | Acc : 0.8186
     Batch 150 | Loss : 0.2929 | Acc : 0.8666
Epoch 00029 | Train Loss : 0.3231 | Eval Loss : 0.3278 | Train acc : 0.8511 | Eval Acc : 0.8461 | Eval Log. Respected : 0.9189
     Batch 000 | Loss : 0.4500 | Acc : 0.7957
     Batch 005 | Loss : 0.3156 | Acc : 0.8560
     Batch 010 | Loss : 0.3120 | Acc : 0.8586
     Batch 015 | Loss : 0.3376 | Acc : 0.8476
     Batch 020 | Loss : 0.3349 | Acc : 0.8437
     Batch 025 | Loss : 0.2968 | Acc : 0.8640
     Batch 030 | Loss : 0.2889 | Acc : 0.8664
     Batch 035 | Loss : 0.4039 | Acc : 0.8169
     Batch 040 | Loss : 0.3131 | Acc : 0.8523
     Batch 045 | Loss : 0.3487 | Acc : 0.8397
     Batch 050 | Loss : 0.2931 | Acc : 0.8637
     Batch 055 | Loss : 0.2910 | Acc : 0.8682
     Batch 060 | Loss : 0.3429 | Acc : 0.8403
     Batch 065 | Loss : 0.3466 | Acc : 0.8355
     Batch 070 | Loss : 0.2961 | Acc : 0.8669
     Batch 075 | Loss : 0.2994 | Acc : 0.8618
     Batch 080 | Loss : 0.3294 | Acc : 0.8471
     Batch 085 | Loss : 0.2975 | Acc : 0.8628
     Batch 090 | Loss : 0.2870 | Acc : 0.8703
     Batch 095 | Loss : 0.3316 | Acc : 0.8475
     Batch 100 | Loss : 0.3322 | Acc : 0.8436
     Batch 105 | Loss : 0.3197 | Acc : 0.8589
     Batch 110 | Loss : 0.2925 | Acc : 0.8674
     Batch 115 | Loss : 0.3392 | Acc : 0.8409
     Batch 120 | Loss : 0.2918 | Acc : 0.8658
     Batch 125 | Loss : 0.2813 | Acc : 0.8718
     Batch 130 | Loss : 0.3256 | Acc : 0.8459
     Batch 135 | Loss : 0.3416 | Acc : 0.8389
     Batch 140 | Loss : 0.3096 | Acc : 0.8596
     Batch 145 | Loss : 0.2481 | Acc : 0.8921
     Batch 150 | Loss : 0.2629 | Acc : 0.8876
Epoch 00030 | Train Loss : 0.3193 | Eval Loss : 0.3296 | Train acc : 0.8529 | Eval Acc : 0.8468 | Eval Log. Respected : 0.9335
     Batch 000 | Loss : 0.3493 | Acc : 0.8351
     Batch 005 | Loss : 0.2881 | Acc : 0.8660
     Batch 010 | Loss : 0.3061 | Acc : 0.8600
     Batch 015 | Loss : 0.3602 | Acc : 0.8317
     Batch 020 | Loss : 0.3598 | Acc : 0.8364
     Batch 025 | Loss : 0.3551 | Acc : 0.8305
     Batch 030 | Loss : 0.3128 | Acc : 0.8553
     Batch 035 | Loss : 0.2901 | Acc : 0.8657
     Batch 040 | Loss : 0.3145 | Acc : 0.8564
     Batch 045 | Loss : 0.2847 | Acc : 0.8787
     Batch 050 | Loss : 0.2902 | Acc : 0.8666
     Batch 055 | Loss : 0.2669 | Acc : 0.8791
     Batch 060 | Loss : 0.2675 | Acc : 0.8843
     Batch 065 | Loss : 0.3485 | Acc : 0.8414
     Batch 070 | Loss : 0.3354 | Acc : 0.8435
     Batch 075 | Loss : 0.3072 | Acc : 0.8573
     Batch 080 | Loss : 0.2985 | Acc : 0.8659
     Batch 085 | Loss : 0.3378 | Acc : 0.8409
     Batch 090 | Loss : 0.3095 | Acc : 0.8617
     Batch 095 | Loss : 0.3805 | Acc : 0.8233
     Batch 100 | Loss : 0.2726 | Acc : 0.8791
     Batch 105 | Loss : 0.3181 | Acc : 0.8532
     Batch 110 | Loss : 0.2998 | Acc : 0.8614
     Batch 115 | Loss : 0.4067 | Acc : 0.8242
     Batch 120 | Loss : 0.3240 | Acc : 0.8471
     Batch 125 | Loss : 0.2757 | Acc : 0.8733
     Batch 130 | Loss : 0.3614 | Acc : 0.8312
     Batch 135 | Loss : 0.3335 | Acc : 0.8397
     Batch 140 | Loss : 0.2899 | Acc : 0.8723
     Batch 145 | Loss : 0.2901 | Acc : 0.8695
     Batch 150 | Loss : 0.2774 | Acc : 0.8746
Epoch 00031 | Train Loss : 0.3203 | Eval Loss : 0.3290 | Train acc : 0.8523 | Eval Acc : 0.8471 | Eval Log. Respected : 0.9394
     Batch 000 | Loss : 0.2983 | Acc : 0.8587
     Batch 005 | Loss : 0.3661 | Acc : 0.8183
     Batch 010 | Loss : 0.3558 | Acc : 0.8297
     Batch 015 | Loss : 0.2899 | Acc : 0.8709
     Batch 020 | Loss : 0.2817 | Acc : 0.8703
     Batch 025 | Loss : 0.2886 | Acc : 0.8711
     Batch 030 | Loss : 0.3681 | Acc : 0.8249
     Batch 035 | Loss : 0.4080 | Acc : 0.8110
     Batch 040 | Loss : 0.3606 | Acc : 0.8264
     Batch 045 | Loss : 0.3042 | Acc : 0.8615
     Batch 050 | Loss : 0.3028 | Acc : 0.8620
     Batch 055 | Loss : 0.2922 | Acc : 0.8678
     Batch 060 | Loss : 0.2787 | Acc : 0.8778
     Batch 065 | Loss : 0.2807 | Acc : 0.8765
     Batch 070 | Loss : 0.2767 | Acc : 0.8754
     Batch 075 | Loss : 0.3401 | Acc : 0.8395
     Batch 080 | Loss : 0.3929 | Acc : 0.8245
     Batch 085 | Loss : 0.3208 | Acc : 0.8513
     Batch 090 | Loss : 0.3230 | Acc : 0.8510
     Batch 095 | Loss : 0.3447 | Acc : 0.8411
     Batch 100 | Loss : 0.3140 | Acc : 0.8532
     Batch 105 | Loss : 0.2746 | Acc : 0.8755
     Batch 110 | Loss : 0.2485 | Acc : 0.8920
     Batch 115 | Loss : 0.3999 | Acc : 0.8173
     Batch 120 | Loss : 0.3432 | Acc : 0.8389
     Batch 125 | Loss : 0.3408 | Acc : 0.8381
     Batch 130 | Loss : 0.3032 | Acc : 0.8637
     Batch 135 | Loss : 0.3366 | Acc : 0.8408
     Batch 140 | Loss : 0.2942 | Acc : 0.8627
     Batch 145 | Loss : 0.3676 | Acc : 0.8292
     Batch 150 | Loss : 0.3220 | Acc : 0.8465
Epoch 00032 | Train Loss : 0.3196 | Eval Loss : 0.3216 | Train acc : 0.8527 | Eval Acc : 0.8498 | Eval Log. Respected : 0.9223
     Batch 000 | Loss : 0.3281 | Acc : 0.8472
     Batch 005 | Loss : 0.3556 | Acc : 0.8338
     Batch 010 | Loss : 0.3673 | Acc : 0.8346
     Batch 015 | Loss : 0.2773 | Acc : 0.8749
     Batch 020 | Loss : 0.3388 | Acc : 0.8427
     Batch 025 | Loss : 0.2884 | Acc : 0.8656
     Batch 030 | Loss : 0.3209 | Acc : 0.8532
     Batch 035 | Loss : 0.3442 | Acc : 0.8361
     Batch 040 | Loss : 0.3058 | Acc : 0.8584
     Batch 045 | Loss : 0.3249 | Acc : 0.8482
     Batch 050 | Loss : 0.2955 | Acc : 0.8643
     Batch 055 | Loss : 0.2686 | Acc : 0.8828
     Batch 060 | Loss : 0.3302 | Acc : 0.8486
     Batch 065 | Loss : 0.3604 | Acc : 0.8342
     Batch 070 | Loss : 0.3269 | Acc : 0.8483
     Batch 075 | Loss : 0.2831 | Acc : 0.8714
     Batch 080 | Loss : 0.2909 | Acc : 0.8726
     Batch 085 | Loss : 0.3231 | Acc : 0.8447
     Batch 090 | Loss : 0.2881 | Acc : 0.8663
     Batch 095 | Loss : 0.2922 | Acc : 0.8695
     Batch 100 | Loss : 0.3200 | Acc : 0.8533
     Batch 105 | Loss : 0.2875 | Acc : 0.8707
     Batch 110 | Loss : 0.2838 | Acc : 0.8694
     Batch 115 | Loss : 0.2848 | Acc : 0.8673
     Batch 120 | Loss : 0.3009 | Acc : 0.8615
     Batch 125 | Loss : 0.3346 | Acc : 0.8472
     Batch 130 | Loss : 0.3924 | Acc : 0.8151
     Batch 135 | Loss : 0.3748 | Acc : 0.8199
     Batch 140 | Loss : 0.2766 | Acc : 0.8770
     Batch 145 | Loss : 0.2807 | Acc : 0.8666
     Batch 150 | Loss : 0.3265 | Acc : 0.8473
Epoch 00033 | Train Loss : 0.3176 | Eval Loss : 0.3246 | Train acc : 0.8534 | Eval Acc : 0.8470 | Eval Log. Respected : 0.9221
     Batch 000 | Loss : 0.3093 | Acc : 0.8560
     Batch 005 | Loss : 0.3068 | Acc : 0.8583
     Batch 010 | Loss : 0.2653 | Acc : 0.8816
     Batch 015 | Loss : 0.2806 | Acc : 0.8721
     Batch 020 | Loss : 0.3708 | Acc : 0.8219
     Batch 025 | Loss : 0.3317 | Acc : 0.8441
     Batch 030 | Loss : 0.3236 | Acc : 0.8535
     Batch 035 | Loss : 0.3061 | Acc : 0.8562
     Batch 040 | Loss : 0.3198 | Acc : 0.8553
     Batch 045 | Loss : 0.2782 | Acc : 0.8754
     Batch 050 | Loss : 0.3176 | Acc : 0.8535
     Batch 055 | Loss : 0.3414 | Acc : 0.8418
     Batch 060 | Loss : 0.3094 | Acc : 0.8578
     Batch 065 | Loss : 0.3061 | Acc : 0.8569
     Batch 070 | Loss : 0.4275 | Acc : 0.8010
     Batch 075 | Loss : 0.3528 | Acc : 0.8289
     Batch 080 | Loss : 0.3507 | Acc : 0.8422
     Batch 085 | Loss : 0.3951 | Acc : 0.8128
     Batch 090 | Loss : 0.3476 | Acc : 0.8368
     Batch 095 | Loss : 0.3325 | Acc : 0.8473
     Batch 100 | Loss : 0.3107 | Acc : 0.8573
     Batch 105 | Loss : 0.3085 | Acc : 0.8579
     Batch 110 | Loss : 0.3335 | Acc : 0.8424
     Batch 115 | Loss : 0.2948 | Acc : 0.8705
     Batch 120 | Loss : 0.3256 | Acc : 0.8473
     Batch 125 | Loss : 0.2948 | Acc : 0.8677
     Batch 130 | Loss : 0.3214 | Acc : 0.8502
     Batch 135 | Loss : 0.2970 | Acc : 0.8616
     Batch 140 | Loss : 0.3052 | Acc : 0.8578
     Batch 145 | Loss : 0.2834 | Acc : 0.8715
     Batch 150 | Loss : 0.2530 | Acc : 0.8835
Epoch 00034 | Train Loss : 0.3190 | Eval Loss : 0.3191 | Train acc : 0.8532 | Eval Acc : 0.8503 | Eval Log. Respected : 0.9381
     Batch 000 | Loss : 0.2922 | Acc : 0.8652
     Batch 005 | Loss : 0.3016 | Acc : 0.8633
     Batch 010 | Loss : 0.3249 | Acc : 0.8464
     Batch 015 | Loss : 0.2946 | Acc : 0.8654
     Batch 020 | Loss : 0.3575 | Acc : 0.8298
     Batch 025 | Loss : 0.2754 | Acc : 0.8785
     Batch 030 | Loss : 0.2992 | Acc : 0.8636
     Batch 035 | Loss : 0.2675 | Acc : 0.8791
     Batch 040 | Loss : 0.3157 | Acc : 0.8529
     Batch 045 | Loss : 0.2958 | Acc : 0.8620
     Batch 050 | Loss : 0.3306 | Acc : 0.8450
     Batch 055 | Loss : 0.2999 | Acc : 0.8627
     Batch 060 | Loss : 0.2893 | Acc : 0.8690
     Batch 065 | Loss : 0.3398 | Acc : 0.8401
     Batch 070 | Loss : 0.3593 | Acc : 0.8331
     Batch 075 | Loss : 0.3516 | Acc : 0.8363
     Batch 080 | Loss : 0.3278 | Acc : 0.8467
     Batch 085 | Loss : 0.3532 | Acc : 0.8359
     Batch 090 | Loss : 0.2716 | Acc : 0.8770
     Batch 095 | Loss : 0.3817 | Acc : 0.8207
     Batch 100 | Loss : 0.3252 | Acc : 0.8528
     Batch 105 | Loss : 0.2769 | Acc : 0.8769
     Batch 110 | Loss : 0.3342 | Acc : 0.8447
     Batch 115 | Loss : 0.3330 | Acc : 0.8414
     Batch 120 | Loss : 0.3555 | Acc : 0.8274
     Batch 125 | Loss : 0.3473 | Acc : 0.8392
     Batch 130 | Loss : 0.3042 | Acc : 0.8607
     Batch 135 | Loss : 0.3699 | Acc : 0.8242
     Batch 140 | Loss : 0.2952 | Acc : 0.8698
     Batch 145 | Loss : 0.3095 | Acc : 0.8577
     Batch 150 | Loss : 0.4238 | Acc : 0.8112
Epoch 00035 | Train Loss : 0.3186 | Eval Loss : 0.3252 | Train acc : 0.8535 | Eval Acc : 0.8482 | Eval Log. Respected : 0.9347
     Batch 000 | Loss : 0.3082 | Acc : 0.8604
     Batch 005 | Loss : 0.3246 | Acc : 0.8495
     Batch 010 | Loss : 0.2878 | Acc : 0.8686
     Batch 015 | Loss : 0.3433 | Acc : 0.8476
     Batch 020 | Loss : 0.3077 | Acc : 0.8580
     Batch 025 | Loss : 0.3797 | Acc : 0.8171
     Batch 030 | Loss : 0.3120 | Acc : 0.8597
     Batch 035 | Loss : 0.3030 | Acc : 0.8615
     Batch 040 | Loss : 0.3554 | Acc : 0.8320
     Batch 045 | Loss : 0.3484 | Acc : 0.8316
     Batch 050 | Loss : 0.2870 | Acc : 0.8687
     Batch 055 | Loss : 0.2983 | Acc : 0.8606
     Batch 060 | Loss : 0.2832 | Acc : 0.8709
     Batch 065 | Loss : 0.3385 | Acc : 0.8415
     Batch 070 | Loss : 0.2970 | Acc : 0.8660
     Batch 075 | Loss : 0.3161 | Acc : 0.8547
     Batch 080 | Loss : 0.3076 | Acc : 0.8579
     Batch 085 | Loss : 0.2827 | Acc : 0.8731
     Batch 090 | Loss : 0.2839 | Acc : 0.8731
     Batch 095 | Loss : 0.3135 | Acc : 0.8526
     Batch 100 | Loss : 0.3111 | Acc : 0.8546
     Batch 105 | Loss : 0.3200 | Acc : 0.8559
     Batch 110 | Loss : 0.3471 | Acc : 0.8356
     Batch 115 | Loss : 0.2848 | Acc : 0.8713
     Batch 120 | Loss : 0.3326 | Acc : 0.8471
     Batch 125 | Loss : 0.3315 | Acc : 0.8457
     Batch 130 | Loss : 0.3431 | Acc : 0.8394
     Batch 135 | Loss : 0.3557 | Acc : 0.8321
     Batch 140 | Loss : 0.3600 | Acc : 0.8320
     Batch 145 | Loss : 0.2700 | Acc : 0.8769
     Batch 150 | Loss : 0.3105 | Acc : 0.8592
Epoch 00036 | Train Loss : 0.3175 | Eval Loss : 0.3212 | Train acc : 0.8539 | Eval Acc : 0.8497 | Eval Log. Respected : 0.9222
     Batch 000 | Loss : 0.2748 | Acc : 0.8749
     Batch 005 | Loss : 0.3332 | Acc : 0.8432
     Batch 010 | Loss : 0.3147 | Acc : 0.8541
     Batch 015 | Loss : 0.3122 | Acc : 0.8571
     Batch 020 | Loss : 0.3254 | Acc : 0.8462
     Batch 025 | Loss : 0.3217 | Acc : 0.8543
     Batch 030 | Loss : 0.3016 | Acc : 0.8631
     Batch 035 | Loss : 0.3813 | Acc : 0.8229
     Batch 040 | Loss : 0.2992 | Acc : 0.8665
     Batch 045 | Loss : 0.3003 | Acc : 0.8591
     Batch 050 | Loss : 0.3083 | Acc : 0.8569
     Batch 055 | Loss : 0.2866 | Acc : 0.8661
     Batch 060 | Loss : 0.3470 | Acc : 0.8384
     Batch 065 | Loss : 0.3005 | Acc : 0.8571
     Batch 070 | Loss : 0.3182 | Acc : 0.8560
     Batch 075 | Loss : 0.2943 | Acc : 0.8646
     Batch 080 | Loss : 0.3300 | Acc : 0.8465
     Batch 085 | Loss : 0.2852 | Acc : 0.8724
     Batch 090 | Loss : 0.3296 | Acc : 0.8437
     Batch 095 | Loss : 0.3615 | Acc : 0.8256
     Batch 100 | Loss : 0.2783 | Acc : 0.8721
     Batch 105 | Loss : 0.2829 | Acc : 0.8702
     Batch 110 | Loss : 0.3278 | Acc : 0.8497
     Batch 115 | Loss : 0.3227 | Acc : 0.8492
     Batch 120 | Loss : 0.3322 | Acc : 0.8487
     Batch 125 | Loss : 0.3001 | Acc : 0.8608
     Batch 130 | Loss : 0.2760 | Acc : 0.8742
     Batch 135 | Loss : 0.3288 | Acc : 0.8494
     Batch 140 | Loss : 0.3176 | Acc : 0.8502
     Batch 145 | Loss : 0.2992 | Acc : 0.8611
     Batch 150 | Loss : 0.3262 | Acc : 0.8498
Epoch 00037 | Train Loss : 0.3173 | Eval Loss : 0.3200 | Train acc : 0.8539 | Eval Acc : 0.8506 | Eval Log. Respected : 0.9282
     Batch 000 | Loss : 0.3193 | Acc : 0.8536
     Batch 005 | Loss : 0.3285 | Acc : 0.8449
     Batch 010 | Loss : 0.3487 | Acc : 0.8386
     Batch 015 | Loss : 0.2769 | Acc : 0.8752
     Batch 020 | Loss : 0.3209 | Acc : 0.8464
     Batch 025 | Loss : 0.2832 | Acc : 0.8702
     Batch 030 | Loss : 0.3932 | Acc : 0.8141
     Batch 035 | Loss : 0.2905 | Acc : 0.8671
     Batch 040 | Loss : 0.3606 | Acc : 0.8354
     Batch 045 | Loss : 0.3134 | Acc : 0.8550
     Batch 050 | Loss : 0.2788 | Acc : 0.8705
     Batch 055 | Loss : 0.3128 | Acc : 0.8602
     Batch 060 | Loss : 0.2922 | Acc : 0.8628
     Batch 065 | Loss : 0.3457 | Acc : 0.8382
     Batch 070 | Loss : 0.3392 | Acc : 0.8413
     Batch 075 | Loss : 0.2877 | Acc : 0.8678
     Batch 080 | Loss : 0.2978 | Acc : 0.8641
     Batch 085 | Loss : 0.3220 | Acc : 0.8504
     Batch 090 | Loss : 0.3089 | Acc : 0.8578
     Batch 095 | Loss : 0.2883 | Acc : 0.8687
     Batch 100 | Loss : 0.2691 | Acc : 0.8783
     Batch 105 | Loss : 0.2789 | Acc : 0.8737
     Batch 110 | Loss : 0.3181 | Acc : 0.8527
     Batch 115 | Loss : 0.2846 | Acc : 0.8720
     Batch 120 | Loss : 0.3005 | Acc : 0.8611
     Batch 125 | Loss : 0.2829 | Acc : 0.8729
     Batch 130 | Loss : 0.2800 | Acc : 0.8739
     Batch 135 | Loss : 0.2955 | Acc : 0.8672
     Batch 140 | Loss : 0.2726 | Acc : 0.8807
     Batch 145 | Loss : 0.3196 | Acc : 0.8524
     Batch 150 | Loss : 0.2972 | Acc : 0.8632
Epoch 00038 | Train Loss : 0.3150 | Eval Loss : 0.3290 | Train acc : 0.8548 | Eval Acc : 0.8464 | Eval Log. Respected : 0.9298
     Batch 000 | Loss : 0.2831 | Acc : 0.8721
     Batch 005 | Loss : 0.3279 | Acc : 0.8513
     Batch 010 | Loss : 0.2955 | Acc : 0.8671
     Batch 015 | Loss : 0.3527 | Acc : 0.8311
     Batch 020 | Loss : 0.3028 | Acc : 0.8625
     Batch 025 | Loss : 0.3506 | Acc : 0.8343
     Batch 030 | Loss : 0.3186 | Acc : 0.8532
     Batch 035 | Loss : 0.3548 | Acc : 0.8343
     Batch 040 | Loss : 0.3513 | Acc : 0.8296
     Batch 045 | Loss : 0.3381 | Acc : 0.8383
     Batch 050 | Loss : 0.2815 | Acc : 0.8757
     Batch 055 | Loss : 0.3294 | Acc : 0.8443
     Batch 060 | Loss : 0.2792 | Acc : 0.8757
     Batch 065 | Loss : 0.3389 | Acc : 0.8413
     Batch 070 | Loss : 0.3113 | Acc : 0.8527
     Batch 075 | Loss : 0.3105 | Acc : 0.8542
     Batch 080 | Loss : 0.2495 | Acc : 0.8941
     Batch 085 | Loss : 0.3274 | Acc : 0.8464
     Batch 090 | Loss : 0.3096 | Acc : 0.8582
     Batch 095 | Loss : 0.3409 | Acc : 0.8438
     Batch 100 | Loss : 0.3643 | Acc : 0.8267
     Batch 105 | Loss : 0.3369 | Acc : 0.8388
     Batch 110 | Loss : 0.2859 | Acc : 0.8707
     Batch 115 | Loss : 0.4098 | Acc : 0.8172
     Batch 120 | Loss : 0.3096 | Acc : 0.8599
     Batch 125 | Loss : 0.3421 | Acc : 0.8373
     Batch 130 | Loss : 0.3016 | Acc : 0.8587
     Batch 135 | Loss : 0.2967 | Acc : 0.8669
     Batch 140 | Loss : 0.2758 | Acc : 0.8728
     Batch 145 | Loss : 0.3121 | Acc : 0.8527
     Batch 150 | Loss : 0.2866 | Acc : 0.8713
Epoch 00039 | Train Loss : 0.3186 | Eval Loss : 0.3253 | Train acc : 0.8530 | Eval Acc : 0.8481 | Eval Log. Respected : 0.9300
     Batch 000 | Loss : 0.3671 | Acc : 0.8327
     Batch 005 | Loss : 0.3890 | Acc : 0.8124
     Batch 010 | Loss : 0.2853 | Acc : 0.8778
     Batch 015 | Loss : 0.3068 | Acc : 0.8578
     Batch 020 | Loss : 0.3470 | Acc : 0.8374
     Batch 025 | Loss : 0.3468 | Acc : 0.8367
     Batch 030 | Loss : 0.2971 | Acc : 0.8665
     Batch 035 | Loss : 0.2747 | Acc : 0.8769
     Batch 040 | Loss : 0.3333 | Acc : 0.8471
     Batch 045 | Loss : 0.3141 | Acc : 0.8573
     Batch 050 | Loss : 0.3129 | Acc : 0.8549
     Batch 055 | Loss : 0.2803 | Acc : 0.8725
     Batch 060 | Loss : 0.3200 | Acc : 0.8535
     Batch 065 | Loss : 0.2637 | Acc : 0.8827
     Batch 070 | Loss : 0.2918 | Acc : 0.8698
     Batch 075 | Loss : 0.3361 | Acc : 0.8446
     Batch 080 | Loss : 0.3150 | Acc : 0.8564
     Batch 085 | Loss : 0.2800 | Acc : 0.8702
     Batch 090 | Loss : 0.3525 | Acc : 0.8385
     Batch 095 | Loss : 0.3648 | Acc : 0.8347
     Batch 100 | Loss : 0.2937 | Acc : 0.8645
     Batch 105 | Loss : 0.3070 | Acc : 0.8593
     Batch 110 | Loss : 0.2805 | Acc : 0.8729
     Batch 115 | Loss : 0.3158 | Acc : 0.8534
     Batch 120 | Loss : 0.4065 | Acc : 0.8056
     Batch 125 | Loss : 0.3129 | Acc : 0.8522
     Batch 130 | Loss : 0.2915 | Acc : 0.8677
     Batch 135 | Loss : 0.3186 | Acc : 0.8535
     Batch 140 | Loss : 0.3235 | Acc : 0.8493
     Batch 145 | Loss : 0.3392 | Acc : 0.8420
     Batch 150 | Loss : 0.3062 | Acc : 0.8588
Epoch 00040 | Train Loss : 0.3164 | Eval Loss : 0.3275 | Train acc : 0.8543 | Eval Acc : 0.8475 | Eval Log. Respected : 0.9166
     Batch 000 | Loss : 0.3885 | Acc : 0.8147
     Batch 005 | Loss : 0.3102 | Acc : 0.8584
     Batch 010 | Loss : 0.3158 | Acc : 0.8595
     Batch 015 | Loss : 0.3078 | Acc : 0.8581
     Batch 020 | Loss : 0.3423 | Acc : 0.8390
     Batch 025 | Loss : 0.3491 | Acc : 0.8348
     Batch 030 | Loss : 0.2624 | Acc : 0.8825
     Batch 035 | Loss : 0.3143 | Acc : 0.8514
     Batch 040 | Loss : 0.3029 | Acc : 0.8594
     Batch 045 | Loss : 0.3215 | Acc : 0.8518
     Batch 050 | Loss : 0.2924 | Acc : 0.8654
     Batch 055 | Loss : 0.3069 | Acc : 0.8610
     Batch 060 | Loss : 0.2920 | Acc : 0.8740
     Batch 065 | Loss : 0.3714 | Acc : 0.8273
     Batch 070 | Loss : 0.3218 | Acc : 0.8509
     Batch 075 | Loss : 0.2863 | Acc : 0.8696
     Batch 080 | Loss : 0.3457 | Acc : 0.8482
     Batch 085 | Loss : 0.3370 | Acc : 0.8370
     Batch 090 | Loss : 0.3801 | Acc : 0.8219
     Batch 095 | Loss : 0.3037 | Acc : 0.8601
     Batch 100 | Loss : 0.2554 | Acc : 0.8849
     Batch 105 | Loss : 0.3351 | Acc : 0.8413
     Batch 110 | Loss : 0.3060 | Acc : 0.8585
     Batch 115 | Loss : 0.2825 | Acc : 0.8714
     Batch 120 | Loss : 0.3071 | Acc : 0.8601
     Batch 125 | Loss : 0.3119 | Acc : 0.8581
     Batch 130 | Loss : 0.3646 | Acc : 0.8306
     Batch 135 | Loss : 0.2955 | Acc : 0.8625
     Batch 140 | Loss : 0.2653 | Acc : 0.8814
     Batch 145 | Loss : 0.3623 | Acc : 0.8325
     Batch 150 | Loss : 0.3094 | Acc : 0.8602
Epoch 00041 | Train Loss : 0.3160 | Eval Loss : 0.3237 | Train acc : 0.8547 | Eval Acc : 0.8486 | Eval Log. Respected : 0.9230
     Batch 000 | Loss : 0.2960 | Acc : 0.8630
     Batch 005 | Loss : 0.3001 | Acc : 0.8623
     Batch 010 | Loss : 0.2857 | Acc : 0.8702
     Batch 015 | Loss : 0.3894 | Acc : 0.8183
     Batch 020 | Loss : 0.3312 | Acc : 0.8446
     Batch 025 | Loss : 0.3049 | Acc : 0.8609
     Batch 030 | Loss : 0.3457 | Acc : 0.8377
     Batch 035 | Loss : 0.3634 | Acc : 0.8268
     Batch 040 | Loss : 0.2877 | Acc : 0.8653
     Batch 045 | Loss : 0.2925 | Acc : 0.8656
     Batch 050 | Loss : 0.3680 | Acc : 0.8228
     Batch 055 | Loss : 0.3079 | Acc : 0.8572
     Batch 060 | Loss : 0.3925 | Acc : 0.8184
     Batch 065 | Loss : 0.2944 | Acc : 0.8651
     Batch 070 | Loss : 0.2454 | Acc : 0.8913
     Batch 075 | Loss : 0.3013 | Acc : 0.8627
     Batch 080 | Loss : 0.3280 | Acc : 0.8507
     Batch 085 | Loss : 0.3057 | Acc : 0.8568
     Batch 090 | Loss : 0.3689 | Acc : 0.8273
     Batch 095 | Loss : 0.3045 | Acc : 0.8647
     Batch 100 | Loss : 0.3188 | Acc : 0.8517
     Batch 105 | Loss : 0.3279 | Acc : 0.8458
     Batch 110 | Loss : 0.3303 | Acc : 0.8506
     Batch 115 | Loss : 0.3499 | Acc : 0.8407
     Batch 120 | Loss : 0.3028 | Acc : 0.8613
     Batch 125 | Loss : 0.2657 | Acc : 0.8829
     Batch 130 | Loss : 0.2990 | Acc : 0.8634
     Batch 135 | Loss : 0.4069 | Acc : 0.8121
     Batch 140 | Loss : 0.3450 | Acc : 0.8361
     Batch 145 | Loss : 0.2603 | Acc : 0.8841
     Batch 150 | Loss : 0.3616 | Acc : 0.8283
Epoch 00042 | Train Loss : 0.3160 | Eval Loss : 0.3217 | Train acc : 0.8543 | Eval Acc : 0.8494 | Eval Log. Respected : 0.9282
     Batch 000 | Loss : 0.3227 | Acc : 0.8516
     Batch 005 | Loss : 0.2652 | Acc : 0.8803
     Batch 010 | Loss : 0.3076 | Acc : 0.8567
     Batch 015 | Loss : 0.3139 | Acc : 0.8562
     Batch 020 | Loss : 0.3155 | Acc : 0.8559
     Batch 025 | Loss : 0.3159 | Acc : 0.8550
     Batch 030 | Loss : 0.2611 | Acc : 0.8844
     Batch 035 | Loss : 0.3132 | Acc : 0.8547
     Batch 040 | Loss : 0.3287 | Acc : 0.8544
     Batch 045 | Loss : 0.2862 | Acc : 0.8682
     Batch 050 | Loss : 0.3392 | Acc : 0.8439
     Batch 055 | Loss : 0.3468 | Acc : 0.8378
     Batch 060 | Loss : 0.3087 | Acc : 0.8549
     Batch 065 | Loss : 0.3092 | Acc : 0.8582
     Batch 070 | Loss : 0.3034 | Acc : 0.8608
     Batch 075 | Loss : 0.3655 | Acc : 0.8321
     Batch 080 | Loss : 0.2634 | Acc : 0.8839
     Batch 085 | Loss : 0.2874 | Acc : 0.8702
     Batch 090 | Loss : 0.3528 | Acc : 0.8287
     Batch 095 | Loss : 0.3056 | Acc : 0.8599
     Batch 100 | Loss : 0.2761 | Acc : 0.8758
     Batch 105 | Loss : 0.2562 | Acc : 0.8912
     Batch 110 | Loss : 0.3225 | Acc : 0.8508
     Batch 115 | Loss : 0.3344 | Acc : 0.8419
     Batch 120 | Loss : 0.3170 | Acc : 0.8529
     Batch 125 | Loss : 0.3401 | Acc : 0.8381
     Batch 130 | Loss : 0.3171 | Acc : 0.8516
     Batch 135 | Loss : 0.2711 | Acc : 0.8775
     Batch 140 | Loss : 0.3492 | Acc : 0.8345
     Batch 145 | Loss : 0.2925 | Acc : 0.8667
     Batch 150 | Loss : 0.3432 | Acc : 0.8413
Epoch 00043 | Train Loss : 0.3157 | Eval Loss : 0.3279 | Train acc : 0.8546 | Eval Acc : 0.8488 | Eval Log. Respected : 0.9376
     Batch 000 | Loss : 0.2646 | Acc : 0.8787
     Batch 005 | Loss : 0.3137 | Acc : 0.8558
     Batch 010 | Loss : 0.2893 | Acc : 0.8713
     Batch 015 | Loss : 0.3559 | Acc : 0.8319
     Batch 020 | Loss : 0.3118 | Acc : 0.8560
     Batch 025 | Loss : 0.3188 | Acc : 0.8597
     Batch 030 | Loss : 0.3061 | Acc : 0.8617
     Batch 035 | Loss : 0.3011 | Acc : 0.8638
     Batch 040 | Loss : 0.3294 | Acc : 0.8479
     Batch 045 | Loss : 0.3782 | Acc : 0.8235
     Batch 050 | Loss : 0.3164 | Acc : 0.8512
     Batch 055 | Loss : 0.4112 | Acc : 0.8066
     Batch 060 | Loss : 0.3101 | Acc : 0.8570
     Batch 065 | Loss : 0.2910 | Acc : 0.8669
     Batch 070 | Loss : 0.3175 | Acc : 0.8568
     Batch 075 | Loss : 0.2958 | Acc : 0.8640
     Batch 080 | Loss : 0.3514 | Acc : 0.8301
     Batch 085 | Loss : 0.3120 | Acc : 0.8544
     Batch 090 | Loss : 0.3350 | Acc : 0.8424
     Batch 095 | Loss : 0.2813 | Acc : 0.8714
     Batch 100 | Loss : 0.2929 | Acc : 0.8628
     Batch 105 | Loss : 0.3144 | Acc : 0.8589
     Batch 110 | Loss : 0.2918 | Acc : 0.8669
     Batch 115 | Loss : 0.3004 | Acc : 0.8646
     Batch 120 | Loss : 0.2769 | Acc : 0.8766
     Batch 125 | Loss : 0.3068 | Acc : 0.8553
     Batch 130 | Loss : 0.2775 | Acc : 0.8743
     Batch 135 | Loss : 0.2945 | Acc : 0.8622
     Batch 140 | Loss : 0.2946 | Acc : 0.8677
     Batch 145 | Loss : 0.3178 | Acc : 0.8455
     Batch 150 | Loss : 0.3177 | Acc : 0.8526
Epoch 00044 | Train Loss : 0.3151 | Eval Loss : 0.3195 | Train acc : 0.8548 | Eval Acc : 0.8510 | Eval Log. Respected : 0.9293
     Batch 000 | Loss : 0.3238 | Acc : 0.8484
     Batch 005 | Loss : 0.3076 | Acc : 0.8574
     Batch 010 | Loss : 0.3718 | Acc : 0.8264
     Batch 015 | Loss : 0.2893 | Acc : 0.8724
     Batch 020 | Loss : 0.2857 | Acc : 0.8707
     Batch 025 | Loss : 0.3534 | Acc : 0.8379
     Batch 030 | Loss : 0.3060 | Acc : 0.8579
     Batch 035 | Loss : 0.3155 | Acc : 0.8495
     Batch 040 | Loss : 0.3000 | Acc : 0.8631
     Batch 045 | Loss : 0.3501 | Acc : 0.8358
     Batch 050 | Loss : 0.3446 | Acc : 0.8411
     Batch 055 | Loss : 0.3607 | Acc : 0.8313
     Batch 060 | Loss : 0.3762 | Acc : 0.8211
     Batch 065 | Loss : 0.3021 | Acc : 0.8614
     Batch 070 | Loss : 0.3223 | Acc : 0.8487
     Batch 075 | Loss : 0.3488 | Acc : 0.8325
     Batch 080 | Loss : 0.2496 | Acc : 0.8906
     Batch 085 | Loss : 0.2951 | Acc : 0.8635
     Batch 090 | Loss : 0.3015 | Acc : 0.8614
     Batch 095 | Loss : 0.3108 | Acc : 0.8577
     Batch 100 | Loss : 0.3459 | Acc : 0.8376
     Batch 105 | Loss : 0.2944 | Acc : 0.8659
     Batch 110 | Loss : 0.2860 | Acc : 0.8714
     Batch 115 | Loss : 0.3281 | Acc : 0.8474
     Batch 120 | Loss : 0.3279 | Acc : 0.8432
     Batch 125 | Loss : 0.2967 | Acc : 0.8652
     Batch 130 | Loss : 0.2928 | Acc : 0.8646
     Batch 135 | Loss : 0.3074 | Acc : 0.8561
     Batch 140 | Loss : 0.2667 | Acc : 0.8792
     Batch 145 | Loss : 0.2956 | Acc : 0.8627
     Batch 150 | Loss : 0.3460 | Acc : 0.8389
Epoch 00045 | Train Loss : 0.3127 | Eval Loss : 0.3253 | Train acc : 0.8557 | Eval Acc : 0.8482 | Eval Log. Respected : 0.9195
     Batch 000 | Loss : 0.3159 | Acc : 0.8575
     Batch 005 | Loss : 0.3628 | Acc : 0.8301
     Batch 010 | Loss : 0.3161 | Acc : 0.8512
     Batch 015 | Loss : 0.3421 | Acc : 0.8449
     Batch 020 | Loss : 0.2957 | Acc : 0.8657
     Batch 025 | Loss : 0.3259 | Acc : 0.8478
     Batch 030 | Loss : 0.3261 | Acc : 0.8504
     Batch 035 | Loss : 0.3177 | Acc : 0.8496
     Batch 040 | Loss : 0.2898 | Acc : 0.8662
     Batch 045 | Loss : 0.3622 | Acc : 0.8319
     Batch 050 | Loss : 0.3691 | Acc : 0.8351
     Batch 055 | Loss : 0.3514 | Acc : 0.8411
     Batch 060 | Loss : 0.3657 | Acc : 0.8290
     Batch 065 | Loss : 0.3430 | Acc : 0.8432
     Batch 070 | Loss : 0.3087 | Acc : 0.8620
     Batch 075 | Loss : 0.2665 | Acc : 0.8799
     Batch 080 | Loss : 0.3216 | Acc : 0.8501
     Batch 085 | Loss : 0.2882 | Acc : 0.8649
     Batch 090 | Loss : 0.3390 | Acc : 0.8423
     Batch 095 | Loss : 0.3567 | Acc : 0.8291
     Batch 100 | Loss : 0.3058 | Acc : 0.8640
     Batch 105 | Loss : 0.2893 | Acc : 0.8676
     Batch 110 | Loss : 0.3578 | Acc : 0.8409
     Batch 115 | Loss : 0.3029 | Acc : 0.8666
     Batch 120 | Loss : 0.3155 | Acc : 0.8532
     Batch 125 | Loss : 0.2932 | Acc : 0.8677
     Batch 130 | Loss : 0.3263 | Acc : 0.8482
     Batch 135 | Loss : 0.3579 | Acc : 0.8288
     Batch 140 | Loss : 0.2959 | Acc : 0.8639
     Batch 145 | Loss : 0.3238 | Acc : 0.8473
     Batch 150 | Loss : 0.3999 | Acc : 0.8240
Epoch 00046 | Train Loss : 0.3142 | Eval Loss : 0.3278 | Train acc : 0.8551 | Eval Acc : 0.8455 | Eval Log. Respected : 0.9304
     Batch 000 | Loss : 0.2872 | Acc : 0.8675
     Batch 005 | Loss : 0.3067 | Acc : 0.8604
     Batch 010 | Loss : 0.3375 | Acc : 0.8493
     Batch 015 | Loss : 0.3018 | Acc : 0.8606
     Batch 020 | Loss : 0.3494 | Acc : 0.8333
     Batch 025 | Loss : 0.3881 | Acc : 0.8144
     Batch 030 | Loss : 0.2927 | Acc : 0.8656
     Batch 035 | Loss : 0.2803 | Acc : 0.8736
     Batch 040 | Loss : 0.3144 | Acc : 0.8554
     Batch 045 | Loss : 0.3644 | Acc : 0.8353
     Batch 050 | Loss : 0.3008 | Acc : 0.8625
     Batch 055 | Loss : 0.3297 | Acc : 0.8441
     Batch 060 | Loss : 0.3532 | Acc : 0.8303
     Batch 065 | Loss : 0.2906 | Acc : 0.8721
     Batch 070 | Loss : 0.2624 | Acc : 0.8825
     Batch 075 | Loss : 0.2987 | Acc : 0.8615
     Batch 080 | Loss : 0.3033 | Acc : 0.8650
     Batch 085 | Loss : 0.3309 | Acc : 0.8432
     Batch 090 | Loss : 0.3420 | Acc : 0.8400
     Batch 095 | Loss : 0.3188 | Acc : 0.8534
     Batch 100 | Loss : 0.3083 | Acc : 0.8585
     Batch 105 | Loss : 0.2795 | Acc : 0.8760
     Batch 110 | Loss : 0.2830 | Acc : 0.8691
     Batch 115 | Loss : 0.3198 | Acc : 0.8535
     Batch 120 | Loss : 0.3637 | Acc : 0.8294
     Batch 125 | Loss : 0.3013 | Acc : 0.8592
     Batch 130 | Loss : 0.2830 | Acc : 0.8691
     Batch 135 | Loss : 0.2878 | Acc : 0.8710
     Batch 140 | Loss : 0.2585 | Acc : 0.8832
     Batch 145 | Loss : 0.3376 | Acc : 0.8375
     Batch 150 | Loss : 0.2859 | Acc : 0.8746
Epoch 00047 | Train Loss : 0.3147 | Eval Loss : 0.3275 | Train acc : 0.8550 | Eval Acc : 0.8478 | Eval Log. Respected : 0.9189
     Batch 000 | Loss : 0.3189 | Acc : 0.8506
     Batch 005 | Loss : 0.2753 | Acc : 0.8720
     Batch 010 | Loss : 0.2843 | Acc : 0.8708
     Batch 015 | Loss : 0.3281 | Acc : 0.8450
     Batch 020 | Loss : 0.3618 | Acc : 0.8242
     Batch 025 | Loss : 0.3386 | Acc : 0.8379
     Batch 030 | Loss : 0.3788 | Acc : 0.8233
     Batch 035 | Loss : 0.2724 | Acc : 0.8789
     Batch 040 | Loss : 0.3867 | Acc : 0.8265
     Batch 045 | Loss : 0.2813 | Acc : 0.8726
     Batch 050 | Loss : 0.3149 | Acc : 0.8526
     Batch 055 | Loss : 0.3124 | Acc : 0.8588
     Batch 060 | Loss : 0.2532 | Acc : 0.8862
     Batch 065 | Loss : 0.2906 | Acc : 0.8656
     Batch 070 | Loss : 0.2973 | Acc : 0.8633
     Batch 075 | Loss : 0.3085 | Acc : 0.8584
     Batch 080 | Loss : 0.3007 | Acc : 0.8637
     Batch 085 | Loss : 0.3820 | Acc : 0.8210
     Batch 090 | Loss : 0.2836 | Acc : 0.8706
     Batch 095 | Loss : 0.3203 | Acc : 0.8549
     Batch 100 | Loss : 0.2816 | Acc : 0.8772
     Batch 105 | Loss : 0.2829 | Acc : 0.8732
     Batch 110 | Loss : 0.3652 | Acc : 0.8369
     Batch 115 | Loss : 0.3013 | Acc : 0.8571
     Batch 120 | Loss : 0.3633 | Acc : 0.8322
     Batch 125 | Loss : 0.2800 | Acc : 0.8728
     Batch 130 | Loss : 0.2672 | Acc : 0.8787
     Batch 135 | Loss : 0.2770 | Acc : 0.8765
     Batch 140 | Loss : 0.3450 | Acc : 0.8366
     Batch 145 | Loss : 0.3188 | Acc : 0.8511
     Batch 150 | Loss : 0.3800 | Acc : 0.8265
Epoch 00048 | Train Loss : 0.3124 | Eval Loss : 0.3220 | Train acc : 0.8558 | Eval Acc : 0.8488 | Eval Log. Respected : 0.9320
     Batch 000 | Loss : 0.2894 | Acc : 0.8704
     Batch 005 | Loss : 0.3658 | Acc : 0.8291
     Batch 010 | Loss : 0.2934 | Acc : 0.8637
     Batch 015 | Loss : 0.3531 | Acc : 0.8350
     Batch 020 | Loss : 0.3266 | Acc : 0.8448
     Batch 025 | Loss : 0.3282 | Acc : 0.8483
     Batch 030 | Loss : 0.2742 | Acc : 0.8757
     Batch 035 | Loss : 0.3005 | Acc : 0.8664
     Batch 040 | Loss : 0.2579 | Acc : 0.8834
     Batch 045 | Loss : 0.2725 | Acc : 0.8767
     Batch 050 | Loss : 0.2678 | Acc : 0.8773
     Batch 055 | Loss : 0.3094 | Acc : 0.8575
     Batch 060 | Loss : 0.3346 | Acc : 0.8422
     Batch 065 | Loss : 0.2964 | Acc : 0.8651
     Batch 070 | Loss : 0.3296 | Acc : 0.8427
     Batch 075 | Loss : 0.2566 | Acc : 0.8848
     Batch 080 | Loss : 0.3219 | Acc : 0.8486
     Batch 085 | Loss : 0.3913 | Acc : 0.8206
     Batch 090 | Loss : 0.3093 | Acc : 0.8541
     Batch 095 | Loss : 0.3076 | Acc : 0.8572
     Batch 100 | Loss : 0.3750 | Acc : 0.8204
     Batch 105 | Loss : 0.3007 | Acc : 0.8586
     Batch 110 | Loss : 0.3608 | Acc : 0.8332
     Batch 115 | Loss : 0.2622 | Acc : 0.8822
     Batch 120 | Loss : 0.3045 | Acc : 0.8645
     Batch 125 | Loss : 0.2686 | Acc : 0.8815
     Batch 130 | Loss : 0.2660 | Acc : 0.8789
     Batch 135 | Loss : 0.3390 | Acc : 0.8405
     Batch 140 | Loss : 0.2819 | Acc : 0.8682
     Batch 145 | Loss : 0.3060 | Acc : 0.8565
     Batch 150 | Loss : 0.2990 | Acc : 0.8630
Epoch 00049 | Train Loss : 0.3118 | Eval Loss : 0.3173 | Train acc : 0.8561 | Eval Acc : 0.8515 | Eval Log. Respected : 0.9276
     Batch 000 | Loss : 0.3029 | Acc : 0.8594
     Batch 005 | Loss : 0.2763 | Acc : 0.8740
     Batch 010 | Loss : 0.3403 | Acc : 0.8443
     Batch 015 | Loss : 0.2688 | Acc : 0.8792
     Batch 020 | Loss : 0.3488 | Acc : 0.8345
     Batch 025 | Loss : 0.3389 | Acc : 0.8442
     Batch 030 | Loss : 0.2574 | Acc : 0.8873
     Batch 035 | Loss : 0.3767 | Acc : 0.8286
     Batch 040 | Loss : 0.3359 | Acc : 0.8359
     Batch 045 | Loss : 0.3222 | Acc : 0.8523
     Batch 050 | Loss : 0.2812 | Acc : 0.8707
     Batch 055 | Loss : 0.3655 | Acc : 0.8302
     Batch 060 | Loss : 0.2861 | Acc : 0.8717
     Batch 065 | Loss : 0.2539 | Acc : 0.8861
     Batch 070 | Loss : 0.3567 | Acc : 0.8289
     Batch 075 | Loss : 0.2900 | Acc : 0.8687
     Batch 080 | Loss : 0.2980 | Acc : 0.8630
     Batch 085 | Loss : 0.3379 | Acc : 0.8367
     Batch 090 | Loss : 0.3200 | Acc : 0.8518
     Batch 095 | Loss : 0.3208 | Acc : 0.8511
     Batch 100 | Loss : 0.3177 | Acc : 0.8494
     Batch 105 | Loss : 0.3246 | Acc : 0.8476
     Batch 110 | Loss : 0.2703 | Acc : 0.8773
     Batch 115 | Loss : 0.2794 | Acc : 0.8746
     Batch 120 | Loss : 0.3140 | Acc : 0.8600
     Batch 125 | Loss : 0.3347 | Acc : 0.8467
     Batch 130 | Loss : 0.2811 | Acc : 0.8720
     Batch 135 | Loss : 0.2993 | Acc : 0.8636
     Batch 140 | Loss : 0.3204 | Acc : 0.8500
     Batch 145 | Loss : 0.2997 | Acc : 0.8637
     Batch 150 | Loss : 0.2880 | Acc : 0.8696
Epoch 00050 | Train Loss : 0.3135 | Eval Loss : 0.3179 | Train acc : 0.8555 | Eval Acc : 0.8511 | Eval Log. Respected : 0.9403
     Batch 000 | Loss : 0.3617 | Acc : 0.8323
     Batch 005 | Loss : 0.3194 | Acc : 0.8494
     Batch 010 | Loss : 0.2811 | Acc : 0.8733
     Batch 015 | Loss : 0.2690 | Acc : 0.8785
     Batch 020 | Loss : 0.3207 | Acc : 0.8539
     Batch 025 | Loss : 0.2682 | Acc : 0.8770
     Batch 030 | Loss : 0.3394 | Acc : 0.8395
     Batch 035 | Loss : 0.3942 | Acc : 0.8136
     Batch 040 | Loss : 0.3898 | Acc : 0.8142
     Batch 045 | Loss : 0.2867 | Acc : 0.8700
     Batch 050 | Loss : 0.3058 | Acc : 0.8559
     Batch 055 | Loss : 0.3062 | Acc : 0.8583
     Batch 060 | Loss : 0.2688 | Acc : 0.8769
     Batch 065 | Loss : 0.3276 | Acc : 0.8443
     Batch 070 | Loss : 0.3082 | Acc : 0.8600
     Batch 075 | Loss : 0.2777 | Acc : 0.8769
     Batch 080 | Loss : 0.3315 | Acc : 0.8507
     Batch 085 | Loss : 0.2969 | Acc : 0.8593
     Batch 090 | Loss : 0.3021 | Acc : 0.8625
     Batch 095 | Loss : 0.2957 | Acc : 0.8606
     Batch 100 | Loss : 0.3765 | Acc : 0.8197
     Batch 105 | Loss : 0.2883 | Acc : 0.8699
     Batch 110 | Loss : 0.2974 | Acc : 0.8662
     Batch 115 | Loss : 0.2720 | Acc : 0.8751
     Batch 120 | Loss : 0.3512 | Acc : 0.8347
     Batch 125 | Loss : 0.2925 | Acc : 0.8686
     Batch 130 | Loss : 0.3067 | Acc : 0.8597
     Batch 135 | Loss : 0.3340 | Acc : 0.8463
     Batch 140 | Loss : 0.3067 | Acc : 0.8557
     Batch 145 | Loss : 0.3138 | Acc : 0.8513
     Batch 150 | Loss : 0.2750 | Acc : 0.8763
Epoch 00051 | Train Loss : 0.3135 | Eval Loss : 0.3207 | Train acc : 0.8554 | Eval Acc : 0.8503 | Eval Log. Respected : 0.9363
     Batch 000 | Loss : 0.3178 | Acc : 0.8525
     Batch 005 | Loss : 0.2894 | Acc : 0.8701
     Batch 010 | Loss : 0.3320 | Acc : 0.8447
     Batch 015 | Loss : 0.3012 | Acc : 0.8628
     Batch 020 | Loss : 0.3195 | Acc : 0.8520
     Batch 025 | Loss : 0.3623 | Acc : 0.8342
     Batch 030 | Loss : 0.3495 | Acc : 0.8318
     Batch 035 | Loss : 0.3248 | Acc : 0.8508
     Batch 040 | Loss : 0.2984 | Acc : 0.8644
     Batch 045 | Loss : 0.3533 | Acc : 0.8328
     Batch 050 | Loss : 0.3266 | Acc : 0.8512
     Batch 055 | Loss : 0.3209 | Acc : 0.8521
     Batch 060 | Loss : 0.2879 | Acc : 0.8713
     Batch 065 | Loss : 0.3181 | Acc : 0.8505
     Batch 070 | Loss : 0.3028 | Acc : 0.8622
     Batch 075 | Loss : 0.3017 | Acc : 0.8606
     Batch 080 | Loss : 0.2853 | Acc : 0.8680
     Batch 085 | Loss : 0.3124 | Acc : 0.8561
     Batch 090 | Loss : 0.2514 | Acc : 0.8842
     Batch 095 | Loss : 0.2970 | Acc : 0.8601
     Batch 100 | Loss : 0.2909 | Acc : 0.8637
     Batch 105 | Loss : 0.2795 | Acc : 0.8722
     Batch 110 | Loss : 0.3499 | Acc : 0.8379
     Batch 115 | Loss : 0.3319 | Acc : 0.8431
     Batch 120 | Loss : 0.2906 | Acc : 0.8668
     Batch 125 | Loss : 0.3291 | Acc : 0.8444
     Batch 130 | Loss : 0.3594 | Acc : 0.8281
     Batch 135 | Loss : 0.3909 | Acc : 0.8125
     Batch 140 | Loss : 0.3068 | Acc : 0.8607
     Batch 145 | Loss : 0.3010 | Acc : 0.8622
     Batch 150 | Loss : 0.2885 | Acc : 0.8670
Epoch 00052 | Train Loss : 0.3129 | Eval Loss : 0.3187 | Train acc : 0.8559 | Eval Acc : 0.8511 | Eval Log. Respected : 0.9285
     Batch 000 | Loss : 0.2884 | Acc : 0.8659
     Batch 005 | Loss : 0.2799 | Acc : 0.8778
     Batch 010 | Loss : 0.2992 | Acc : 0.8631
     Batch 015 | Loss : 0.3265 | Acc : 0.8503
     Batch 020 | Loss : 0.2558 | Acc : 0.8833
     Batch 025 | Loss : 0.3210 | Acc : 0.8489
     Batch 030 | Loss : 0.3121 | Acc : 0.8587
     Batch 035 | Loss : 0.3341 | Acc : 0.8421
     Batch 040 | Loss : 0.3006 | Acc : 0.8599
     Batch 045 | Loss : 0.2960 | Acc : 0.8661
     Batch 050 | Loss : 0.2990 | Acc : 0.8622
     Batch 055 | Loss : 0.2668 | Acc : 0.8791
     Batch 060 | Loss : 0.4705 | Acc : 0.7876
     Batch 065 | Loss : 0.2996 | Acc : 0.8675
     Batch 070 | Loss : 0.3234 | Acc : 0.8518
     Batch 075 | Loss : 0.2967 | Acc : 0.8653
     Batch 080 | Loss : 0.2961 | Acc : 0.8643
     Batch 085 | Loss : 0.3086 | Acc : 0.8592
     Batch 090 | Loss : 0.2575 | Acc : 0.8905
     Batch 095 | Loss : 0.2957 | Acc : 0.8655
     Batch 100 | Loss : 0.2754 | Acc : 0.8772
     Batch 105 | Loss : 0.2937 | Acc : 0.8650
     Batch 110 | Loss : 0.3754 | Acc : 0.8238
     Batch 115 | Loss : 0.2976 | Acc : 0.8625
     Batch 120 | Loss : 0.3498 | Acc : 0.8397
     Batch 125 | Loss : 0.3201 | Acc : 0.8497
     Batch 130 | Loss : 0.3252 | Acc : 0.8471
     Batch 135 | Loss : 0.3283 | Acc : 0.8493
     Batch 140 | Loss : 0.3440 | Acc : 0.8408
     Batch 145 | Loss : 0.3183 | Acc : 0.8478
     Batch 150 | Loss : 0.3354 | Acc : 0.8462
Epoch 00053 | Train Loss : 0.3132 | Eval Loss : 0.3198 | Train acc : 0.8559 | Eval Acc : 0.8510 | Eval Log. Respected : 0.9303
     Batch 000 | Loss : 0.3180 | Acc : 0.8539
     Batch 005 | Loss : 0.2906 | Acc : 0.8639
     Batch 010 | Loss : 0.2782 | Acc : 0.8734
     Batch 015 | Loss : 0.3252 | Acc : 0.8472
     Batch 020 | Loss : 0.3286 | Acc : 0.8441
     Batch 025 | Loss : 0.3865 | Acc : 0.8213
     Batch 030 | Loss : 0.2785 | Acc : 0.8735
     Batch 035 | Loss : 0.2940 | Acc : 0.8661
     Batch 040 | Loss : 0.3130 | Acc : 0.8550
     Batch 045 | Loss : 0.3321 | Acc : 0.8489
     Batch 050 | Loss : 0.3007 | Acc : 0.8623
     Batch 055 | Loss : 0.2752 | Acc : 0.8730
     Batch 060 | Loss : 0.2854 | Acc : 0.8701
     Batch 065 | Loss : 0.3368 | Acc : 0.8468
     Batch 070 | Loss : 0.2949 | Acc : 0.8661
     Batch 075 | Loss : 0.2890 | Acc : 0.8676
     Batch 080 | Loss : 0.3162 | Acc : 0.8559
     Batch 085 | Loss : 0.3042 | Acc : 0.8595
     Batch 090 | Loss : 0.2742 | Acc : 0.8783
     Batch 095 | Loss : 0.3389 | Acc : 0.8438
     Batch 100 | Loss : 0.3390 | Acc : 0.8449
     Batch 105 | Loss : 0.2803 | Acc : 0.8756
     Batch 110 | Loss : 0.2872 | Acc : 0.8699
     Batch 115 | Loss : 0.3147 | Acc : 0.8547
     Batch 120 | Loss : 0.2981 | Acc : 0.8657
     Batch 125 | Loss : 0.3051 | Acc : 0.8570
     Batch 130 | Loss : 0.2815 | Acc : 0.8717
     Batch 135 | Loss : 0.3038 | Acc : 0.8626
     Batch 140 | Loss : 0.2941 | Acc : 0.8647
     Batch 145 | Loss : 0.2853 | Acc : 0.8732
     Batch 150 | Loss : 0.2633 | Acc : 0.8786
Epoch 00054 | Train Loss : 0.3113 | Eval Loss : 0.3222 | Train acc : 0.8566 | Eval Acc : 0.8507 | Eval Log. Respected : 0.9326
     Batch 000 | Loss : 0.3055 | Acc : 0.8557
     Batch 005 | Loss : 0.2659 | Acc : 0.8800
     Batch 010 | Loss : 0.3032 | Acc : 0.8578
     Batch 015 | Loss : 0.3112 | Acc : 0.8543
     Batch 020 | Loss : 0.3244 | Acc : 0.8493
     Batch 025 | Loss : 0.3178 | Acc : 0.8574
     Batch 030 | Loss : 0.3025 | Acc : 0.8619
     Batch 035 | Loss : 0.2730 | Acc : 0.8709
     Batch 040 | Loss : 0.3028 | Acc : 0.8543
     Batch 045 | Loss : 0.3151 | Acc : 0.8528
     Batch 050 | Loss : 0.3055 | Acc : 0.8589
     Batch 055 | Loss : 0.3295 | Acc : 0.8454
     Batch 060 | Loss : 0.2782 | Acc : 0.8748
     Batch 065 | Loss : 0.2581 | Acc : 0.8800
     Batch 070 | Loss : 0.2870 | Acc : 0.8702
     Batch 075 | Loss : 0.3270 | Acc : 0.8515
     Batch 080 | Loss : 0.3571 | Acc : 0.8294
     Batch 085 | Loss : 0.3993 | Acc : 0.8155
     Batch 090 | Loss : 0.2944 | Acc : 0.8661
     Batch 095 | Loss : 0.2976 | Acc : 0.8618
     Batch 100 | Loss : 0.3201 | Acc : 0.8484
     Batch 105 | Loss : 0.2962 | Acc : 0.8658
     Batch 110 | Loss : 0.3111 | Acc : 0.8562
     Batch 115 | Loss : 0.3310 | Acc : 0.8445
     Batch 120 | Loss : 0.2598 | Acc : 0.8851
     Batch 125 | Loss : 0.3018 | Acc : 0.8641
     Batch 130 | Loss : 0.3531 | Acc : 0.8355
     Batch 135 | Loss : 0.2940 | Acc : 0.8629
     Batch 140 | Loss : 0.3105 | Acc : 0.8550
     Batch 145 | Loss : 0.2939 | Acc : 0.8626
     Batch 150 | Loss : 0.2663 | Acc : 0.8818
Epoch 00055 | Train Loss : 0.3113 | Eval Loss : 0.3236 | Train acc : 0.8566 | Eval Acc : 0.8498 | Eval Log. Respected : 0.9292
     Batch 000 | Loss : 0.3343 | Acc : 0.8426
     Batch 005 | Loss : 0.3350 | Acc : 0.8407
     Batch 010 | Loss : 0.2510 | Acc : 0.8878
     Batch 015 | Loss : 0.2807 | Acc : 0.8714
     Batch 020 | Loss : 0.3306 | Acc : 0.8493
     Batch 025 | Loss : 0.2938 | Acc : 0.8660
     Batch 030 | Loss : 0.3202 | Acc : 0.8534
     Batch 035 | Loss : 0.3092 | Acc : 0.8567
     Batch 040 | Loss : 0.3370 | Acc : 0.8403
     Batch 045 | Loss : 0.2769 | Acc : 0.8714
     Batch 050 | Loss : 0.2877 | Acc : 0.8677
     Batch 055 | Loss : 0.3547 | Acc : 0.8295
     Batch 060 | Loss : 0.2995 | Acc : 0.8611
     Batch 065 | Loss : 0.3294 | Acc : 0.8443
     Batch 070 | Loss : 0.2908 | Acc : 0.8668
     Batch 075 | Loss : 0.2875 | Acc : 0.8684
     Batch 080 | Loss : 0.2734 | Acc : 0.8724
     Batch 085 | Loss : 0.3284 | Acc : 0.8466
     Batch 090 | Loss : 0.2752 | Acc : 0.8838
     Batch 095 | Loss : 0.2845 | Acc : 0.8708
     Batch 100 | Loss : 0.2818 | Acc : 0.8749
     Batch 105 | Loss : 0.3501 | Acc : 0.8310
     Batch 110 | Loss : 0.3292 | Acc : 0.8464
     Batch 115 | Loss : 0.3276 | Acc : 0.8511
     Batch 120 | Loss : 0.3045 | Acc : 0.8591
     Batch 125 | Loss : 0.3285 | Acc : 0.8425
     Batch 130 | Loss : 0.2945 | Acc : 0.8648
     Batch 135 | Loss : 0.3686 | Acc : 0.8347
     Batch 140 | Loss : 0.3020 | Acc : 0.8594
     Batch 145 | Loss : 0.3274 | Acc : 0.8460
     Batch 150 | Loss : 0.2895 | Acc : 0.8668
Epoch 00056 | Train Loss : 0.3108 | Eval Loss : 0.3228 | Train acc : 0.8572 | Eval Acc : 0.8507 | Eval Log. Respected : 0.9198
     Batch 000 | Loss : 0.3050 | Acc : 0.8587
     Batch 005 | Loss : 0.3283 | Acc : 0.8435
     Batch 010 | Loss : 0.3190 | Acc : 0.8504
     Batch 015 | Loss : 0.2574 | Acc : 0.8827
     Batch 020 | Loss : 0.3097 | Acc : 0.8608
     Batch 025 | Loss : 0.3272 | Acc : 0.8456
     Batch 030 | Loss : 0.2917 | Acc : 0.8715
     Batch 035 | Loss : 0.2782 | Acc : 0.8739
     Batch 040 | Loss : 0.3437 | Acc : 0.8393
     Batch 045 | Loss : 0.2530 | Acc : 0.8873
     Batch 050 | Loss : 0.2872 | Acc : 0.8669
     Batch 055 | Loss : 0.3127 | Acc : 0.8540
     Batch 060 | Loss : 0.3034 | Acc : 0.8628
     Batch 065 | Loss : 0.2901 | Acc : 0.8647
     Batch 070 | Loss : 0.2752 | Acc : 0.8766
     Batch 075 | Loss : 0.2437 | Acc : 0.8911
     Batch 080 | Loss : 0.2904 | Acc : 0.8652
     Batch 085 | Loss : 0.3560 | Acc : 0.8324
     Batch 090 | Loss : 0.3379 | Acc : 0.8383
     Batch 095 | Loss : 0.2925 | Acc : 0.8649
     Batch 100 | Loss : 0.3446 | Acc : 0.8398
     Batch 105 | Loss : 0.3057 | Acc : 0.8613
     Batch 110 | Loss : 0.3663 | Acc : 0.8275
     Batch 115 | Loss : 0.2834 | Acc : 0.8702
     Batch 120 | Loss : 0.3002 | Acc : 0.8651
     Batch 125 | Loss : 0.3104 | Acc : 0.8548
     Batch 130 | Loss : 0.3823 | Acc : 0.8248
     Batch 135 | Loss : 0.2828 | Acc : 0.8748
     Batch 140 | Loss : 0.2767 | Acc : 0.8731
     Batch 145 | Loss : 0.3308 | Acc : 0.8500
     Batch 150 | Loss : 0.3173 | Acc : 0.8540
Epoch 00057 | Train Loss : 0.3108 | Eval Loss : 0.3184 | Train acc : 0.8570 | Eval Acc : 0.8514 | Eval Log. Respected : 0.9314
     Batch 000 | Loss : 0.3065 | Acc : 0.8608
     Batch 005 | Loss : 0.2874 | Acc : 0.8673
     Batch 010 | Loss : 0.3098 | Acc : 0.8539
     Batch 015 | Loss : 0.3084 | Acc : 0.8545
     Batch 020 | Loss : 0.3437 | Acc : 0.8377
     Batch 025 | Loss : 0.3008 | Acc : 0.8646
     Batch 030 | Loss : 0.3243 | Acc : 0.8550
     Batch 035 | Loss : 0.3221 | Acc : 0.8500
     Batch 040 | Loss : 0.3116 | Acc : 0.8559
     Batch 045 | Loss : 0.2767 | Acc : 0.8733
     Batch 050 | Loss : 0.3077 | Acc : 0.8589
     Batch 055 | Loss : 0.3639 | Acc : 0.8282
     Batch 060 | Loss : 0.3203 | Acc : 0.8514
     Batch 065 | Loss : 0.2940 | Acc : 0.8647
     Batch 070 | Loss : 0.3516 | Acc : 0.8401
     Batch 075 | Loss : 0.2946 | Acc : 0.8646
     Batch 080 | Loss : 0.2633 | Acc : 0.8840
     Batch 085 | Loss : 0.2854 | Acc : 0.8697
     Batch 090 | Loss : 0.3603 | Acc : 0.8382
     Batch 095 | Loss : 0.2675 | Acc : 0.8816
     Batch 100 | Loss : 0.3564 | Acc : 0.8351
     Batch 105 | Loss : 0.3598 | Acc : 0.8337
     Batch 110 | Loss : 0.3301 | Acc : 0.8434
     Batch 115 | Loss : 0.3181 | Acc : 0.8487
     Batch 120 | Loss : 0.2919 | Acc : 0.8664
     Batch 125 | Loss : 0.3266 | Acc : 0.8489
     Batch 130 | Loss : 0.3051 | Acc : 0.8557
     Batch 135 | Loss : 0.3157 | Acc : 0.8500
     Batch 140 | Loss : 0.3239 | Acc : 0.8514
     Batch 145 | Loss : 0.3257 | Acc : 0.8503
     Batch 150 | Loss : 0.3414 | Acc : 0.8376
Epoch 00058 | Train Loss : 0.3099 | Eval Loss : 0.3210 | Train acc : 0.8571 | Eval Acc : 0.8510 | Eval Log. Respected : 0.9407
     Batch 000 | Loss : 0.2974 | Acc : 0.8641
     Batch 005 | Loss : 0.2832 | Acc : 0.8686
     Batch 010 | Loss : 0.3959 | Acc : 0.8188
     Batch 015 | Loss : 0.3216 | Acc : 0.8487
     Batch 020 | Loss : 0.2777 | Acc : 0.8777
     Batch 025 | Loss : 0.3815 | Acc : 0.8152
     Batch 030 | Loss : 0.3007 | Acc : 0.8611
     Batch 035 | Loss : 0.2656 | Acc : 0.8803
     Batch 040 | Loss : 0.3090 | Acc : 0.8608
     Batch 045 | Loss : 0.3840 | Acc : 0.8237
     Batch 050 | Loss : 0.2971 | Acc : 0.8638
     Batch 055 | Loss : 0.2739 | Acc : 0.8722
     Batch 060 | Loss : 0.2724 | Acc : 0.8789
     Batch 065 | Loss : 0.2852 | Acc : 0.8696
     Batch 070 | Loss : 0.3583 | Acc : 0.8312
     Batch 075 | Loss : 0.3247 | Acc : 0.8458
     Batch 080 | Loss : 0.2750 | Acc : 0.8789
     Batch 085 | Loss : 0.2668 | Acc : 0.8774
     Batch 090 | Loss : 0.3335 | Acc : 0.8433
     Batch 095 | Loss : 0.2769 | Acc : 0.8756
     Batch 100 | Loss : 0.3398 | Acc : 0.8483
     Batch 105 | Loss : 0.3520 | Acc : 0.8344
     Batch 110 | Loss : 0.3334 | Acc : 0.8426
     Batch 115 | Loss : 0.3102 | Acc : 0.8582
     Batch 120 | Loss : 0.2790 | Acc : 0.8738
     Batch 125 | Loss : 0.3168 | Acc : 0.8519
     Batch 130 | Loss : 0.2571 | Acc : 0.8914
     Batch 135 | Loss : 0.3174 | Acc : 0.8521
     Batch 140 | Loss : 0.3036 | Acc : 0.8583
     Batch 145 | Loss : 0.3395 | Acc : 0.8397
     Batch 150 | Loss : 0.2664 | Acc : 0.8811
Epoch 00059 | Train Loss : 0.3096 | Eval Loss : 0.3226 | Train acc : 0.8572 | Eval Acc : 0.8494 | Eval Log. Respected : 0.9422
     Batch 000 | Loss : 0.3283 | Acc : 0.8485
     Batch 005 | Loss : 0.3447 | Acc : 0.8476
     Batch 010 | Loss : 0.3524 | Acc : 0.8331
     Batch 015 | Loss : 0.3106 | Acc : 0.8544
     Batch 020 | Loss : 0.3582 | Acc : 0.8327
     Batch 025 | Loss : 0.2850 | Acc : 0.8712
     Batch 030 | Loss : 0.3373 | Acc : 0.8387
     Batch 035 | Loss : 0.3151 | Acc : 0.8542
     Batch 040 | Loss : 0.3243 | Acc : 0.8464
     Batch 045 | Loss : 0.3110 | Acc : 0.8539
     Batch 050 | Loss : 0.3442 | Acc : 0.8471
     Batch 055 | Loss : 0.3292 | Acc : 0.8459
     Batch 060 | Loss : 0.3147 | Acc : 0.8529
     Batch 065 | Loss : 0.3354 | Acc : 0.8437
     Batch 070 | Loss : 0.3311 | Acc : 0.8449
     Batch 075 | Loss : 0.3023 | Acc : 0.8616
     Batch 080 | Loss : 0.3486 | Acc : 0.8338
     Batch 085 | Loss : 0.2839 | Acc : 0.8686
     Batch 090 | Loss : 0.3068 | Acc : 0.8564
     Batch 095 | Loss : 0.3193 | Acc : 0.8525
     Batch 100 | Loss : 0.3317 | Acc : 0.8459
     Batch 105 | Loss : 0.3319 | Acc : 0.8442
     Batch 110 | Loss : 0.3308 | Acc : 0.8443
     Batch 115 | Loss : 0.3609 | Acc : 0.8256
     Batch 120 | Loss : 0.2747 | Acc : 0.8746
     Batch 125 | Loss : 0.3215 | Acc : 0.8511
     Batch 130 | Loss : 0.2994 | Acc : 0.8620
     Batch 135 | Loss : 0.3484 | Acc : 0.8362
     Batch 140 | Loss : 0.3338 | Acc : 0.8422
     Batch 145 | Loss : 0.3383 | Acc : 0.8376
     Batch 150 | Loss : 0.2928 | Acc : 0.8647
Epoch 00060 | Train Loss : 0.3095 | Eval Loss : 0.3195 | Train acc : 0.8574 | Eval Acc : 0.8520 | Eval Log. Respected : 0.9414
     Batch 000 | Loss : 0.3204 | Acc : 0.8518
     Batch 005 | Loss : 0.2624 | Acc : 0.8798
     Batch 010 | Loss : 0.3433 | Acc : 0.8386
     Batch 015 | Loss : 0.2808 | Acc : 0.8731
     Batch 020 | Loss : 0.3207 | Acc : 0.8508
     Batch 025 | Loss : 0.3679 | Acc : 0.8260
     Batch 030 | Loss : 0.3336 | Acc : 0.8435
     Batch 035 | Loss : 0.3305 | Acc : 0.8454
     Batch 040 | Loss : 0.2719 | Acc : 0.8728
     Batch 045 | Loss : 0.3243 | Acc : 0.8471
     Batch 050 | Loss : 0.3625 | Acc : 0.8279
     Batch 055 | Loss : 0.3281 | Acc : 0.8490
     Batch 060 | Loss : 0.2554 | Acc : 0.8872
     Batch 065 | Loss : 0.3323 | Acc : 0.8461
     Batch 070 | Loss : 0.4129 | Acc : 0.8064
     Batch 075 | Loss : 0.3497 | Acc : 0.8353
     Batch 080 | Loss : 0.3490 | Acc : 0.8349
     Batch 085 | Loss : 0.3075 | Acc : 0.8589
     Batch 090 | Loss : 0.3838 | Acc : 0.8185
     Batch 095 | Loss : 0.3324 | Acc : 0.8481
     Batch 100 | Loss : 0.2645 | Acc : 0.8788
     Batch 105 | Loss : 0.3184 | Acc : 0.8524
     Batch 110 | Loss : 0.2895 | Acc : 0.8706
     Batch 115 | Loss : 0.2670 | Acc : 0.8813
     Batch 120 | Loss : 0.3178 | Acc : 0.8514
     Batch 125 | Loss : 0.3524 | Acc : 0.8341
     Batch 130 | Loss : 0.2756 | Acc : 0.8729
     Batch 135 | Loss : 0.2485 | Acc : 0.8896
     Batch 140 | Loss : 0.2778 | Acc : 0.8754
     Batch 145 | Loss : 0.3038 | Acc : 0.8617
     Batch 150 | Loss : 0.2839 | Acc : 0.8733
Epoch 00061 | Train Loss : 0.3094 | Eval Loss : 0.3255 | Train acc : 0.8574 | Eval Acc : 0.8496 | Eval Log. Respected : 0.9466
     Batch 000 | Loss : 0.2810 | Acc : 0.8699
     Batch 005 | Loss : 0.2589 | Acc : 0.8832
     Batch 010 | Loss : 0.2913 | Acc : 0.8640
     Batch 015 | Loss : 0.3070 | Acc : 0.8613
     Batch 020 | Loss : 0.2755 | Acc : 0.8739
     Batch 025 | Loss : 0.3204 | Acc : 0.8493
     Batch 030 | Loss : 0.2776 | Acc : 0.8703
     Batch 035 | Loss : 0.3415 | Acc : 0.8356
     Batch 040 | Loss : 0.3114 | Acc : 0.8544
     Batch 045 | Loss : 0.3350 | Acc : 0.8470
     Batch 050 | Loss : 0.3139 | Acc : 0.8574
     Batch 055 | Loss : 0.2872 | Acc : 0.8696
     Batch 060 | Loss : 0.3106 | Acc : 0.8536
     Batch 065 | Loss : 0.3436 | Acc : 0.8364
     Batch 070 | Loss : 0.2645 | Acc : 0.8821
     Batch 075 | Loss : 0.3265 | Acc : 0.8497
     Batch 080 | Loss : 0.2742 | Acc : 0.8758
     Batch 085 | Loss : 0.2839 | Acc : 0.8776
     Batch 090 | Loss : 0.4314 | Acc : 0.8089
     Batch 095 | Loss : 0.3556 | Acc : 0.8343
     Batch 100 | Loss : 0.2751 | Acc : 0.8738
     Batch 105 | Loss : 0.3245 | Acc : 0.8594
     Batch 110 | Loss : 0.2957 | Acc : 0.8679
     Batch 115 | Loss : 0.3036 | Acc : 0.8614
     Batch 120 | Loss : 0.3633 | Acc : 0.8261
     Batch 125 | Loss : 0.3111 | Acc : 0.8609
     Batch 130 | Loss : 0.2850 | Acc : 0.8742
     Batch 135 | Loss : 0.3081 | Acc : 0.8584
     Batch 140 | Loss : 0.3319 | Acc : 0.8457
     Batch 145 | Loss : 0.2811 | Acc : 0.8756
     Batch 150 | Loss : 0.3161 | Acc : 0.8530
Epoch 00062 | Train Loss : 0.3098 | Eval Loss : 0.3264 | Train acc : 0.8574 | Eval Acc : 0.8475 | Eval Log. Respected : 0.9402
     Batch 000 | Loss : 0.2817 | Acc : 0.8699
     Batch 005 | Loss : 0.3003 | Acc : 0.8609
     Batch 010 | Loss : 0.2867 | Acc : 0.8665
     Batch 015 | Loss : 0.2518 | Acc : 0.8840
     Batch 020 | Loss : 0.3155 | Acc : 0.8512
     Batch 025 | Loss : 0.2951 | Acc : 0.8613
     Batch 030 | Loss : 0.3015 | Acc : 0.8620
     Batch 035 | Loss : 0.2697 | Acc : 0.8789
     Batch 040 | Loss : 0.3113 | Acc : 0.8551
     Batch 045 | Loss : 0.3085 | Acc : 0.8562
     Batch 050 | Loss : 0.2967 | Acc : 0.8626
     Batch 055 | Loss : 0.4067 | Acc : 0.8137
     Batch 060 | Loss : 0.3459 | Acc : 0.8363
     Batch 065 | Loss : 0.3197 | Acc : 0.8520
     Batch 070 | Loss : 0.2959 | Acc : 0.8637
     Batch 075 | Loss : 0.2807 | Acc : 0.8754
     Batch 080 | Loss : 0.3576 | Acc : 0.8308
     Batch 085 | Loss : 0.2674 | Acc : 0.8829
     Batch 090 | Loss : 0.3739 | Acc : 0.8308
     Batch 095 | Loss : 0.3011 | Acc : 0.8647
     Batch 100 | Loss : 0.3381 | Acc : 0.8401
     Batch 105 | Loss : 0.2712 | Acc : 0.8746
     Batch 110 | Loss : 0.2817 | Acc : 0.8696
     Batch 115 | Loss : 0.4145 | Acc : 0.8084
     Batch 120 | Loss : 0.3174 | Acc : 0.8506
     Batch 125 | Loss : 0.2725 | Acc : 0.8789
     Batch 130 | Loss : 0.3038 | Acc : 0.8610
     Batch 135 | Loss : 0.3230 | Acc : 0.8548
     Batch 140 | Loss : 0.3181 | Acc : 0.8515
     Batch 145 | Loss : 0.3041 | Acc : 0.8586
     Batch 150 | Loss : 0.3199 | Acc : 0.8495
Epoch 00063 | Train Loss : 0.3103 | Eval Loss : 0.3198 | Train acc : 0.8572 | Eval Acc : 0.8519 | Eval Log. Respected : 0.9343
     Batch 000 | Loss : 0.3682 | Acc : 0.8286
     Batch 005 | Loss : 0.2837 | Acc : 0.8678
     Batch 010 | Loss : 0.2810 | Acc : 0.8730
     Batch 015 | Loss : 0.2758 | Acc : 0.8756
     Batch 020 | Loss : 0.3089 | Acc : 0.8532
     Batch 025 | Loss : 0.3333 | Acc : 0.8445
     Batch 030 | Loss : 0.3485 | Acc : 0.8301
     Batch 035 | Loss : 0.2930 | Acc : 0.8672
     Batch 040 | Loss : 0.2688 | Acc : 0.8779
     Batch 045 | Loss : 0.3118 | Acc : 0.8540
     Batch 050 | Loss : 0.3117 | Acc : 0.8539
     Batch 055 | Loss : 0.3320 | Acc : 0.8456
     Batch 060 | Loss : 0.2586 | Acc : 0.8838
     Batch 065 | Loss : 0.3018 | Acc : 0.8588
     Batch 070 | Loss : 0.3379 | Acc : 0.8446
     Batch 075 | Loss : 0.3105 | Acc : 0.8538
     Batch 080 | Loss : 0.2794 | Acc : 0.8759
     Batch 085 | Loss : 0.3061 | Acc : 0.8580
     Batch 090 | Loss : 0.2982 | Acc : 0.8635
     Batch 095 | Loss : 0.2901 | Acc : 0.8643
     Batch 100 | Loss : 0.3295 | Acc : 0.8473
     Batch 105 | Loss : 0.3577 | Acc : 0.8273
     Batch 110 | Loss : 0.3208 | Acc : 0.8481
     Batch 115 | Loss : 0.2700 | Acc : 0.8811
     Batch 120 | Loss : 0.3000 | Acc : 0.8627
     Batch 125 | Loss : 0.3346 | Acc : 0.8419
     Batch 130 | Loss : 0.3213 | Acc : 0.8513
     Batch 135 | Loss : 0.2913 | Acc : 0.8646
     Batch 140 | Loss : 0.3040 | Acc : 0.8599
     Batch 145 | Loss : 0.2990 | Acc : 0.8618
     Batch 150 | Loss : 0.3290 | Acc : 0.8503
Epoch 00064 | Train Loss : 0.3079 | Eval Loss : 0.3213 | Train acc : 0.8580 | Eval Acc : 0.8503 | Eval Log. Respected : 0.9276
     Batch 000 | Loss : 0.3331 | Acc : 0.8414
     Batch 005 | Loss : 0.2942 | Acc : 0.8650
     Batch 010 | Loss : 0.2448 | Acc : 0.8926
     Batch 015 | Loss : 0.3134 | Acc : 0.8511
     Batch 020 | Loss : 0.3026 | Acc : 0.8574
     Batch 025 | Loss : 0.2900 | Acc : 0.8674
     Batch 030 | Loss : 0.2980 | Acc : 0.8616
     Batch 035 | Loss : 0.3015 | Acc : 0.8622
     Batch 040 | Loss : 0.2939 | Acc : 0.8686
     Batch 045 | Loss : 0.3708 | Acc : 0.8263
     Batch 050 | Loss : 0.3201 | Acc : 0.8535
     Batch 055 | Loss : 0.3269 | Acc : 0.8497
     Batch 060 | Loss : 0.2976 | Acc : 0.8619
     Batch 065 | Loss : 0.2966 | Acc : 0.8624
     Batch 070 | Loss : 0.2477 | Acc : 0.8855
     Batch 075 | Loss : 0.2896 | Acc : 0.8687
     Batch 080 | Loss : 0.3239 | Acc : 0.8504
     Batch 085 | Loss : 0.3256 | Acc : 0.8480
     Batch 090 | Loss : 0.3581 | Acc : 0.8322
     Batch 095 | Loss : 0.3476 | Acc : 0.8357
     Batch 100 | Loss : 0.3003 | Acc : 0.8621
     Batch 105 | Loss : 0.3630 | Acc : 0.8367
     Batch 110 | Loss : 0.3431 | Acc : 0.8425
     Batch 115 | Loss : 0.3413 | Acc : 0.8376
     Batch 120 | Loss : 0.2694 | Acc : 0.8826
     Batch 125 | Loss : 0.3450 | Acc : 0.8396
     Batch 130 | Loss : 0.3304 | Acc : 0.8451
     Batch 135 | Loss : 0.3061 | Acc : 0.8616
     Batch 140 | Loss : 0.3100 | Acc : 0.8563
     Batch 145 | Loss : 0.3766 | Acc : 0.8289
     Batch 150 | Loss : 0.3238 | Acc : 0.8481
Epoch 00065 | Train Loss : 0.3083 | Eval Loss : 0.3165 | Train acc : 0.8581 | Eval Acc : 0.8522 | Eval Log. Respected : 0.9325
     Batch 000 | Loss : 0.3088 | Acc : 0.8550
     Batch 005 | Loss : 0.2918 | Acc : 0.8644
     Batch 010 | Loss : 0.2758 | Acc : 0.8746
     Batch 015 | Loss : 0.3197 | Acc : 0.8500
     Batch 020 | Loss : 0.3124 | Acc : 0.8536
     Batch 025 | Loss : 0.2949 | Acc : 0.8611
     Batch 030 | Loss : 0.2914 | Acc : 0.8647
     Batch 035 | Loss : 0.2814 | Acc : 0.8687
     Batch 040 | Loss : 0.3328 | Acc : 0.8402
     Batch 045 | Loss : 0.3683 | Acc : 0.8331
     Batch 050 | Loss : 0.3142 | Acc : 0.8523
     Batch 055 | Loss : 0.3055 | Acc : 0.8628
     Batch 060 | Loss : 0.3314 | Acc : 0.8482
     Batch 065 | Loss : 0.2751 | Acc : 0.8754
     Batch 070 | Loss : 0.3230 | Acc : 0.8526
     Batch 075 | Loss : 0.2925 | Acc : 0.8625
     Batch 080 | Loss : 0.2645 | Acc : 0.8808
     Batch 085 | Loss : 0.3248 | Acc : 0.8505
     Batch 090 | Loss : 0.3140 | Acc : 0.8518
     Batch 095 | Loss : 0.3378 | Acc : 0.8448
     Batch 100 | Loss : 0.3029 | Acc : 0.8556
     Batch 105 | Loss : 0.2720 | Acc : 0.8835
     Batch 110 | Loss : 0.3357 | Acc : 0.8441
     Batch 115 | Loss : 0.3884 | Acc : 0.8217
     Batch 120 | Loss : 0.3006 | Acc : 0.8610
     Batch 125 | Loss : 0.2919 | Acc : 0.8632
     Batch 130 | Loss : 0.3162 | Acc : 0.8516
     Batch 135 | Loss : 0.2632 | Acc : 0.8821
     Batch 140 | Loss : 0.3435 | Acc : 0.8472
     Batch 145 | Loss : 0.3842 | Acc : 0.8164
     Batch 150 | Loss : 0.2830 | Acc : 0.8728
Epoch 00066 | Train Loss : 0.3083 | Eval Loss : 0.3207 | Train acc : 0.8579 | Eval Acc : 0.8508 | Eval Log. Respected : 0.9234
     Batch 000 | Loss : 0.3083 | Acc : 0.8587
     Batch 005 | Loss : 0.3154 | Acc : 0.8551
     Batch 010 | Loss : 0.2482 | Acc : 0.8872
     Batch 015 | Loss : 0.2724 | Acc : 0.8787
     Batch 020 | Loss : 0.2992 | Acc : 0.8603
     Batch 025 | Loss : 0.2495 | Acc : 0.8860
     Batch 030 | Loss : 0.3453 | Acc : 0.8460
     Batch 035 | Loss : 0.2824 | Acc : 0.8750
     Batch 040 | Loss : 0.3583 | Acc : 0.8346
     Batch 045 | Loss : 0.4003 | Acc : 0.8113
     Batch 050 | Loss : 0.2692 | Acc : 0.8785
     Batch 055 | Loss : 0.2971 | Acc : 0.8637
     Batch 060 | Loss : 0.2729 | Acc : 0.8767
     Batch 065 | Loss : 0.3902 | Acc : 0.8152
     Batch 070 | Loss : 0.3021 | Acc : 0.8638
     Batch 075 | Loss : 0.3120 | Acc : 0.8546
     Batch 080 | Loss : 0.2905 | Acc : 0.8631
     Batch 085 | Loss : 0.2589 | Acc : 0.8866
     Batch 090 | Loss : 0.3497 | Acc : 0.8413
     Batch 095 | Loss : 0.3171 | Acc : 0.8520
     Batch 100 | Loss : 0.3105 | Acc : 0.8568
     Batch 105 | Loss : 0.3205 | Acc : 0.8509
     Batch 110 | Loss : 0.2584 | Acc : 0.8865
     Batch 115 | Loss : 0.3733 | Acc : 0.8278
     Batch 120 | Loss : 0.3094 | Acc : 0.8565
     Batch 125 | Loss : 0.3068 | Acc : 0.8657
     Batch 130 | Loss : 0.3078 | Acc : 0.8566
     Batch 135 | Loss : 0.3625 | Acc : 0.8350
     Batch 140 | Loss : 0.2983 | Acc : 0.8669
     Batch 145 | Loss : 0.3309 | Acc : 0.8414
     Batch 150 | Loss : 0.3003 | Acc : 0.8630
Epoch 00067 | Train Loss : 0.3101 | Eval Loss : 0.3175 | Train acc : 0.8573 | Eval Acc : 0.8516 | Eval Log. Respected : 0.9375
     Batch 000 | Loss : 0.3242 | Acc : 0.8478
     Batch 005 | Loss : 0.3002 | Acc : 0.8626
     Batch 010 | Loss : 0.2978 | Acc : 0.8648
     Batch 015 | Loss : 0.2665 | Acc : 0.8777
     Batch 020 | Loss : 0.2995 | Acc : 0.8586
     Batch 025 | Loss : 0.2985 | Acc : 0.8654
     Batch 030 | Loss : 0.3356 | Acc : 0.8426
     Batch 035 | Loss : 0.3378 | Acc : 0.8430
     Batch 040 | Loss : 0.2769 | Acc : 0.8744
     Batch 045 | Loss : 0.3453 | Acc : 0.8336
     Batch 050 | Loss : 0.3264 | Acc : 0.8453
     Batch 055 | Loss : 0.2882 | Acc : 0.8701
     Batch 060 | Loss : 0.3391 | Acc : 0.8381
     Batch 065 | Loss : 0.2620 | Acc : 0.8840
     Batch 070 | Loss : 0.3180 | Acc : 0.8511
     Batch 075 | Loss : 0.4297 | Acc : 0.8052
     Batch 080 | Loss : 0.3263 | Acc : 0.8497
     Batch 085 | Loss : 0.2918 | Acc : 0.8650
     Batch 090 | Loss : 0.3115 | Acc : 0.8590
     Batch 095 | Loss : 0.2964 | Acc : 0.8682
     Batch 100 | Loss : 0.2859 | Acc : 0.8678
     Batch 105 | Loss : 0.3135 | Acc : 0.8519
     Batch 110 | Loss : 0.3145 | Acc : 0.8507
     Batch 115 | Loss : 0.2592 | Acc : 0.8895
     Batch 120 | Loss : 0.3064 | Acc : 0.8621
     Batch 125 | Loss : 0.3040 | Acc : 0.8595
     Batch 130 | Loss : 0.3140 | Acc : 0.8548
     Batch 135 | Loss : 0.2609 | Acc : 0.8846
     Batch 140 | Loss : 0.3827 | Acc : 0.8242
     Batch 145 | Loss : 0.3675 | Acc : 0.8265
     Batch 150 | Loss : 0.3405 | Acc : 0.8445
Epoch 00068 | Train Loss : 0.3086 | Eval Loss : 0.3263 | Train acc : 0.8579 | Eval Acc : 0.8473 | Eval Log. Respected : 0.9252
     Batch 000 | Loss : 0.2768 | Acc : 0.8747
     Batch 005 | Loss : 0.3073 | Acc : 0.8595
     Batch 010 | Loss : 0.2839 | Acc : 0.8716
     Batch 015 | Loss : 0.2868 | Acc : 0.8693
     Batch 020 | Loss : 0.3726 | Acc : 0.8324
     Batch 025 | Loss : 0.3924 | Acc : 0.8181
     Batch 030 | Loss : 0.2968 | Acc : 0.8645
     Batch 035 | Loss : 0.3037 | Acc : 0.8636
     Batch 040 | Loss : 0.3157 | Acc : 0.8520
     Batch 045 | Loss : 0.2719 | Acc : 0.8749
     Batch 050 | Loss : 0.3850 | Acc : 0.8288
     Batch 055 | Loss : 0.3073 | Acc : 0.8602
     Batch 060 | Loss : 0.3226 | Acc : 0.8499
     Batch 065 | Loss : 0.2525 | Acc : 0.8853
     Batch 070 | Loss : 0.3236 | Acc : 0.8486
     Batch 075 | Loss : 0.2841 | Acc : 0.8764
     Batch 080 | Loss : 0.3728 | Acc : 0.8199
     Batch 085 | Loss : 0.2775 | Acc : 0.8750
     Batch 090 | Loss : 0.2738 | Acc : 0.8737
     Batch 095 | Loss : 0.3453 | Acc : 0.8368
     Batch 100 | Loss : 0.2992 | Acc : 0.8635
     Batch 105 | Loss : 0.2915 | Acc : 0.8700
     Batch 110 | Loss : 0.3030 | Acc : 0.8590
     Batch 115 | Loss : 0.3356 | Acc : 0.8400
     Batch 120 | Loss : 0.3076 | Acc : 0.8576
     Batch 125 | Loss : 0.3103 | Acc : 0.8517
     Batch 130 | Loss : 0.3476 | Acc : 0.8372
     Batch 135 | Loss : 0.3335 | Acc : 0.8469
     Batch 140 | Loss : 0.3661 | Acc : 0.8280
     Batch 145 | Loss : 0.2727 | Acc : 0.8769
     Batch 150 | Loss : 0.2405 | Acc : 0.8945
Epoch 00069 | Train Loss : 0.3072 | Eval Loss : 0.3206 | Train acc : 0.8586 | Eval Acc : 0.8511 | Eval Log. Respected : 0.9301
     Batch 000 | Loss : 0.3220 | Acc : 0.8478
     Batch 005 | Loss : 0.3160 | Acc : 0.8585
     Batch 010 | Loss : 0.2902 | Acc : 0.8663
     Batch 015 | Loss : 0.3543 | Acc : 0.8349
     Batch 020 | Loss : 0.3358 | Acc : 0.8440
     Batch 025 | Loss : 0.2988 | Acc : 0.8668
     Batch 030 | Loss : 0.2849 | Acc : 0.8657
     Batch 035 | Loss : 0.2813 | Acc : 0.8744
     Batch 040 | Loss : 0.2742 | Acc : 0.8764
     Batch 045 | Loss : 0.2916 | Acc : 0.8670
     Batch 050 | Loss : 0.2799 | Acc : 0.8710
     Batch 055 | Loss : 0.3076 | Acc : 0.8604
     Batch 060 | Loss : 0.3289 | Acc : 0.8463
     Batch 065 | Loss : 0.3014 | Acc : 0.8627
     Batch 070 | Loss : 0.2854 | Acc : 0.8673
     Batch 075 | Loss : 0.2954 | Acc : 0.8644
     Batch 080 | Loss : 0.3279 | Acc : 0.8483
     Batch 085 | Loss : 0.2859 | Acc : 0.8713
     Batch 090 | Loss : 0.2958 | Acc : 0.8621
     Batch 095 | Loss : 0.2936 | Acc : 0.8624
     Batch 100 | Loss : 0.2874 | Acc : 0.8721
     Batch 105 | Loss : 0.3510 | Acc : 0.8366
     Batch 110 | Loss : 0.2837 | Acc : 0.8717
     Batch 115 | Loss : 0.2712 | Acc : 0.8785
     Batch 120 | Loss : 0.2831 | Acc : 0.8686
     Batch 125 | Loss : 0.2728 | Acc : 0.8749
     Batch 130 | Loss : 0.2909 | Acc : 0.8669
     Batch 135 | Loss : 0.2856 | Acc : 0.8732
     Batch 140 | Loss : 0.2950 | Acc : 0.8647
     Batch 145 | Loss : 0.2884 | Acc : 0.8642
     Batch 150 | Loss : 0.2893 | Acc : 0.8684
Epoch 00070 | Train Loss : 0.3073 | Eval Loss : 0.3207 | Train acc : 0.8585 | Eval Acc : 0.8507 | Eval Log. Respected : 0.9508
     Batch 000 | Loss : 0.2629 | Acc : 0.8828
     Batch 005 | Loss : 0.3844 | Acc : 0.8251
     Batch 010 | Loss : 0.3009 | Acc : 0.8643
     Batch 015 | Loss : 0.3148 | Acc : 0.8543
     Batch 020 | Loss : 0.2615 | Acc : 0.8842
     Batch 025 | Loss : 0.2517 | Acc : 0.8838
     Batch 030 | Loss : 0.2879 | Acc : 0.8743
     Batch 035 | Loss : 0.3030 | Acc : 0.8614
     Batch 040 | Loss : 0.2981 | Acc : 0.8603
     Batch 045 | Loss : 0.2864 | Acc : 0.8693
     Batch 050 | Loss : 0.2700 | Acc : 0.8821
     Batch 055 | Loss : 0.2926 | Acc : 0.8641
     Batch 060 | Loss : 0.2791 | Acc : 0.8728
     Batch 065 | Loss : 0.2634 | Acc : 0.8766
     Batch 070 | Loss : 0.3265 | Acc : 0.8503
     Batch 075 | Loss : 0.2839 | Acc : 0.8718
     Batch 080 | Loss : 0.2838 | Acc : 0.8695
     Batch 085 | Loss : 0.3323 | Acc : 0.8447
     Batch 090 | Loss : 0.2924 | Acc : 0.8637
     Batch 095 | Loss : 0.3244 | Acc : 0.8494
     Batch 100 | Loss : 0.3507 | Acc : 0.8319
     Batch 105 | Loss : 0.3278 | Acc : 0.8480
     Batch 110 | Loss : 0.2577 | Acc : 0.8871
     Batch 115 | Loss : 0.2459 | Acc : 0.8908
     Batch 120 | Loss : 0.3142 | Acc : 0.8509
     Batch 125 | Loss : 0.2936 | Acc : 0.8662
     Batch 130 | Loss : 0.2866 | Acc : 0.8691
     Batch 135 | Loss : 0.3165 | Acc : 0.8523
     Batch 140 | Loss : 0.3564 | Acc : 0.8323
     Batch 145 | Loss : 0.3034 | Acc : 0.8561
     Batch 150 | Loss : 0.3002 | Acc : 0.8646
Epoch 00071 | Train Loss : 0.3080 | Eval Loss : 0.3230 | Train acc : 0.8581 | Eval Acc : 0.8502 | Eval Log. Respected : 0.9300
     Batch 000 | Loss : 0.2728 | Acc : 0.8786
     Batch 005 | Loss : 0.2941 | Acc : 0.8654
     Batch 010 | Loss : 0.3454 | Acc : 0.8436
     Batch 015 | Loss : 0.3349 | Acc : 0.8440
     Batch 020 | Loss : 0.2610 | Acc : 0.8840
     Batch 025 | Loss : 0.3293 | Acc : 0.8475
     Batch 030 | Loss : 0.3131 | Acc : 0.8529
     Batch 035 | Loss : 0.3022 | Acc : 0.8604
     Batch 040 | Loss : 0.3226 | Acc : 0.8522
     Batch 045 | Loss : 0.3767 | Acc : 0.8322
     Batch 050 | Loss : 0.3124 | Acc : 0.8555
     Batch 055 | Loss : 0.2910 | Acc : 0.8682
     Batch 060 | Loss : 0.3057 | Acc : 0.8572
     Batch 065 | Loss : 0.2709 | Acc : 0.8734
     Batch 070 | Loss : 0.2624 | Acc : 0.8795
     Batch 075 | Loss : 0.2927 | Acc : 0.8660
     Batch 080 | Loss : 0.3291 | Acc : 0.8413
     Batch 085 | Loss : 0.2975 | Acc : 0.8602
     Batch 090 | Loss : 0.3066 | Acc : 0.8601
     Batch 095 | Loss : 0.3012 | Acc : 0.8558
     Batch 100 | Loss : 0.3153 | Acc : 0.8528
     Batch 105 | Loss : 0.3028 | Acc : 0.8606
     Batch 110 | Loss : 0.2854 | Acc : 0.8693
     Batch 115 | Loss : 0.2961 | Acc : 0.8639
     Batch 120 | Loss : 0.3017 | Acc : 0.8629
     Batch 125 | Loss : 0.2844 | Acc : 0.8709
     Batch 130 | Loss : 0.3713 | Acc : 0.8254
     Batch 135 | Loss : 0.2615 | Acc : 0.8811
     Batch 140 | Loss : 0.3230 | Acc : 0.8520
     Batch 145 | Loss : 0.3012 | Acc : 0.8638
     Batch 150 | Loss : 0.3302 | Acc : 0.8460
Epoch 00072 | Train Loss : 0.3080 | Eval Loss : 0.3195 | Train acc : 0.8581 | Eval Acc : 0.8513 | Eval Log. Respected : 0.9248
     Batch 000 | Loss : 0.3273 | Acc : 0.8470
     Batch 005 | Loss : 0.2846 | Acc : 0.8722
     Batch 010 | Loss : 0.4169 | Acc : 0.8241
     Batch 015 | Loss : 0.3358 | Acc : 0.8438
     Batch 020 | Loss : 0.2917 | Acc : 0.8704
     Batch 025 | Loss : 0.2533 | Acc : 0.8911
     Batch 030 | Loss : 0.3426 | Acc : 0.8439
     Batch 035 | Loss : 0.3315 | Acc : 0.8413
     Batch 040 | Loss : 0.2634 | Acc : 0.8830
     Batch 045 | Loss : 0.2984 | Acc : 0.8607
     Batch 050 | Loss : 0.3268 | Acc : 0.8431
     Batch 055 | Loss : 0.3051 | Acc : 0.8600
     Batch 060 | Loss : 0.3366 | Acc : 0.8392
     Batch 065 | Loss : 0.2922 | Acc : 0.8678
     Batch 070 | Loss : 0.2694 | Acc : 0.8783
     Batch 075 | Loss : 0.2929 | Acc : 0.8673
     Batch 080 | Loss : 0.2776 | Acc : 0.8738
     Batch 085 | Loss : 0.2887 | Acc : 0.8703
     Batch 090 | Loss : 0.2576 | Acc : 0.8850
     Batch 095 | Loss : 0.3090 | Acc : 0.8555
     Batch 100 | Loss : 0.2910 | Acc : 0.8670
     Batch 105 | Loss : 0.3191 | Acc : 0.8541
     Batch 110 | Loss : 0.3466 | Acc : 0.8415
     Batch 115 | Loss : 0.3085 | Acc : 0.8544
     Batch 120 | Loss : 0.2529 | Acc : 0.8926
     Batch 125 | Loss : 0.2792 | Acc : 0.8726
     Batch 130 | Loss : 0.3550 | Acc : 0.8388
     Batch 135 | Loss : 0.2946 | Acc : 0.8640
     Batch 140 | Loss : 0.3270 | Acc : 0.8475
     Batch 145 | Loss : 0.2822 | Acc : 0.8729
     Batch 150 | Loss : 0.3027 | Acc : 0.8620
Epoch 00073 | Train Loss : 0.3074 | Eval Loss : 0.3169 | Train acc : 0.8586 | Eval Acc : 0.8526 | Eval Log. Respected : 0.9325
     Batch 000 | Loss : 0.2554 | Acc : 0.8878
     Batch 005 | Loss : 0.2792 | Acc : 0.8729
     Batch 010 | Loss : 0.2795 | Acc : 0.8739
     Batch 015 | Loss : 0.2833 | Acc : 0.8733
     Batch 020 | Loss : 0.3277 | Acc : 0.8499
     Batch 025 | Loss : 0.2941 | Acc : 0.8630
     Batch 030 | Loss : 0.2920 | Acc : 0.8652
     Batch 035 | Loss : 0.3338 | Acc : 0.8440
     Batch 040 | Loss : 0.2653 | Acc : 0.8786
     Batch 045 | Loss : 0.2658 | Acc : 0.8780
     Batch 050 | Loss : 0.2833 | Acc : 0.8703
     Batch 055 | Loss : 0.3079 | Acc : 0.8556
     Batch 060 | Loss : 0.2874 | Acc : 0.8678
     Batch 065 | Loss : 0.3716 | Acc : 0.8229
     Batch 070 | Loss : 0.3226 | Acc : 0.8537
     Batch 075 | Loss : 0.2563 | Acc : 0.8816
     Batch 080 | Loss : 0.3444 | Acc : 0.8385
     Batch 085 | Loss : 0.3422 | Acc : 0.8404
     Batch 090 | Loss : 0.3557 | Acc : 0.8358
     Batch 095 | Loss : 0.3303 | Acc : 0.8445
     Batch 100 | Loss : 0.3109 | Acc : 0.8522
     Batch 105 | Loss : 0.3330 | Acc : 0.8461
     Batch 110 | Loss : 0.3064 | Acc : 0.8540
     Batch 115 | Loss : 0.3173 | Acc : 0.8520
     Batch 120 | Loss : 0.3939 | Acc : 0.8192
     Batch 125 | Loss : 0.3636 | Acc : 0.8218
     Batch 130 | Loss : 0.3023 | Acc : 0.8559
     Batch 135 | Loss : 0.4026 | Acc : 0.8163
     Batch 140 | Loss : 0.3714 | Acc : 0.8295
     Batch 145 | Loss : 0.3094 | Acc : 0.8585
     Batch 150 | Loss : 0.3117 | Acc : 0.8545
Epoch 00074 | Train Loss : 0.3060 | Eval Loss : 0.3176 | Train acc : 0.8590 | Eval Acc : 0.8522 | Eval Log. Respected : 0.9272
     Batch 000 | Loss : 0.3303 | Acc : 0.8474
     Batch 005 | Loss : 0.2660 | Acc : 0.8815
     Batch 010 | Loss : 0.2851 | Acc : 0.8666
     Batch 015 | Loss : 0.3252 | Acc : 0.8504
     Batch 020 | Loss : 0.2894 | Acc : 0.8697
     Batch 025 | Loss : 0.3458 | Acc : 0.8347
     Batch 030 | Loss : 0.2893 | Acc : 0.8679
     Batch 035 | Loss : 0.2644 | Acc : 0.8805
     Batch 040 | Loss : 0.3222 | Acc : 0.8533
     Batch 045 | Loss : 0.3068 | Acc : 0.8589
     Batch 050 | Loss : 0.3150 | Acc : 0.8497
     Batch 055 | Loss : 0.3457 | Acc : 0.8427
     Batch 060 | Loss : 0.3013 | Acc : 0.8594
     Batch 065 | Loss : 0.2959 | Acc : 0.8608
     Batch 070 | Loss : 0.2917 | Acc : 0.8647
     Batch 075 | Loss : 0.2811 | Acc : 0.8716
     Batch 080 | Loss : 0.2585 | Acc : 0.8880
     Batch 085 | Loss : 0.3093 | Acc : 0.8562
     Batch 090 | Loss : 0.3915 | Acc : 0.8174
     Batch 095 | Loss : 0.3247 | Acc : 0.8466
     Batch 100 | Loss : 0.2893 | Acc : 0.8667
     Batch 105 | Loss : 0.2912 | Acc : 0.8655
     Batch 110 | Loss : 0.3351 | Acc : 0.8441
     Batch 115 | Loss : 0.2935 | Acc : 0.8639
     Batch 120 | Loss : 0.3260 | Acc : 0.8463
     Batch 125 | Loss : 0.2693 | Acc : 0.8766
     Batch 130 | Loss : 0.3719 | Acc : 0.8192
     Batch 135 | Loss : 0.3178 | Acc : 0.8569
     Batch 140 | Loss : 0.3345 | Acc : 0.8416
     Batch 145 | Loss : 0.2820 | Acc : 0.8692
     Batch 150 | Loss : 0.2771 | Acc : 0.8723
Epoch 00075 | Train Loss : 0.3044 | Eval Loss : 0.3141 | Train acc : 0.8597 | Eval Acc : 0.8538 | Eval Log. Respected : 0.9234
     Batch 000 | Loss : 0.2700 | Acc : 0.8790
     Batch 005 | Loss : 0.3276 | Acc : 0.8482
     Batch 010 | Loss : 0.2748 | Acc : 0.8779
     Batch 015 | Loss : 0.3842 | Acc : 0.8304
     Batch 020 | Loss : 0.3335 | Acc : 0.8442
     Batch 025 | Loss : 0.2664 | Acc : 0.8844
     Batch 030 | Loss : 0.2897 | Acc : 0.8626
     Batch 035 | Loss : 0.3160 | Acc : 0.8553
     Batch 040 | Loss : 0.2956 | Acc : 0.8669
     Batch 045 | Loss : 0.2532 | Acc : 0.8876
     Batch 050 | Loss : 0.3174 | Acc : 0.8517
     Batch 055 | Loss : 0.3598 | Acc : 0.8331
     Batch 060 | Loss : 0.2515 | Acc : 0.8891
     Batch 065 | Loss : 0.3056 | Acc : 0.8609
     Batch 070 | Loss : 0.3212 | Acc : 0.8508
     Batch 075 | Loss : 0.3415 | Acc : 0.8349
     Batch 080 | Loss : 0.2925 | Acc : 0.8665
     Batch 085 | Loss : 0.3125 | Acc : 0.8545
     Batch 090 | Loss : 0.3040 | Acc : 0.8594
     Batch 095 | Loss : 0.3472 | Acc : 0.8337
     Batch 100 | Loss : 0.3022 | Acc : 0.8609
     Batch 105 | Loss : 0.2864 | Acc : 0.8682
     Batch 110 | Loss : 0.3494 | Acc : 0.8370
     Batch 115 | Loss : 0.2905 | Acc : 0.8658
     Batch 120 | Loss : 0.3219 | Acc : 0.8522
     Batch 125 | Loss : 0.2739 | Acc : 0.8771
     Batch 130 | Loss : 0.2925 | Acc : 0.8648
     Batch 135 | Loss : 0.2935 | Acc : 0.8639
     Batch 140 | Loss : 0.2908 | Acc : 0.8686
     Batch 145 | Loss : 0.2528 | Acc : 0.8910
     Batch 150 | Loss : 0.2812 | Acc : 0.8759
Epoch 00076 | Train Loss : 0.3056 | Eval Loss : 0.3167 | Train acc : 0.8591 | Eval Acc : 0.8524 | Eval Log. Respected : 0.9407
     Batch 000 | Loss : 0.3525 | Acc : 0.8300
     Batch 005 | Loss : 0.3247 | Acc : 0.8436
     Batch 010 | Loss : 0.3129 | Acc : 0.8552
     Batch 015 | Loss : 0.2794 | Acc : 0.8701
     Batch 020 | Loss : 0.3233 | Acc : 0.8515
     Batch 025 | Loss : 0.3180 | Acc : 0.8500
     Batch 030 | Loss : 0.3614 | Acc : 0.8287
     Batch 035 | Loss : 0.2772 | Acc : 0.8738
     Batch 040 | Loss : 0.3193 | Acc : 0.8516
     Batch 045 | Loss : 0.3266 | Acc : 0.8463
     Batch 050 | Loss : 0.2682 | Acc : 0.8788
     Batch 055 | Loss : 0.3007 | Acc : 0.8563
     Batch 060 | Loss : 0.2911 | Acc : 0.8624
     Batch 065 | Loss : 0.3471 | Acc : 0.8341
     Batch 070 | Loss : 0.2717 | Acc : 0.8804
     Batch 075 | Loss : 0.3959 | Acc : 0.8175
     Batch 080 | Loss : 0.2822 | Acc : 0.8750
     Batch 085 | Loss : 0.3214 | Acc : 0.8503
     Batch 090 | Loss : 0.2458 | Acc : 0.8901
     Batch 095 | Loss : 0.3766 | Acc : 0.8190
     Batch 100 | Loss : 0.2984 | Acc : 0.8615
     Batch 105 | Loss : 0.2891 | Acc : 0.8670
     Batch 110 | Loss : 0.3305 | Acc : 0.8491
     Batch 115 | Loss : 0.2604 | Acc : 0.8834
     Batch 120 | Loss : 0.2745 | Acc : 0.8742
     Batch 125 | Loss : 0.3659 | Acc : 0.8339
     Batch 130 | Loss : 0.3318 | Acc : 0.8471
     Batch 135 | Loss : 0.2520 | Acc : 0.8870
     Batch 140 | Loss : 0.2707 | Acc : 0.8771
     Batch 145 | Loss : 0.3328 | Acc : 0.8412
     Batch 150 | Loss : 0.2692 | Acc : 0.8836
Epoch 00077 | Train Loss : 0.3046 | Eval Loss : 0.3151 | Train acc : 0.8594 | Eval Acc : 0.8528 | Eval Log. Respected : 0.9257
     Batch 000 | Loss : 0.3066 | Acc : 0.8613
     Batch 005 | Loss : 0.3296 | Acc : 0.8436
     Batch 010 | Loss : 0.3786 | Acc : 0.8232
     Batch 015 | Loss : 0.2720 | Acc : 0.8770
     Batch 020 | Loss : 0.3401 | Acc : 0.8421
     Batch 025 | Loss : 0.3285 | Acc : 0.8429
     Batch 030 | Loss : 0.2820 | Acc : 0.8708
     Batch 035 | Loss : 0.3151 | Acc : 0.8526
     Batch 040 | Loss : 0.3116 | Acc : 0.8540
     Batch 045 | Loss : 0.2815 | Acc : 0.8742
     Batch 050 | Loss : 0.3222 | Acc : 0.8499
     Batch 055 | Loss : 0.3019 | Acc : 0.8602
     Batch 060 | Loss : 0.2915 | Acc : 0.8682
     Batch 065 | Loss : 0.2853 | Acc : 0.8694
     Batch 070 | Loss : 0.3110 | Acc : 0.8518
     Batch 075 | Loss : 0.2831 | Acc : 0.8691
     Batch 080 | Loss : 0.3002 | Acc : 0.8588
     Batch 085 | Loss : 0.3022 | Acc : 0.8597
     Batch 090 | Loss : 0.2962 | Acc : 0.8618
     Batch 095 | Loss : 0.3298 | Acc : 0.8453
     Batch 100 | Loss : 0.2837 | Acc : 0.8685
     Batch 105 | Loss : 0.3337 | Acc : 0.8456
     Batch 110 | Loss : 0.2708 | Acc : 0.8768
     Batch 115 | Loss : 0.3279 | Acc : 0.8478
     Batch 120 | Loss : 0.3346 | Acc : 0.8423
     Batch 125 | Loss : 0.3062 | Acc : 0.8556
     Batch 130 | Loss : 0.3042 | Acc : 0.8588
     Batch 135 | Loss : 0.2966 | Acc : 0.8654
     Batch 140 | Loss : 0.2850 | Acc : 0.8692
     Batch 145 | Loss : 0.2806 | Acc : 0.8686
     Batch 150 | Loss : 0.3001 | Acc : 0.8587
Epoch 00078 | Train Loss : 0.3045 | Eval Loss : 0.3173 | Train acc : 0.8596 | Eval Acc : 0.8528 | Eval Log. Respected : 0.9226
     Batch 000 | Loss : 0.2536 | Acc : 0.8830
     Batch 005 | Loss : 0.2439 | Acc : 0.8894
     Batch 010 | Loss : 0.2935 | Acc : 0.8650
     Batch 015 | Loss : 0.3586 | Acc : 0.8299
     Batch 020 | Loss : 0.3292 | Acc : 0.8485
     Batch 025 | Loss : 0.2954 | Acc : 0.8627
     Batch 030 | Loss : 0.3203 | Acc : 0.8447
     Batch 035 | Loss : 0.3447 | Acc : 0.8375
     Batch 040 | Loss : 0.3041 | Acc : 0.8640
     Batch 045 | Loss : 0.2818 | Acc : 0.8682
     Batch 050 | Loss : 0.2801 | Acc : 0.8715
     Batch 055 | Loss : 0.3059 | Acc : 0.8658
     Batch 060 | Loss : 0.3079 | Acc : 0.8544
     Batch 065 | Loss : 0.2417 | Acc : 0.8958
     Batch 070 | Loss : 0.2545 | Acc : 0.8835
     Batch 075 | Loss : 0.2926 | Acc : 0.8654
     Batch 080 | Loss : 0.2718 | Acc : 0.8781
     Batch 085 | Loss : 0.2626 | Acc : 0.8815
     Batch 090 | Loss : 0.3336 | Acc : 0.8446
     Batch 095 | Loss : 0.3289 | Acc : 0.8508
     Batch 100 | Loss : 0.2777 | Acc : 0.8703
     Batch 105 | Loss : 0.2673 | Acc : 0.8811
     Batch 110 | Loss : 0.3352 | Acc : 0.8427
     Batch 115 | Loss : 0.2774 | Acc : 0.8736
     Batch 120 | Loss : 0.3075 | Acc : 0.8590
     Batch 125 | Loss : 0.2989 | Acc : 0.8618
     Batch 130 | Loss : 0.3149 | Acc : 0.8551
     Batch 135 | Loss : 0.3115 | Acc : 0.8588
     Batch 140 | Loss : 0.3020 | Acc : 0.8638
     Batch 145 | Loss : 0.3378 | Acc : 0.8416
     Batch 150 | Loss : 0.2934 | Acc : 0.8652
Epoch 00079 | Train Loss : 0.3056 | Eval Loss : 0.3220 | Train acc : 0.8593 | Eval Acc : 0.8508 | Eval Log. Respected : 0.9365
     Batch 000 | Loss : 0.4222 | Acc : 0.8079
     Batch 005 | Loss : 0.3413 | Acc : 0.8398
     Batch 010 | Loss : 0.3188 | Acc : 0.8508
     Batch 015 | Loss : 0.3081 | Acc : 0.8536
     Batch 020 | Loss : 0.2731 | Acc : 0.8767
     Batch 025 | Loss : 0.2853 | Acc : 0.8738
     Batch 030 | Loss : 0.2683 | Acc : 0.8754
     Batch 035 | Loss : 0.3190 | Acc : 0.8512
     Batch 040 | Loss : 0.2536 | Acc : 0.8845
     Batch 045 | Loss : 0.3151 | Acc : 0.8525
     Batch 050 | Loss : 0.3319 | Acc : 0.8448
     Batch 055 | Loss : 0.2553 | Acc : 0.8868
     Batch 060 | Loss : 0.3199 | Acc : 0.8514
     Batch 065 | Loss : 0.3130 | Acc : 0.8558
     Batch 070 | Loss : 0.3527 | Acc : 0.8313
     Batch 075 | Loss : 0.3180 | Acc : 0.8513
     Batch 080 | Loss : 0.3365 | Acc : 0.8440
     Batch 085 | Loss : 0.2523 | Acc : 0.8889
     Batch 090 | Loss : 0.2840 | Acc : 0.8679
     Batch 095 | Loss : 0.3005 | Acc : 0.8596
     Batch 100 | Loss : 0.2914 | Acc : 0.8680
     Batch 105 | Loss : 0.2808 | Acc : 0.8746
     Batch 110 | Loss : 0.3067 | Acc : 0.8559
     Batch 115 | Loss : 0.3590 | Acc : 0.8336
     Batch 120 | Loss : 0.3258 | Acc : 0.8474
     Batch 125 | Loss : 0.2984 | Acc : 0.8633
     Batch 130 | Loss : 0.2602 | Acc : 0.8837
     Batch 135 | Loss : 0.2678 | Acc : 0.8771
     Batch 140 | Loss : 0.3706 | Acc : 0.8233
     Batch 145 | Loss : 0.3226 | Acc : 0.8501
     Batch 150 | Loss : 0.3157 | Acc : 0.8485
Epoch 00080 | Train Loss : 0.3043 | Eval Loss : 0.3198 | Train acc : 0.8598 | Eval Acc : 0.8515 | Eval Log. Respected : 0.9287
     Batch 000 | Loss : 0.3183 | Acc : 0.8561
     Batch 005 | Loss : 0.2521 | Acc : 0.8890
     Batch 010 | Loss : 0.2686 | Acc : 0.8777
     Batch 015 | Loss : 0.2861 | Acc : 0.8714
     Batch 020 | Loss : 0.2715 | Acc : 0.8741
     Batch 025 | Loss : 0.2669 | Acc : 0.8795
     Batch 030 | Loss : 0.2820 | Acc : 0.8735
     Batch 035 | Loss : 0.3449 | Acc : 0.8399
     Batch 040 | Loss : 0.2700 | Acc : 0.8788
     Batch 045 | Loss : 0.2987 | Acc : 0.8636
     Batch 050 | Loss : 0.2586 | Acc : 0.8816
     Batch 055 | Loss : 0.3152 | Acc : 0.8520
     Batch 060 | Loss : 0.2715 | Acc : 0.8757
     Batch 065 | Loss : 0.3376 | Acc : 0.8431
     Batch 070 | Loss : 0.2773 | Acc : 0.8742
     Batch 075 | Loss : 0.3359 | Acc : 0.8465
     Batch 080 | Loss : 0.2849 | Acc : 0.8696
     Batch 085 | Loss : 0.3065 | Acc : 0.8606
     Batch 090 | Loss : 0.2991 | Acc : 0.8622
     Batch 095 | Loss : 0.3130 | Acc : 0.8555
     Batch 100 | Loss : 0.2632 | Acc : 0.8844
     Batch 105 | Loss : 0.2926 | Acc : 0.8667
     Batch 110 | Loss : 0.3190 | Acc : 0.8533
     Batch 115 | Loss : 0.2882 | Acc : 0.8694
     Batch 120 | Loss : 0.3638 | Acc : 0.8297
     Batch 125 | Loss : 0.3545 | Acc : 0.8254
     Batch 130 | Loss : 0.3159 | Acc : 0.8508
     Batch 135 | Loss : 0.3162 | Acc : 0.8529
     Batch 140 | Loss : 0.3305 | Acc : 0.8473
     Batch 145 | Loss : 0.2924 | Acc : 0.8660
     Batch 150 | Loss : 0.2548 | Acc : 0.8848
Epoch 00081 | Train Loss : 0.3046 | Eval Loss : 0.3187 | Train acc : 0.8599 | Eval Acc : 0.8520 | Eval Log. Respected : 0.9362
     Batch 000 | Loss : 0.2910 | Acc : 0.8631
     Batch 005 | Loss : 0.3200 | Acc : 0.8513
     Batch 010 | Loss : 0.2819 | Acc : 0.8689
     Batch 015 | Loss : 0.2645 | Acc : 0.8787
     Batch 020 | Loss : 0.3233 | Acc : 0.8488
     Batch 025 | Loss : 0.2849 | Acc : 0.8672
     Batch 030 | Loss : 0.2768 | Acc : 0.8727
     Batch 035 | Loss : 0.2670 | Acc : 0.8809
     Batch 040 | Loss : 0.3428 | Acc : 0.8386
     Batch 045 | Loss : 0.2920 | Acc : 0.8675
     Batch 050 | Loss : 0.3411 | Acc : 0.8404
     Batch 055 | Loss : 0.3198 | Acc : 0.8508
     Batch 060 | Loss : 0.2918 | Acc : 0.8625
     Batch 065 | Loss : 0.2911 | Acc : 0.8673
     Batch 070 | Loss : 0.2643 | Acc : 0.8833
     Batch 075 | Loss : 0.3274 | Acc : 0.8486
     Batch 080 | Loss : 0.2960 | Acc : 0.8671
     Batch 085 | Loss : 0.3464 | Acc : 0.8377
     Batch 090 | Loss : 0.3490 | Acc : 0.8298
     Batch 095 | Loss : 0.2802 | Acc : 0.8773
     Batch 100 | Loss : 0.3380 | Acc : 0.8425
     Batch 105 | Loss : 0.2295 | Acc : 0.8990
     Batch 110 | Loss : 0.2547 | Acc : 0.8825
     Batch 115 | Loss : 0.2762 | Acc : 0.8717
     Batch 120 | Loss : 0.2712 | Acc : 0.8785
     Batch 125 | Loss : 0.3202 | Acc : 0.8513
     Batch 130 | Loss : 0.2745 | Acc : 0.8796
     Batch 135 | Loss : 0.3052 | Acc : 0.8568
     Batch 140 | Loss : 0.3153 | Acc : 0.8539
     Batch 145 | Loss : 0.2955 | Acc : 0.8641
     Batch 150 | Loss : 0.2811 | Acc : 0.8707
Epoch 00082 | Train Loss : 0.3044 | Eval Loss : 0.3184 | Train acc : 0.8599 | Eval Acc : 0.8517 | Eval Log. Respected : 0.9323
     Batch 000 | Loss : 0.2867 | Acc : 0.8682
     Batch 005 | Loss : 0.3218 | Acc : 0.8521
     Batch 010 | Loss : 0.3027 | Acc : 0.8588
     Batch 015 | Loss : 0.3289 | Acc : 0.8447
     Batch 020 | Loss : 0.2882 | Acc : 0.8670
     Batch 025 | Loss : 0.2471 | Acc : 0.8916
     Batch 030 | Loss : 0.3052 | Acc : 0.8617
     Batch 035 | Loss : 0.2938 | Acc : 0.8663
     Batch 040 | Loss : 0.2670 | Acc : 0.8798
     Batch 045 | Loss : 0.3302 | Acc : 0.8491
     Batch 050 | Loss : 0.2885 | Acc : 0.8693
     Batch 055 | Loss : 0.2971 | Acc : 0.8619
     Batch 060 | Loss : 0.3221 | Acc : 0.8467
     Batch 065 | Loss : 0.3345 | Acc : 0.8490
     Batch 070 | Loss : 0.3182 | Acc : 0.8518
     Batch 075 | Loss : 0.2997 | Acc : 0.8638
     Batch 080 | Loss : 0.3331 | Acc : 0.8428
     Batch 085 | Loss : 0.3376 | Acc : 0.8368
     Batch 090 | Loss : 0.3173 | Acc : 0.8524
     Batch 095 | Loss : 0.3280 | Acc : 0.8453
     Batch 100 | Loss : 0.2837 | Acc : 0.8717
     Batch 105 | Loss : 0.3092 | Acc : 0.8593
     Batch 110 | Loss : 0.2759 | Acc : 0.8737
     Batch 115 | Loss : 0.2915 | Acc : 0.8701
     Batch 120 | Loss : 0.3070 | Acc : 0.8572
     Batch 125 | Loss : 0.2715 | Acc : 0.8789
     Batch 130 | Loss : 0.3091 | Acc : 0.8578
     Batch 135 | Loss : 0.3375 | Acc : 0.8428
     Batch 140 | Loss : 0.2769 | Acc : 0.8730
     Batch 145 | Loss : 0.3417 | Acc : 0.8407
     Batch 150 | Loss : 0.3768 | Acc : 0.8199
Epoch 00083 | Train Loss : 0.3035 | Eval Loss : 0.3161 | Train acc : 0.8601 | Eval Acc : 0.8519 | Eval Log. Respected : 0.9334
     Batch 000 | Loss : 0.3187 | Acc : 0.8538
     Batch 005 | Loss : 0.2539 | Acc : 0.8852
     Batch 010 | Loss : 0.2946 | Acc : 0.8648
     Batch 015 | Loss : 0.3005 | Acc : 0.8617
     Batch 020 | Loss : 0.3188 | Acc : 0.8552
     Batch 025 | Loss : 0.2661 | Acc : 0.8815
     Batch 030 | Loss : 0.3056 | Acc : 0.8607
     Batch 035 | Loss : 0.2956 | Acc : 0.8614
     Batch 040 | Loss : 0.2647 | Acc : 0.8828
     Batch 045 | Loss : 0.2965 | Acc : 0.8623
     Batch 050 | Loss : 0.2686 | Acc : 0.8774
     Batch 055 | Loss : 0.2564 | Acc : 0.8840
     Batch 060 | Loss : 0.2962 | Acc : 0.8656
     Batch 065 | Loss : 0.2927 | Acc : 0.8633
     Batch 070 | Loss : 0.3562 | Acc : 0.8344
     Batch 075 | Loss : 0.2926 | Acc : 0.8661
     Batch 080 | Loss : 0.2834 | Acc : 0.8716
     Batch 085 | Loss : 0.3455 | Acc : 0.8396
     Batch 090 | Loss : 0.3063 | Acc : 0.8605
     Batch 095 | Loss : 0.3122 | Acc : 0.8575
     Batch 100 | Loss : 0.3392 | Acc : 0.8460
     Batch 105 | Loss : 0.2892 | Acc : 0.8683
     Batch 110 | Loss : 0.3322 | Acc : 0.8403
     Batch 115 | Loss : 0.3913 | Acc : 0.8206
     Batch 120 | Loss : 0.2670 | Acc : 0.8768
     Batch 125 | Loss : 0.2856 | Acc : 0.8684
     Batch 130 | Loss : 0.3224 | Acc : 0.8501
     Batch 135 | Loss : 0.3182 | Acc : 0.8503
     Batch 140 | Loss : 0.3563 | Acc : 0.8321
     Batch 145 | Loss : 0.3291 | Acc : 0.8466
     Batch 150 | Loss : 0.2876 | Acc : 0.8678
Epoch 00084 | Train Loss : 0.3036 | Eval Loss : 0.3213 | Train acc : 0.8600 | Eval Acc : 0.8524 | Eval Log. Respected : 0.9318
     Batch 000 | Loss : 0.2745 | Acc : 0.8780
     Batch 005 | Loss : 0.2574 | Acc : 0.8831
     Batch 010 | Loss : 0.3026 | Acc : 0.8603
     Batch 015 | Loss : 0.3272 | Acc : 0.8462
     Batch 020 | Loss : 0.2724 | Acc : 0.8774
     Batch 025 | Loss : 0.3264 | Acc : 0.8466
     Batch 030 | Loss : 0.2657 | Acc : 0.8840
     Batch 035 | Loss : 0.2921 | Acc : 0.8691
     Batch 040 | Loss : 0.2762 | Acc : 0.8747
     Batch 045 | Loss : 0.3056 | Acc : 0.8603
     Batch 050 | Loss : 0.3212 | Acc : 0.8528
     Batch 055 | Loss : 0.3389 | Acc : 0.8409
     Batch 060 | Loss : 0.2805 | Acc : 0.8759
     Batch 065 | Loss : 0.3355 | Acc : 0.8409
     Batch 070 | Loss : 0.2667 | Acc : 0.8821
     Batch 075 | Loss : 0.3415 | Acc : 0.8427
     Batch 080 | Loss : 0.3199 | Acc : 0.8483
     Batch 085 | Loss : 0.2994 | Acc : 0.8625
     Batch 090 | Loss : 0.2664 | Acc : 0.8786
     Batch 095 | Loss : 0.3015 | Acc : 0.8607
     Batch 100 | Loss : 0.3265 | Acc : 0.8474
     Batch 105 | Loss : 0.3000 | Acc : 0.8603
     Batch 110 | Loss : 0.3136 | Acc : 0.8552
     Batch 115 | Loss : 0.2811 | Acc : 0.8721
     Batch 120 | Loss : 0.3417 | Acc : 0.8456
     Batch 125 | Loss : 0.3257 | Acc : 0.8447
     Batch 130 | Loss : 0.3074 | Acc : 0.8598
     Batch 135 | Loss : 0.2853 | Acc : 0.8689
     Batch 140 | Loss : 0.2805 | Acc : 0.8753
     Batch 145 | Loss : 0.3068 | Acc : 0.8573
     Batch 150 | Loss : 0.2922 | Acc : 0.8676
Epoch 00085 | Train Loss : 0.3032 | Eval Loss : 0.3268 | Train acc : 0.8604 | Eval Acc : 0.8472 | Eval Log. Respected : 0.9286
     Batch 000 | Loss : 0.3179 | Acc : 0.8563
     Batch 005 | Loss : 0.2988 | Acc : 0.8638
     Batch 010 | Loss : 0.2795 | Acc : 0.8676
     Batch 015 | Loss : 0.3048 | Acc : 0.8578
     Batch 020 | Loss : 0.2841 | Acc : 0.8727
     Batch 025 | Loss : 0.3027 | Acc : 0.8596
     Batch 030 | Loss : 0.2978 | Acc : 0.8624
     Batch 035 | Loss : 0.3098 | Acc : 0.8586
     Batch 040 | Loss : 0.2914 | Acc : 0.8628
     Batch 045 | Loss : 0.2888 | Acc : 0.8670
     Batch 050 | Loss : 0.3621 | Acc : 0.8340
     Batch 055 | Loss : 0.3335 | Acc : 0.8377
     Batch 060 | Loss : 0.3167 | Acc : 0.8507
     Batch 065 | Loss : 0.3558 | Acc : 0.8318
     Batch 070 | Loss : 0.3215 | Acc : 0.8498
     Batch 075 | Loss : 0.2623 | Acc : 0.8815
     Batch 080 | Loss : 0.2932 | Acc : 0.8634
     Batch 085 | Loss : 0.2850 | Acc : 0.8698
     Batch 090 | Loss : 0.3714 | Acc : 0.8265
     Batch 095 | Loss : 0.3022 | Acc : 0.8586
     Batch 100 | Loss : 0.2690 | Acc : 0.8815
     Batch 105 | Loss : 0.3014 | Acc : 0.8663
     Batch 110 | Loss : 0.3109 | Acc : 0.8547
     Batch 115 | Loss : 0.2938 | Acc : 0.8635
     Batch 120 | Loss : 0.3080 | Acc : 0.8572
     Batch 125 | Loss : 0.2612 | Acc : 0.8831
     Batch 130 | Loss : 0.3649 | Acc : 0.8331
     Batch 135 | Loss : 0.3133 | Acc : 0.8545
     Batch 140 | Loss : 0.2786 | Acc : 0.8710
     Batch 145 | Loss : 0.3011 | Acc : 0.8601
     Batch 150 | Loss : 0.3449 | Acc : 0.8349
Epoch 00086 | Train Loss : 0.3031 | Eval Loss : 0.3171 | Train acc : 0.8603 | Eval Acc : 0.8529 | Eval Log. Respected : 0.9377
     Batch 000 | Loss : 0.3665 | Acc : 0.8275
     Batch 005 | Loss : 0.2964 | Acc : 0.8675
     Batch 010 | Loss : 0.2701 | Acc : 0.8786
     Batch 015 | Loss : 0.3047 | Acc : 0.8576
     Batch 020 | Loss : 0.3445 | Acc : 0.8363
     Batch 025 | Loss : 0.2866 | Acc : 0.8704
     Batch 030 | Loss : 0.3291 | Acc : 0.8456
     Batch 035 | Loss : 0.3439 | Acc : 0.8444
     Batch 040 | Loss : 0.2467 | Acc : 0.8897
     Batch 045 | Loss : 0.2826 | Acc : 0.8676
     Batch 050 | Loss : 0.3467 | Acc : 0.8347
     Batch 055 | Loss : 0.3014 | Acc : 0.8624
     Batch 060 | Loss : 0.2352 | Acc : 0.8941
     Batch 065 | Loss : 0.3583 | Acc : 0.8358
     Batch 070 | Loss : 0.3624 | Acc : 0.8286
     Batch 075 | Loss : 0.2893 | Acc : 0.8707
     Batch 080 | Loss : 0.2712 | Acc : 0.8752
     Batch 085 | Loss : 0.3558 | Acc : 0.8332
     Batch 090 | Loss : 0.3099 | Acc : 0.8541
     Batch 095 | Loss : 0.2869 | Acc : 0.8703
     Batch 100 | Loss : 0.3075 | Acc : 0.8586
     Batch 105 | Loss : 0.3488 | Acc : 0.8344
     Batch 110 | Loss : 0.2648 | Acc : 0.8830
     Batch 115 | Loss : 0.2870 | Acc : 0.8680
     Batch 120 | Loss : 0.3127 | Acc : 0.8532
     Batch 125 | Loss : 0.2739 | Acc : 0.8724
     Batch 130 | Loss : 0.3107 | Acc : 0.8590
     Batch 135 | Loss : 0.2815 | Acc : 0.8713
     Batch 140 | Loss : 0.3183 | Acc : 0.8519
     Batch 145 | Loss : 0.2823 | Acc : 0.8724
     Batch 150 | Loss : 0.2994 | Acc : 0.8582
Epoch 00087 | Train Loss : 0.3032 | Eval Loss : 0.3216 | Train acc : 0.8600 | Eval Acc : 0.8522 | Eval Log. Respected : 0.9396
     Batch 000 | Loss : 0.2595 | Acc : 0.8808
     Batch 005 | Loss : 0.3085 | Acc : 0.8558
     Batch 010 | Loss : 0.2639 | Acc : 0.8822
     Batch 015 | Loss : 0.2712 | Acc : 0.8777
     Batch 020 | Loss : 0.3130 | Acc : 0.8525
     Batch 025 | Loss : 0.2967 | Acc : 0.8581
     Batch 030 | Loss : 0.3200 | Acc : 0.8518
     Batch 035 | Loss : 0.3033 | Acc : 0.8592
     Batch 040 | Loss : 0.2716 | Acc : 0.8752
     Batch 045 | Loss : 0.2709 | Acc : 0.8779
     Batch 050 | Loss : 0.2549 | Acc : 0.8859
     Batch 055 | Loss : 0.2871 | Acc : 0.8672
     Batch 060 | Loss : 0.2858 | Acc : 0.8666
     Batch 065 | Loss : 0.3084 | Acc : 0.8574
     Batch 070 | Loss : 0.3197 | Acc : 0.8523
     Batch 075 | Loss : 0.2772 | Acc : 0.8728
     Batch 080 | Loss : 0.3092 | Acc : 0.8608
     Batch 085 | Loss : 0.3906 | Acc : 0.8234
     Batch 090 | Loss : 0.2745 | Acc : 0.8741
     Batch 095 | Loss : 0.3157 | Acc : 0.8520
     Batch 100 | Loss : 0.3160 | Acc : 0.8530
     Batch 105 | Loss : 0.3096 | Acc : 0.8620
     Batch 110 | Loss : 0.2999 | Acc : 0.8616
     Batch 115 | Loss : 0.3335 | Acc : 0.8409
     Batch 120 | Loss : 0.2694 | Acc : 0.8777
     Batch 125 | Loss : 0.2529 | Acc : 0.8867
     Batch 130 | Loss : 0.3161 | Acc : 0.8513
     Batch 135 | Loss : 0.2877 | Acc : 0.8712
     Batch 140 | Loss : 0.3012 | Acc : 0.8617
     Batch 145 | Loss : 0.3521 | Acc : 0.8346
     Batch 150 | Loss : 0.3193 | Acc : 0.8470
Epoch 00088 | Train Loss : 0.3027 | Eval Loss : 0.3157 | Train acc : 0.8606 | Eval Acc : 0.8531 | Eval Log. Respected : 0.9357
     Batch 000 | Loss : 0.2808 | Acc : 0.8717
     Batch 005 | Loss : 0.2790 | Acc : 0.8767
     Batch 010 | Loss : 0.2875 | Acc : 0.8687
     Batch 015 | Loss : 0.2933 | Acc : 0.8641
     Batch 020 | Loss : 0.3070 | Acc : 0.8610
     Batch 025 | Loss : 0.2915 | Acc : 0.8674
     Batch 030 | Loss : 0.2729 | Acc : 0.8765
     Batch 035 | Loss : 0.3508 | Acc : 0.8395
     Batch 040 | Loss : 0.3553 | Acc : 0.8292
     Batch 045 | Loss : 0.3562 | Acc : 0.8394
     Batch 050 | Loss : 0.2541 | Acc : 0.8877
     Batch 055 | Loss : 0.2947 | Acc : 0.8647
     Batch 060 | Loss : 0.2569 | Acc : 0.8827
     Batch 065 | Loss : 0.3459 | Acc : 0.8427
     Batch 070 | Loss : 0.3252 | Acc : 0.8474
     Batch 075 | Loss : 0.3121 | Acc : 0.8567
     Batch 080 | Loss : 0.2619 | Acc : 0.8836
     Batch 085 | Loss : 0.2879 | Acc : 0.8687
     Batch 090 | Loss : 0.2594 | Acc : 0.8816
     Batch 095 | Loss : 0.4082 | Acc : 0.8159
     Batch 100 | Loss : 0.2751 | Acc : 0.8755
     Batch 105 | Loss : 0.3107 | Acc : 0.8619
     Batch 110 | Loss : 0.3486 | Acc : 0.8333
     Batch 115 | Loss : 0.2852 | Acc : 0.8716
     Batch 120 | Loss : 0.2795 | Acc : 0.8710
     Batch 125 | Loss : 0.3282 | Acc : 0.8498
     Batch 130 | Loss : 0.2936 | Acc : 0.8676
     Batch 135 | Loss : 0.2764 | Acc : 0.8710
     Batch 140 | Loss : 0.2558 | Acc : 0.8844
     Batch 145 | Loss : 0.3716 | Acc : 0.8231
     Batch 150 | Loss : 0.3053 | Acc : 0.8590
Epoch 00089 | Train Loss : 0.3023 | Eval Loss : 0.3186 | Train acc : 0.8607 | Eval Acc : 0.8520 | Eval Log. Respected : 0.9315
     Batch 000 | Loss : 0.2968 | Acc : 0.8613
     Batch 005 | Loss : 0.2584 | Acc : 0.8836
     Batch 010 | Loss : 0.3163 | Acc : 0.8543
     Batch 015 | Loss : 0.2944 | Acc : 0.8629
     Batch 020 | Loss : 0.3170 | Acc : 0.8499
     Batch 025 | Loss : 0.2717 | Acc : 0.8772
     Batch 030 | Loss : 0.3491 | Acc : 0.8402
     Batch 035 | Loss : 0.2719 | Acc : 0.8760
     Batch 040 | Loss : 0.3304 | Acc : 0.8459
     Batch 045 | Loss : 0.3258 | Acc : 0.8457
     Batch 050 | Loss : 0.3024 | Acc : 0.8574
     Batch 055 | Loss : 0.2988 | Acc : 0.8602
     Batch 060 | Loss : 0.3451 | Acc : 0.8345
     Batch 065 | Loss : 0.2584 | Acc : 0.8825
     Batch 070 | Loss : 0.2690 | Acc : 0.8763
     Batch 075 | Loss : 0.3228 | Acc : 0.8530
     Batch 080 | Loss : 0.3406 | Acc : 0.8396
     Batch 085 | Loss : 0.3108 | Acc : 0.8585
     Batch 090 | Loss : 0.2970 | Acc : 0.8621
     Batch 095 | Loss : 0.3045 | Acc : 0.8576
     Batch 100 | Loss : 0.3163 | Acc : 0.8543
     Batch 105 | Loss : 0.3194 | Acc : 0.8536
     Batch 110 | Loss : 0.3149 | Acc : 0.8557
     Batch 115 | Loss : 0.2953 | Acc : 0.8631
     Batch 120 | Loss : 0.2801 | Acc : 0.8736
     Batch 125 | Loss : 0.3215 | Acc : 0.8515
     Batch 130 | Loss : 0.2930 | Acc : 0.8610
     Batch 135 | Loss : 0.3092 | Acc : 0.8593
     Batch 140 | Loss : 0.3026 | Acc : 0.8570
     Batch 145 | Loss : 0.3392 | Acc : 0.8444
     Batch 150 | Loss : 0.3155 | Acc : 0.8619
Epoch 00090 | Train Loss : 0.3028 | Eval Loss : 0.3169 | Train acc : 0.8606 | Eval Acc : 0.8523 | Eval Log. Respected : 0.9382
     Batch 000 | Loss : 0.2667 | Acc : 0.8785
     Batch 005 | Loss : 0.3757 | Acc : 0.8270
     Batch 010 | Loss : 0.2941 | Acc : 0.8655
     Batch 015 | Loss : 0.2990 | Acc : 0.8639
     Batch 020 | Loss : 0.3150 | Acc : 0.8539
     Batch 025 | Loss : 0.3143 | Acc : 0.8524
     Batch 030 | Loss : 0.2788 | Acc : 0.8788
     Batch 035 | Loss : 0.2985 | Acc : 0.8634
     Batch 040 | Loss : 0.2477 | Acc : 0.8881
     Batch 045 | Loss : 0.2754 | Acc : 0.8785
     Batch 050 | Loss : 0.2726 | Acc : 0.8797
     Batch 055 | Loss : 0.2792 | Acc : 0.8753
     Batch 060 | Loss : 0.2948 | Acc : 0.8626
     Batch 065 | Loss : 0.3077 | Acc : 0.8583
     Batch 070 | Loss : 0.2896 | Acc : 0.8697
     Batch 075 | Loss : 0.2854 | Acc : 0.8685
     Batch 080 | Loss : 0.3494 | Acc : 0.8329
     Batch 085 | Loss : 0.3442 | Acc : 0.8411
     Batch 090 | Loss : 0.3099 | Acc : 0.8541
     Batch 095 | Loss : 0.3231 | Acc : 0.8482
     Batch 100 | Loss : 0.3497 | Acc : 0.8359
     Batch 105 | Loss : 0.3153 | Acc : 0.8548
     Batch 110 | Loss : 0.2928 | Acc : 0.8649
     Batch 115 | Loss : 0.2980 | Acc : 0.8608
     Batch 120 | Loss : 0.2500 | Acc : 0.8881
     Batch 125 | Loss : 0.3206 | Acc : 0.8485
     Batch 130 | Loss : 0.2606 | Acc : 0.8838
     Batch 135 | Loss : 0.3801 | Acc : 0.8258
     Batch 140 | Loss : 0.3068 | Acc : 0.8582
     Batch 145 | Loss : 0.2804 | Acc : 0.8719
     Batch 150 | Loss : 0.3235 | Acc : 0.8464
Epoch 00091 | Train Loss : 0.3037 | Eval Loss : 0.3156 | Train acc : 0.8606 | Eval Acc : 0.8528 | Eval Log. Respected : 0.9342
     Batch 000 | Loss : 0.2669 | Acc : 0.8802
     Batch 005 | Loss : 0.3654 | Acc : 0.8250
     Batch 010 | Loss : 0.3177 | Acc : 0.8534
     Batch 015 | Loss : 0.3528 | Acc : 0.8346
     Batch 020 | Loss : 0.3003 | Acc : 0.8633
     Batch 025 | Loss : 0.3489 | Acc : 0.8371
     Batch 030 | Loss : 0.2588 | Acc : 0.8804
     Batch 035 | Loss : 0.2508 | Acc : 0.8876
     Batch 040 | Loss : 0.3068 | Acc : 0.8558
     Batch 045 | Loss : 0.2990 | Acc : 0.8628
     Batch 050 | Loss : 0.2899 | Acc : 0.8714
     Batch 055 | Loss : 0.2982 | Acc : 0.8622
     Batch 060 | Loss : 0.2458 | Acc : 0.8922
     Batch 065 | Loss : 0.2526 | Acc : 0.8851
     Batch 070 | Loss : 0.2718 | Acc : 0.8777
     Batch 075 | Loss : 0.3291 | Acc : 0.8468
     Batch 080 | Loss : 0.3122 | Acc : 0.8551
     Batch 085 | Loss : 0.2848 | Acc : 0.8676
     Batch 090 | Loss : 0.3352 | Acc : 0.8437
     Batch 095 | Loss : 0.2984 | Acc : 0.8596
     Batch 100 | Loss : 0.2730 | Acc : 0.8747
     Batch 105 | Loss : 0.3166 | Acc : 0.8543
     Batch 110 | Loss : 0.2801 | Acc : 0.8689
     Batch 115 | Loss : 0.2731 | Acc : 0.8767
     Batch 120 | Loss : 0.3407 | Acc : 0.8407
     Batch 125 | Loss : 0.2814 | Acc : 0.8725
     Batch 130 | Loss : 0.3542 | Acc : 0.8344
     Batch 135 | Loss : 0.3350 | Acc : 0.8463
     Batch 140 | Loss : 0.3397 | Acc : 0.8342
     Batch 145 | Loss : 0.3079 | Acc : 0.8554
     Batch 150 | Loss : 0.2685 | Acc : 0.8792
Epoch 00092 | Train Loss : 0.3024 | Eval Loss : 0.3204 | Train acc : 0.8608 | Eval Acc : 0.8527 | Eval Log. Respected : 0.9371
     Batch 000 | Loss : 0.2921 | Acc : 0.8648
     Batch 005 | Loss : 0.3159 | Acc : 0.8540
     Batch 010 | Loss : 0.3044 | Acc : 0.8592
     Batch 015 | Loss : 0.3947 | Acc : 0.8190
     Batch 020 | Loss : 0.3497 | Acc : 0.8294
     Batch 025 | Loss : 0.2888 | Acc : 0.8670
     Batch 030 | Loss : 0.2705 | Acc : 0.8775
     Batch 035 | Loss : 0.2602 | Acc : 0.8811
     Batch 040 | Loss : 0.3365 | Acc : 0.8411
     Batch 045 | Loss : 0.3059 | Acc : 0.8566
     Batch 050 | Loss : 0.3334 | Acc : 0.8521
     Batch 055 | Loss : 0.2446 | Acc : 0.8896
     Batch 060 | Loss : 0.2940 | Acc : 0.8686
     Batch 065 | Loss : 0.3108 | Acc : 0.8530
     Batch 070 | Loss : 0.3149 | Acc : 0.8548
     Batch 075 | Loss : 0.3144 | Acc : 0.8550
     Batch 080 | Loss : 0.2433 | Acc : 0.8946
     Batch 085 | Loss : 0.2812 | Acc : 0.8698
     Batch 090 | Loss : 0.2949 | Acc : 0.8578
     Batch 095 | Loss : 0.2899 | Acc : 0.8631
     Batch 100 | Loss : 0.2673 | Acc : 0.8781
     Batch 105 | Loss : 0.2826 | Acc : 0.8684
     Batch 110 | Loss : 0.2595 | Acc : 0.8814
     Batch 115 | Loss : 0.3787 | Acc : 0.8230
     Batch 120 | Loss : 0.3786 | Acc : 0.8172
     Batch 125 | Loss : 0.2805 | Acc : 0.8751
     Batch 130 | Loss : 0.2946 | Acc : 0.8649
     Batch 135 | Loss : 0.3119 | Acc : 0.8553
     Batch 140 | Loss : 0.3087 | Acc : 0.8578
     Batch 145 | Loss : 0.3292 | Acc : 0.8462
     Batch 150 | Loss : 0.2945 | Acc : 0.8636
Epoch 00093 | Train Loss : 0.3024 | Eval Loss : 0.3194 | Train acc : 0.8605 | Eval Acc : 0.8540 | Eval Log. Respected : 0.9288
     Batch 000 | Loss : 0.3004 | Acc : 0.8611
     Batch 005 | Loss : 0.2990 | Acc : 0.8628
     Batch 010 | Loss : 0.3404 | Acc : 0.8446
     Batch 015 | Loss : 0.3191 | Acc : 0.8541
     Batch 020 | Loss : 0.2986 | Acc : 0.8600
     Batch 025 | Loss : 0.3157 | Acc : 0.8566
     Batch 030 | Loss : 0.3220 | Acc : 0.8497
     Batch 035 | Loss : 0.2574 | Acc : 0.8828
     Batch 040 | Loss : 0.2862 | Acc : 0.8670
     Batch 045 | Loss : 0.2857 | Acc : 0.8672
     Batch 050 | Loss : 0.3572 | Acc : 0.8282
     Batch 055 | Loss : 0.2846 | Acc : 0.8693
     Batch 060 | Loss : 0.2853 | Acc : 0.8700
     Batch 065 | Loss : 0.2846 | Acc : 0.8689
     Batch 070 | Loss : 0.2482 | Acc : 0.8882
     Batch 075 | Loss : 0.2694 | Acc : 0.8781
     Batch 080 | Loss : 0.3433 | Acc : 0.8432
     Batch 085 | Loss : 0.3098 | Acc : 0.8548
     Batch 090 | Loss : 0.2911 | Acc : 0.8680
     Batch 095 | Loss : 0.2856 | Acc : 0.8661
     Batch 100 | Loss : 0.2959 | Acc : 0.8631
     Batch 105 | Loss : 0.2906 | Acc : 0.8666
     Batch 110 | Loss : 0.2763 | Acc : 0.8746
     Batch 115 | Loss : 0.3313 | Acc : 0.8442
     Batch 120 | Loss : 0.2840 | Acc : 0.8707
     Batch 125 | Loss : 0.2795 | Acc : 0.8710
     Batch 130 | Loss : 0.2893 | Acc : 0.8703
     Batch 135 | Loss : 0.2825 | Acc : 0.8727
     Batch 140 | Loss : 0.3544 | Acc : 0.8321
     Batch 145 | Loss : 0.3294 | Acc : 0.8461
     Batch 150 | Loss : 0.3546 | Acc : 0.8304
Epoch 00094 | Train Loss : 0.3010 | Eval Loss : 0.3180 | Train acc : 0.8614 | Eval Acc : 0.8517 | Eval Log. Respected : 0.9275
     Batch 000 | Loss : 0.2907 | Acc : 0.8673
     Batch 005 | Loss : 0.2951 | Acc : 0.8673
     Batch 010 | Loss : 0.2979 | Acc : 0.8620
     Batch 015 | Loss : 0.3189 | Acc : 0.8487
     Batch 020 | Loss : 0.3478 | Acc : 0.8353
     Batch 025 | Loss : 0.3161 | Acc : 0.8516
     Batch 030 | Loss : 0.3059 | Acc : 0.8589
     Batch 035 | Loss : 0.3051 | Acc : 0.8630
     Batch 040 | Loss : 0.2719 | Acc : 0.8761
     Batch 045 | Loss : 0.2814 | Acc : 0.8714
     Batch 050 | Loss : 0.3187 | Acc : 0.8521
     Batch 055 | Loss : 0.3114 | Acc : 0.8548
     Batch 060 | Loss : 0.3020 | Acc : 0.8583
     Batch 065 | Loss : 0.2993 | Acc : 0.8606
     Batch 070 | Loss : 0.3093 | Acc : 0.8546
     Batch 075 | Loss : 0.2901 | Acc : 0.8671
     Batch 080 | Loss : 0.3360 | Acc : 0.8436
     Batch 085 | Loss : 0.2972 | Acc : 0.8630
     Batch 090 | Loss : 0.2968 | Acc : 0.8644
     Batch 095 | Loss : 0.3305 | Acc : 0.8489
     Batch 100 | Loss : 0.3238 | Acc : 0.8508
     Batch 105 | Loss : 0.2933 | Acc : 0.8660
     Batch 110 | Loss : 0.2889 | Acc : 0.8682
     Batch 115 | Loss : 0.3100 | Acc : 0.8611
     Batch 120 | Loss : 0.2691 | Acc : 0.8825
     Batch 125 | Loss : 0.3143 | Acc : 0.8549
     Batch 130 | Loss : 0.2797 | Acc : 0.8748
     Batch 135 | Loss : 0.2961 | Acc : 0.8662
     Batch 140 | Loss : 0.2570 | Acc : 0.8871
     Batch 145 | Loss : 0.2753 | Acc : 0.8785
     Batch 150 | Loss : 0.3258 | Acc : 0.8453
Epoch 00095 | Train Loss : 0.3020 | Eval Loss : 0.3144 | Train acc : 0.8609 | Eval Acc : 0.8533 | Eval Log. Respected : 0.9313
     Batch 000 | Loss : 0.2941 | Acc : 0.8624
     Batch 005 | Loss : 0.3566 | Acc : 0.8339
     Batch 010 | Loss : 0.2655 | Acc : 0.8804
     Batch 015 | Loss : 0.3069 | Acc : 0.8589
     Batch 020 | Loss : 0.3293 | Acc : 0.8452
     Batch 025 | Loss : 0.2921 | Acc : 0.8684
     Batch 030 | Loss : 0.3474 | Acc : 0.8364
     Batch 035 | Loss : 0.2663 | Acc : 0.8766
     Batch 040 | Loss : 0.2879 | Acc : 0.8717
     Batch 045 | Loss : 0.3252 | Acc : 0.8463
     Batch 050 | Loss : 0.3097 | Acc : 0.8570
     Batch 055 | Loss : 0.2588 | Acc : 0.8834
     Batch 060 | Loss : 0.3187 | Acc : 0.8525
     Batch 065 | Loss : 0.3382 | Acc : 0.8402
     Batch 070 | Loss : 0.2955 | Acc : 0.8663
     Batch 075 | Loss : 0.2924 | Acc : 0.8718
     Batch 080 | Loss : 0.3279 | Acc : 0.8444
     Batch 085 | Loss : 0.3430 | Acc : 0.8403
     Batch 090 | Loss : 0.3269 | Acc : 0.8459
     Batch 095 | Loss : 0.2829 | Acc : 0.8715
     Batch 100 | Loss : 0.3295 | Acc : 0.8471
     Batch 105 | Loss : 0.3216 | Acc : 0.8521
     Batch 110 | Loss : 0.2950 | Acc : 0.8604
     Batch 115 | Loss : 0.3211 | Acc : 0.8506
     Batch 120 | Loss : 0.2409 | Acc : 0.8913
     Batch 125 | Loss : 0.3071 | Acc : 0.8569
     Batch 130 | Loss : 0.3283 | Acc : 0.8503
     Batch 135 | Loss : 0.3417 | Acc : 0.8431
     Batch 140 | Loss : 0.3164 | Acc : 0.8504
     Batch 145 | Loss : 0.3154 | Acc : 0.8526
     Batch 150 | Loss : 0.3119 | Acc : 0.8516
Epoch 00096 | Train Loss : 0.3009 | Eval Loss : 0.3140 | Train acc : 0.8614 | Eval Acc : 0.8539 | Eval Log. Respected : 0.9287
     Batch 000 | Loss : 0.2719 | Acc : 0.8811
     Batch 005 | Loss : 0.2663 | Acc : 0.8804
     Batch 010 | Loss : 0.3466 | Acc : 0.8358
     Batch 015 | Loss : 0.2763 | Acc : 0.8757
     Batch 020 | Loss : 0.3048 | Acc : 0.8589
     Batch 025 | Loss : 0.3029 | Acc : 0.8572
     Batch 030 | Loss : 0.3253 | Acc : 0.8465
     Batch 035 | Loss : 0.2977 | Acc : 0.8623
     Batch 040 | Loss : 0.2624 | Acc : 0.8838
     Batch 045 | Loss : 0.3496 | Acc : 0.8413
     Batch 050 | Loss : 0.2995 | Acc : 0.8590
     Batch 055 | Loss : 0.3115 | Acc : 0.8582
     Batch 060 | Loss : 0.3154 | Acc : 0.8518
     Batch 065 | Loss : 0.2725 | Acc : 0.8790
     Batch 070 | Loss : 0.3488 | Acc : 0.8354
     Batch 075 | Loss : 0.3066 | Acc : 0.8638
     Batch 080 | Loss : 0.2734 | Acc : 0.8739
     Batch 085 | Loss : 0.3349 | Acc : 0.8479
     Batch 090 | Loss : 0.2813 | Acc : 0.8716
     Batch 095 | Loss : 0.2800 | Acc : 0.8733
     Batch 100 | Loss : 0.2753 | Acc : 0.8769
     Batch 105 | Loss : 0.3239 | Acc : 0.8495
     Batch 110 | Loss : 0.2974 | Acc : 0.8622
     Batch 115 | Loss : 0.2791 | Acc : 0.8709
     Batch 120 | Loss : 0.2859 | Acc : 0.8699
     Batch 125 | Loss : 0.2716 | Acc : 0.8795
     Batch 130 | Loss : 0.3358 | Acc : 0.8379
     Batch 135 | Loss : 0.2852 | Acc : 0.8697
     Batch 140 | Loss : 0.3180 | Acc : 0.8508
     Batch 145 | Loss : 0.2682 | Acc : 0.8777
     Batch 150 | Loss : 0.3183 | Acc : 0.8534
Epoch 00097 | Train Loss : 0.3023 | Eval Loss : 0.3160 | Train acc : 0.8610 | Eval Acc : 0.8531 | Eval Log. Respected : 0.9306
     Batch 000 | Loss : 0.2874 | Acc : 0.8700
     Batch 005 | Loss : 0.3224 | Acc : 0.8469
     Batch 010 | Loss : 0.3193 | Acc : 0.8516
     Batch 015 | Loss : 0.2572 | Acc : 0.8822
     Batch 020 | Loss : 0.3070 | Acc : 0.8584
     Batch 025 | Loss : 0.2852 | Acc : 0.8694
     Batch 030 | Loss : 0.3136 | Acc : 0.8540
     Batch 035 | Loss : 0.2732 | Acc : 0.8794
     Batch 040 | Loss : 0.3242 | Acc : 0.8501
     Batch 045 | Loss : 0.2693 | Acc : 0.8795
     Batch 050 | Loss : 0.2849 | Acc : 0.8688
     Batch 055 | Loss : 0.2941 | Acc : 0.8636
     Batch 060 | Loss : 0.3067 | Acc : 0.8581
     Batch 065 | Loss : 0.3042 | Acc : 0.8559
     Batch 070 | Loss : 0.2729 | Acc : 0.8772
     Batch 075 | Loss : 0.2554 | Acc : 0.8859
     Batch 080 | Loss : 0.2928 | Acc : 0.8636
     Batch 085 | Loss : 0.2919 | Acc : 0.8641
     Batch 090 | Loss : 0.2919 | Acc : 0.8678
     Batch 095 | Loss : 0.3617 | Acc : 0.8319
     Batch 100 | Loss : 0.2990 | Acc : 0.8608
     Batch 105 | Loss : 0.3346 | Acc : 0.8417
     Batch 110 | Loss : 0.2576 | Acc : 0.8823
     Batch 115 | Loss : 0.3536 | Acc : 0.8311
     Batch 120 | Loss : 0.3147 | Acc : 0.8498
     Batch 125 | Loss : 0.2613 | Acc : 0.8835
     Batch 130 | Loss : 0.2439 | Acc : 0.8904
     Batch 135 | Loss : 0.3133 | Acc : 0.8549
     Batch 140 | Loss : 0.2972 | Acc : 0.8602
     Batch 145 | Loss : 0.3466 | Acc : 0.8428
     Batch 150 | Loss : 0.2992 | Acc : 0.8614
Epoch 00098 | Train Loss : 0.2999 | Eval Loss : 0.3168 | Train acc : 0.8619 | Eval Acc : 0.8530 | Eval Log. Respected : 0.9400
     Batch 000 | Loss : 0.3012 | Acc : 0.8574
     Batch 005 | Loss : 0.2581 | Acc : 0.8835
     Batch 010 | Loss : 0.3064 | Acc : 0.8577
     Batch 015 | Loss : 0.3249 | Acc : 0.8477
     Batch 020 | Loss : 0.3246 | Acc : 0.8484
     Batch 025 | Loss : 0.3120 | Acc : 0.8550
     Batch 030 | Loss : 0.2453 | Acc : 0.8867
     Batch 035 | Loss : 0.2869 | Acc : 0.8676
     Batch 040 | Loss : 0.2640 | Acc : 0.8835
     Batch 045 | Loss : 0.3030 | Acc : 0.8593
     Batch 050 | Loss : 0.3302 | Acc : 0.8457
     Batch 055 | Loss : 0.2986 | Acc : 0.8627
     Batch 060 | Loss : 0.2605 | Acc : 0.8828
     Batch 065 | Loss : 0.2765 | Acc : 0.8717
     Batch 070 | Loss : 0.3106 | Acc : 0.8564
     Batch 075 | Loss : 0.2750 | Acc : 0.8740
     Batch 080 | Loss : 0.4119 | Acc : 0.8144
     Batch 085 | Loss : 0.2703 | Acc : 0.8800
     Batch 090 | Loss : 0.2939 | Acc : 0.8649
     Batch 095 | Loss : 0.2604 | Acc : 0.8822
     Batch 100 | Loss : 0.2866 | Acc : 0.8693
     Batch 105 | Loss : 0.3349 | Acc : 0.8405
     Batch 110 | Loss : 0.2621 | Acc : 0.8819
     Batch 115 | Loss : 0.2681 | Acc : 0.8806
     Batch 120 | Loss : 0.3001 | Acc : 0.8645
     Batch 125 | Loss : 0.2636 | Acc : 0.8835
     Batch 130 | Loss : 0.3041 | Acc : 0.8588
     Batch 135 | Loss : 0.3472 | Acc : 0.8416
     Batch 140 | Loss : 0.2698 | Acc : 0.8777
     Batch 145 | Loss : 0.2838 | Acc : 0.8682
     Batch 150 | Loss : 0.2985 | Acc : 0.8658
Epoch 00099 | Train Loss : 0.3008 | Eval Loss : 0.3201 | Train acc : 0.8615 | Eval Acc : 0.8513 | Eval Log. Respected : 0.9325
     Batch 000 | Loss : 0.2776 | Acc : 0.8781
     Batch 005 | Loss : 0.3345 | Acc : 0.8394
     Batch 010 | Loss : 0.3305 | Acc : 0.8411
     Batch 015 | Loss : 0.2767 | Acc : 0.8719
     Batch 020 | Loss : 0.2910 | Acc : 0.8670
     Batch 025 | Loss : 0.3882 | Acc : 0.8239
     Batch 030 | Loss : 0.3068 | Acc : 0.8575
     Batch 035 | Loss : 0.3153 | Acc : 0.8544
     Batch 040 | Loss : 0.3039 | Acc : 0.8579
     Batch 045 | Loss : 0.3234 | Acc : 0.8484
     Batch 050 | Loss : 0.2649 | Acc : 0.8804
     Batch 055 | Loss : 0.2725 | Acc : 0.8760
     Batch 060 | Loss : 0.2858 | Acc : 0.8675
     Batch 065 | Loss : 0.3058 | Acc : 0.8608
     Batch 070 | Loss : 0.2513 | Acc : 0.8868
     Batch 075 | Loss : 0.3362 | Acc : 0.8418
     Batch 080 | Loss : 0.2541 | Acc : 0.8823
     Batch 085 | Loss : 0.3132 | Acc : 0.8533
     Batch 090 | Loss : 0.2574 | Acc : 0.8863
     Batch 095 | Loss : 0.2892 | Acc : 0.8661
     Batch 100 | Loss : 0.2809 | Acc : 0.8701
     Batch 105 | Loss : 0.2902 | Acc : 0.8639
     Batch 110 | Loss : 0.3033 | Acc : 0.8582
     Batch 115 | Loss : 0.2771 | Acc : 0.8782
     Batch 120 | Loss : 0.3022 | Acc : 0.8599
     Batch 125 | Loss : 0.2849 | Acc : 0.8706
     Batch 130 | Loss : 0.3174 | Acc : 0.8506
     Batch 135 | Loss : 0.2703 | Acc : 0.8769
     Batch 140 | Loss : 0.2889 | Acc : 0.8664
     Batch 145 | Loss : 0.2669 | Acc : 0.8781
     Batch 150 | Loss : 0.2971 | Acc : 0.8655
Epoch 00100 | Train Loss : 0.3009 | Eval Loss : 0.3137 | Train acc : 0.8615 | Eval Acc : 0.8541 | Eval Log. Respected : 0.9273
     Batch 000 | Loss : 0.2921 | Acc : 0.8650
     Batch 005 | Loss : 0.3126 | Acc : 0.8589
     Batch 010 | Loss : 0.3084 | Acc : 0.8591
     Batch 015 | Loss : 0.3435 | Acc : 0.8483
     Batch 020 | Loss : 0.3436 | Acc : 0.8308
     Batch 025 | Loss : 0.3007 | Acc : 0.8607
     Batch 030 | Loss : 0.3097 | Acc : 0.8609
     Batch 035 | Loss : 0.3182 | Acc : 0.8551
     Batch 040 | Loss : 0.2847 | Acc : 0.8677
     Batch 045 | Loss : 0.3049 | Acc : 0.8604
     Batch 050 | Loss : 0.2656 | Acc : 0.8813
     Batch 055 | Loss : 0.2365 | Acc : 0.8950
     Batch 060 | Loss : 0.3442 | Acc : 0.8424
     Batch 065 | Loss : 0.3158 | Acc : 0.8528
     Batch 070 | Loss : 0.3464 | Acc : 0.8368
     Batch 075 | Loss : 0.2658 | Acc : 0.8832
     Batch 080 | Loss : 0.2723 | Acc : 0.8742
     Batch 085 | Loss : 0.3111 | Acc : 0.8542
     Batch 090 | Loss : 0.3224 | Acc : 0.8480
     Batch 095 | Loss : 0.2873 | Acc : 0.8711
     Batch 100 | Loss : 0.3320 | Acc : 0.8402
     Batch 105 | Loss : 0.3039 | Acc : 0.8597
     Batch 110 | Loss : 0.3257 | Acc : 0.8431
     Batch 115 | Loss : 0.2803 | Acc : 0.8698
     Batch 120 | Loss : 0.3743 | Acc : 0.8272
     Batch 125 | Loss : 0.3223 | Acc : 0.8493
     Batch 130 | Loss : 0.2952 | Acc : 0.8619
     Batch 135 | Loss : 0.3320 | Acc : 0.8430
     Batch 140 | Loss : 0.3230 | Acc : 0.8481
     Batch 145 | Loss : 0.4011 | Acc : 0.8167
     Batch 150 | Loss : 0.2694 | Acc : 0.8777
Epoch 00101 | Train Loss : 0.2990 | Eval Loss : 0.3178 | Train acc : 0.8623 | Eval Acc : 0.8519 | Eval Log. Respected : 0.9303
     Batch 000 | Loss : 0.3205 | Acc : 0.8514
     Batch 005 | Loss : 0.2962 | Acc : 0.8585
     Batch 010 | Loss : 0.3142 | Acc : 0.8568
     Batch 015 | Loss : 0.2643 | Acc : 0.8810
     Batch 020 | Loss : 0.3290 | Acc : 0.8496
     Batch 025 | Loss : 0.3052 | Acc : 0.8604
     Batch 030 | Loss : 0.2550 | Acc : 0.8878
     Batch 035 | Loss : 0.3161 | Acc : 0.8512
     Batch 040 | Loss : 0.2977 | Acc : 0.8650
     Batch 045 | Loss : 0.2956 | Acc : 0.8633
     Batch 050 | Loss : 0.3163 | Acc : 0.8561
     Batch 055 | Loss : 0.2799 | Acc : 0.8759
     Batch 060 | Loss : 0.2653 | Acc : 0.8780
     Batch 065 | Loss : 0.3256 | Acc : 0.8481
     Batch 070 | Loss : 0.3065 | Acc : 0.8560
     Batch 075 | Loss : 0.2694 | Acc : 0.8808
     Batch 080 | Loss : 0.3018 | Acc : 0.8612
     Batch 085 | Loss : 0.3168 | Acc : 0.8536
     Batch 090 | Loss : 0.2461 | Acc : 0.8879
     Batch 095 | Loss : 0.2915 | Acc : 0.8660
     Batch 100 | Loss : 0.2455 | Acc : 0.8917
     Batch 105 | Loss : 0.2971 | Acc : 0.8657
     Batch 110 | Loss : 0.3026 | Acc : 0.8619
     Batch 115 | Loss : 0.2885 | Acc : 0.8644
     Batch 120 | Loss : 0.2678 | Acc : 0.8773
     Batch 125 | Loss : 0.2700 | Acc : 0.8770
     Batch 130 | Loss : 0.2733 | Acc : 0.8803
     Batch 135 | Loss : 0.3345 | Acc : 0.8436
     Batch 140 | Loss : 0.2687 | Acc : 0.8791
     Batch 145 | Loss : 0.3149 | Acc : 0.8516
     Batch 150 | Loss : 0.3852 | Acc : 0.8211
Epoch 00102 | Train Loss : 0.2996 | Eval Loss : 0.3152 | Train acc : 0.8620 | Eval Acc : 0.8539 | Eval Log. Respected : 0.9405
     Batch 000 | Loss : 0.3000 | Acc : 0.8618
     Batch 005 | Loss : 0.3102 | Acc : 0.8577
     Batch 010 | Loss : 0.2661 | Acc : 0.8815
     Batch 015 | Loss : 0.2766 | Acc : 0.8715
     Batch 020 | Loss : 0.2902 | Acc : 0.8662
     Batch 025 | Loss : 0.2997 | Acc : 0.8616
     Batch 030 | Loss : 0.2772 | Acc : 0.8758
     Batch 035 | Loss : 0.3323 | Acc : 0.8459
     Batch 040 | Loss : 0.3224 | Acc : 0.8488
     Batch 045 | Loss : 0.2687 | Acc : 0.8792
     Batch 050 | Loss : 0.3377 | Acc : 0.8412
     Batch 055 | Loss : 0.2949 | Acc : 0.8656
     Batch 060 | Loss : 0.2714 | Acc : 0.8754
     Batch 065 | Loss : 0.3277 | Acc : 0.8448
     Batch 070 | Loss : 0.2556 | Acc : 0.8816
     Batch 075 | Loss : 0.3546 | Acc : 0.8323
     Batch 080 | Loss : 0.2792 | Acc : 0.8720
     Batch 085 | Loss : 0.3150 | Acc : 0.8542
     Batch 090 | Loss : 0.2730 | Acc : 0.8766
     Batch 095 | Loss : 0.3346 | Acc : 0.8412
     Batch 100 | Loss : 0.2431 | Acc : 0.8879
     Batch 105 | Loss : 0.3082 | Acc : 0.8579
     Batch 110 | Loss : 0.2538 | Acc : 0.8835
     Batch 115 | Loss : 0.2851 | Acc : 0.8714
     Batch 120 | Loss : 0.3087 | Acc : 0.8582
     Batch 125 | Loss : 0.3257 | Acc : 0.8469
     Batch 130 | Loss : 0.3000 | Acc : 0.8600
     Batch 135 | Loss : 0.3313 | Acc : 0.8446
     Batch 140 | Loss : 0.2600 | Acc : 0.8872
     Batch 145 | Loss : 0.2850 | Acc : 0.8687
     Batch 150 | Loss : 0.2841 | Acc : 0.8756
Epoch 00103 | Train Loss : 0.2989 | Eval Loss : 0.3160 | Train acc : 0.8623 | Eval Acc : 0.8528 | Eval Log. Respected : 0.9393
     Batch 000 | Loss : 0.2914 | Acc : 0.8653
     Batch 005 | Loss : 0.2881 | Acc : 0.8661
     Batch 010 | Loss : 0.2857 | Acc : 0.8682
     Batch 015 | Loss : 0.3066 | Acc : 0.8557
     Batch 020 | Loss : 0.2866 | Acc : 0.8688
     Batch 025 | Loss : 0.2764 | Acc : 0.8778
     Batch 030 | Loss : 0.3601 | Acc : 0.8327
     Batch 035 | Loss : 0.3063 | Acc : 0.8593
     Batch 040 | Loss : 0.2595 | Acc : 0.8834
     Batch 045 | Loss : 0.2662 | Acc : 0.8833
     Batch 050 | Loss : 0.2736 | Acc : 0.8734
     Batch 055 | Loss : 0.3142 | Acc : 0.8510
     Batch 060 | Loss : 0.2770 | Acc : 0.8709
     Batch 065 | Loss : 0.2871 | Acc : 0.8703
     Batch 070 | Loss : 0.2737 | Acc : 0.8752
     Batch 075 | Loss : 0.2519 | Acc : 0.8864
     Batch 080 | Loss : 0.2766 | Acc : 0.8748
     Batch 085 | Loss : 0.2867 | Acc : 0.8724
     Batch 090 | Loss : 0.3667 | Acc : 0.8246
     Batch 095 | Loss : 0.3182 | Acc : 0.8504
     Batch 100 | Loss : 0.2606 | Acc : 0.8806
     Batch 105 | Loss : 0.3003 | Acc : 0.8605
     Batch 110 | Loss : 0.2584 | Acc : 0.8867
     Batch 115 | Loss : 0.3364 | Acc : 0.8467
     Batch 120 | Loss : 0.2765 | Acc : 0.8748
     Batch 125 | Loss : 0.3249 | Acc : 0.8480
     Batch 130 | Loss : 0.2679 | Acc : 0.8820
     Batch 135 | Loss : 0.4485 | Acc : 0.8053
     Batch 140 | Loss : 0.3021 | Acc : 0.8643
     Batch 145 | Loss : 0.3073 | Acc : 0.8581
     Batch 150 | Loss : 0.3209 | Acc : 0.8488
Epoch 00104 | Train Loss : 0.2996 | Eval Loss : 0.3179 | Train acc : 0.8622 | Eval Acc : 0.8520 | Eval Log. Respected : 0.9496
     Batch 000 | Loss : 0.2958 | Acc : 0.8697
     Batch 005 | Loss : 0.2710 | Acc : 0.8766
     Batch 010 | Loss : 0.2957 | Acc : 0.8634
     Batch 015 | Loss : 0.3171 | Acc : 0.8535
     Batch 020 | Loss : 0.2792 | Acc : 0.8743
     Batch 025 | Loss : 0.2735 | Acc : 0.8763
     Batch 030 | Loss : 0.2669 | Acc : 0.8776
     Batch 035 | Loss : 0.2794 | Acc : 0.8706
     Batch 040 | Loss : 0.3153 | Acc : 0.8544
     Batch 045 | Loss : 0.2755 | Acc : 0.8772
     Batch 050 | Loss : 0.2987 | Acc : 0.8664
     Batch 055 | Loss : 0.3484 | Acc : 0.8330
     Batch 060 | Loss : 0.3237 | Acc : 0.8477
     Batch 065 | Loss : 0.3607 | Acc : 0.8328
     Batch 070 | Loss : 0.2955 | Acc : 0.8621
     Batch 075 | Loss : 0.3325 | Acc : 0.8416
     Batch 080 | Loss : 0.2968 | Acc : 0.8648
     Batch 085 | Loss : 0.3064 | Acc : 0.8597
     Batch 090 | Loss : 0.2843 | Acc : 0.8700
     Batch 095 | Loss : 0.3296 | Acc : 0.8451
     Batch 100 | Loss : 0.2621 | Acc : 0.8797
     Batch 105 | Loss : 0.3431 | Acc : 0.8404
     Batch 110 | Loss : 0.3040 | Acc : 0.8571
     Batch 115 | Loss : 0.3155 | Acc : 0.8513
     Batch 120 | Loss : 0.2885 | Acc : 0.8686
     Batch 125 | Loss : 0.3136 | Acc : 0.8590
     Batch 130 | Loss : 0.3007 | Acc : 0.8637
     Batch 135 | Loss : 0.2701 | Acc : 0.8753
     Batch 140 | Loss : 0.3205 | Acc : 0.8526
     Batch 145 | Loss : 0.2823 | Acc : 0.8703
     Batch 150 | Loss : 0.2904 | Acc : 0.8673
Epoch 00105 | Train Loss : 0.2988 | Eval Loss : 0.3167 | Train acc : 0.8623 | Eval Acc : 0.8532 | Eval Log. Respected : 0.9363
     Batch 000 | Loss : 0.2440 | Acc : 0.8910
     Batch 005 | Loss : 0.3331 | Acc : 0.8508
     Batch 010 | Loss : 0.3048 | Acc : 0.8567
     Batch 015 | Loss : 0.2663 | Acc : 0.8805
     Batch 020 | Loss : 0.3154 | Acc : 0.8511
     Batch 025 | Loss : 0.2850 | Acc : 0.8691
     Batch 030 | Loss : 0.2753 | Acc : 0.8731
     Batch 035 | Loss : 0.2434 | Acc : 0.8905
     Batch 040 | Loss : 0.3061 | Acc : 0.8612
     Batch 045 | Loss : 0.2752 | Acc : 0.8743
     Batch 050 | Loss : 0.2730 | Acc : 0.8754
     Batch 055 | Loss : 0.2751 | Acc : 0.8771
     Batch 060 | Loss : 0.3103 | Acc : 0.8548
     Batch 065 | Loss : 0.3062 | Acc : 0.8592
     Batch 070 | Loss : 0.3241 | Acc : 0.8486
     Batch 075 | Loss : 0.3469 | Acc : 0.8328
     Batch 080 | Loss : 0.3137 | Acc : 0.8537
     Batch 085 | Loss : 0.2724 | Acc : 0.8771
     Batch 090 | Loss : 0.3108 | Acc : 0.8545
     Batch 095 | Loss : 0.3148 | Acc : 0.8574
     Batch 100 | Loss : 0.2790 | Acc : 0.8729
     Batch 105 | Loss : 0.2705 | Acc : 0.8789
     Batch 110 | Loss : 0.2850 | Acc : 0.8679
     Batch 115 | Loss : 0.2765 | Acc : 0.8733
     Batch 120 | Loss : 0.3236 | Acc : 0.8446
     Batch 125 | Loss : 0.3612 | Acc : 0.8281
     Batch 130 | Loss : 0.3067 | Acc : 0.8573
     Batch 135 | Loss : 0.2886 | Acc : 0.8669
     Batch 140 | Loss : 0.2327 | Acc : 0.8980
     Batch 145 | Loss : 0.2793 | Acc : 0.8733
     Batch 150 | Loss : 0.2970 | Acc : 0.8616
Epoch 00106 | Train Loss : 0.2989 | Eval Loss : 0.3147 | Train acc : 0.8623 | Eval Acc : 0.8538 | Eval Log. Respected : 0.9229
     Batch 000 | Loss : 0.3295 | Acc : 0.8452
     Batch 005 | Loss : 0.2504 | Acc : 0.8884
     Batch 010 | Loss : 0.3033 | Acc : 0.8602
     Batch 015 | Loss : 0.2873 | Acc : 0.8671
     Batch 020 | Loss : 0.3201 | Acc : 0.8498
     Batch 025 | Loss : 0.2859 | Acc : 0.8667
     Batch 030 | Loss : 0.2963 | Acc : 0.8620
     Batch 035 | Loss : 0.3075 | Acc : 0.8559
     Batch 040 | Loss : 0.2388 | Acc : 0.8920
     Batch 045 | Loss : 0.2909 | Acc : 0.8658
     Batch 050 | Loss : 0.3348 | Acc : 0.8523
     Batch 055 | Loss : 0.2710 | Acc : 0.8827
     Batch 060 | Loss : 0.3220 | Acc : 0.8521
     Batch 065 | Loss : 0.3327 | Acc : 0.8479
     Batch 070 | Loss : 0.2986 | Acc : 0.8633
     Batch 075 | Loss : 0.2791 | Acc : 0.8709
     Batch 080 | Loss : 0.3090 | Acc : 0.8537
     Batch 085 | Loss : 0.3090 | Acc : 0.8550
     Batch 090 | Loss : 0.3208 | Acc : 0.8503
     Batch 095 | Loss : 0.2689 | Acc : 0.8775
     Batch 100 | Loss : 0.3144 | Acc : 0.8549
     Batch 105 | Loss : 0.2549 | Acc : 0.8881
     Batch 110 | Loss : 0.3589 | Acc : 0.8326
     Batch 115 | Loss : 0.2896 | Acc : 0.8647
     Batch 120 | Loss : 0.2877 | Acc : 0.8655
     Batch 125 | Loss : 0.3320 | Acc : 0.8430
     Batch 130 | Loss : 0.2782 | Acc : 0.8714
     Batch 135 | Loss : 0.2929 | Acc : 0.8622
     Batch 140 | Loss : 0.2742 | Acc : 0.8763
     Batch 145 | Loss : 0.2455 | Acc : 0.8890
     Batch 150 | Loss : 0.2618 | Acc : 0.8840
Epoch 00107 | Train Loss : 0.2985 | Eval Loss : 0.3167 | Train acc : 0.8625 | Eval Acc : 0.8524 | Eval Log. Respected : 0.9355
     Batch 000 | Loss : 0.2867 | Acc : 0.8665
     Batch 005 | Loss : 0.2905 | Acc : 0.8658
     Batch 010 | Loss : 0.2558 | Acc : 0.8873
     Batch 015 | Loss : 0.2993 | Acc : 0.8623
     Batch 020 | Loss : 0.2627 | Acc : 0.8814
     Batch 025 | Loss : 0.3186 | Acc : 0.8514
     Batch 030 | Loss : 0.2647 | Acc : 0.8808
     Batch 035 | Loss : 0.3215 | Acc : 0.8535
     Batch 040 | Loss : 0.3073 | Acc : 0.8606
     Batch 045 | Loss : 0.3121 | Acc : 0.8546
     Batch 050 | Loss : 0.2642 | Acc : 0.8800
     Batch 055 | Loss : 0.2702 | Acc : 0.8760
     Batch 060 | Loss : 0.3305 | Acc : 0.8463
     Batch 065 | Loss : 0.3384 | Acc : 0.8415
     Batch 070 | Loss : 0.2927 | Acc : 0.8627
     Batch 075 | Loss : 0.2977 | Acc : 0.8653
     Batch 080 | Loss : 0.3712 | Acc : 0.8270
     Batch 085 | Loss : 0.2866 | Acc : 0.8710
     Batch 090 | Loss : 0.2729 | Acc : 0.8738
     Batch 095 | Loss : 0.2795 | Acc : 0.8723
     Batch 100 | Loss : 0.3116 | Acc : 0.8613
     Batch 105 | Loss : 0.2652 | Acc : 0.8821
     Batch 110 | Loss : 0.3294 | Acc : 0.8471
     Batch 115 | Loss : 0.2876 | Acc : 0.8704
     Batch 120 | Loss : 0.4118 | Acc : 0.8171
     Batch 125 | Loss : 0.3150 | Acc : 0.8541
     Batch 130 | Loss : 0.3178 | Acc : 0.8514
     Batch 135 | Loss : 0.4054 | Acc : 0.8215
     Batch 140 | Loss : 0.3142 | Acc : 0.8544
     Batch 145 | Loss : 0.3042 | Acc : 0.8592
     Batch 150 | Loss : 0.2781 | Acc : 0.8717
Epoch 00108 | Train Loss : 0.2993 | Eval Loss : 0.3251 | Train acc : 0.8624 | Eval Acc : 0.8518 | Eval Log. Respected : 0.9354
     Batch 000 | Loss : 0.2819 | Acc : 0.8723
     Batch 005 | Loss : 0.3253 | Acc : 0.8513
     Batch 010 | Loss : 0.2927 | Acc : 0.8660
     Batch 015 | Loss : 0.3222 | Acc : 0.8525
     Batch 020 | Loss : 0.2255 | Acc : 0.8991
     Batch 025 | Loss : 0.2599 | Acc : 0.8810
     Batch 030 | Loss : 0.2825 | Acc : 0.8673
     Batch 035 | Loss : 0.2865 | Acc : 0.8680
     Batch 040 | Loss : 0.3360 | Acc : 0.8471
     Batch 045 | Loss : 0.3398 | Acc : 0.8420
     Batch 050 | Loss : 0.3091 | Acc : 0.8569
     Batch 055 | Loss : 0.3104 | Acc : 0.8563
     Batch 060 | Loss : 0.2978 | Acc : 0.8657
     Batch 065 | Loss : 0.3339 | Acc : 0.8453
     Batch 070 | Loss : 0.3045 | Acc : 0.8607
     Batch 075 | Loss : 0.2925 | Acc : 0.8633
     Batch 080 | Loss : 0.3379 | Acc : 0.8444
     Batch 085 | Loss : 0.3143 | Acc : 0.8534
     Batch 090 | Loss : 0.2747 | Acc : 0.8764
     Batch 095 | Loss : 0.2539 | Acc : 0.8869
     Batch 100 | Loss : 0.3276 | Acc : 0.8445
     Batch 105 | Loss : 0.3318 | Acc : 0.8456
     Batch 110 | Loss : 0.2746 | Acc : 0.8760
     Batch 115 | Loss : 0.2643 | Acc : 0.8781
     Batch 120 | Loss : 0.2812 | Acc : 0.8704
     Batch 125 | Loss : 0.2753 | Acc : 0.8761
     Batch 130 | Loss : 0.3062 | Acc : 0.8602
     Batch 135 | Loss : 0.3438 | Acc : 0.8338
     Batch 140 | Loss : 0.3056 | Acc : 0.8572
     Batch 145 | Loss : 0.2617 | Acc : 0.8832
     Batch 150 | Loss : 0.3132 | Acc : 0.8566
Epoch 00109 | Train Loss : 0.2985 | Eval Loss : 0.3172 | Train acc : 0.8625 | Eval Acc : 0.8523 | Eval Log. Respected : 0.9398
     Batch 000 | Loss : 0.2687 | Acc : 0.8772
     Batch 005 | Loss : 0.3444 | Acc : 0.8398
     Batch 010 | Loss : 0.2616 | Acc : 0.8826
     Batch 015 | Loss : 0.3420 | Acc : 0.8353
     Batch 020 | Loss : 0.3049 | Acc : 0.8569
     Batch 025 | Loss : 0.2552 | Acc : 0.8863
     Batch 030 | Loss : 0.3353 | Acc : 0.8452
     Batch 035 | Loss : 0.3078 | Acc : 0.8588
     Batch 040 | Loss : 0.2933 | Acc : 0.8679
     Batch 045 | Loss : 0.2940 | Acc : 0.8618
     Batch 050 | Loss : 0.2979 | Acc : 0.8633
     Batch 055 | Loss : 0.2673 | Acc : 0.8764
     Batch 060 | Loss : 0.2526 | Acc : 0.8897
     Batch 065 | Loss : 0.2993 | Acc : 0.8597
     Batch 070 | Loss : 0.2832 | Acc : 0.8703
     Batch 075 | Loss : 0.2800 | Acc : 0.8708
     Batch 080 | Loss : 0.2978 | Acc : 0.8622
     Batch 085 | Loss : 0.3241 | Acc : 0.8482
     Batch 090 | Loss : 0.2582 | Acc : 0.8831
     Batch 095 | Loss : 0.3160 | Acc : 0.8498
     Batch 100 | Loss : 0.2991 | Acc : 0.8603
     Batch 105 | Loss : 0.3577 | Acc : 0.8353
     Batch 110 | Loss : 0.3129 | Acc : 0.8553
     Batch 115 | Loss : 0.2525 | Acc : 0.8844
     Batch 120 | Loss : 0.2699 | Acc : 0.8771
     Batch 125 | Loss : 0.2977 | Acc : 0.8619
     Batch 130 | Loss : 0.2901 | Acc : 0.8645
     Batch 135 | Loss : 0.2875 | Acc : 0.8683
     Batch 140 | Loss : 0.3913 | Acc : 0.8142
     Batch 145 | Loss : 0.2653 | Acc : 0.8813
     Batch 150 | Loss : 0.2649 | Acc : 0.8815
Epoch 00110 | Train Loss : 0.2973 | Eval Loss : 0.3156 | Train acc : 0.8630 | Eval Acc : 0.8532 | Eval Log. Respected : 0.9391
     Batch 000 | Loss : 0.3266 | Acc : 0.8473
     Batch 005 | Loss : 0.3211 | Acc : 0.8462
     Batch 010 | Loss : 0.2855 | Acc : 0.8687
     Batch 015 | Loss : 0.2708 | Acc : 0.8790
     Batch 020 | Loss : 0.3245 | Acc : 0.8480
     Batch 025 | Loss : 0.3497 | Acc : 0.8337
     Batch 030 | Loss : 0.3203 | Acc : 0.8508
     Batch 035 | Loss : 0.3068 | Acc : 0.8600
     Batch 040 | Loss : 0.2991 | Acc : 0.8602
     Batch 045 | Loss : 0.2825 | Acc : 0.8734
     Batch 050 | Loss : 0.2957 | Acc : 0.8668
     Batch 055 | Loss : 0.2886 | Acc : 0.8703
     Batch 060 | Loss : 0.3130 | Acc : 0.8551
     Batch 065 | Loss : 0.3361 | Acc : 0.8431
     Batch 070 | Loss : 0.3109 | Acc : 0.8486
     Batch 075 | Loss : 0.3237 | Acc : 0.8527
     Batch 080 | Loss : 0.3445 | Acc : 0.8366
     Batch 085 | Loss : 0.2716 | Acc : 0.8794
     Batch 090 | Loss : 0.2672 | Acc : 0.8771
     Batch 095 | Loss : 0.3061 | Acc : 0.8602
     Batch 100 | Loss : 0.2971 | Acc : 0.8589
     Batch 105 | Loss : 0.2552 | Acc : 0.8865
     Batch 110 | Loss : 0.2920 | Acc : 0.8659
     Batch 115 | Loss : 0.2481 | Acc : 0.8870
     Batch 120 | Loss : 0.2674 | Acc : 0.8766
     Batch 125 | Loss : 0.2774 | Acc : 0.8720
     Batch 130 | Loss : 0.2724 | Acc : 0.8752
     Batch 135 | Loss : 0.3330 | Acc : 0.8424
     Batch 140 | Loss : 0.3158 | Acc : 0.8522
     Batch 145 | Loss : 0.3189 | Acc : 0.8522
     Batch 150 | Loss : 0.3072 | Acc : 0.8538
Epoch 00111 | Train Loss : 0.2974 | Eval Loss : 0.3207 | Train acc : 0.8629 | Eval Acc : 0.8508 | Eval Log. Respected : 0.9432
     Batch 000 | Loss : 0.3181 | Acc : 0.8550
     Batch 005 | Loss : 0.2426 | Acc : 0.8904
     Batch 010 | Loss : 0.3018 | Acc : 0.8603
     Batch 015 | Loss : 0.3085 | Acc : 0.8569
     Batch 020 | Loss : 0.2939 | Acc : 0.8644
     Batch 025 | Loss : 0.2992 | Acc : 0.8674
     Batch 030 | Loss : 0.2713 | Acc : 0.8783
     Batch 035 | Loss : 0.2575 | Acc : 0.8841
     Batch 040 | Loss : 0.3645 | Acc : 0.8302
     Batch 045 | Loss : 0.2943 | Acc : 0.8658
     Batch 050 | Loss : 0.3105 | Acc : 0.8590
     Batch 055 | Loss : 0.2940 | Acc : 0.8630
     Batch 060 | Loss : 0.2682 | Acc : 0.8757
     Batch 065 | Loss : 0.2552 | Acc : 0.8825
     Batch 070 | Loss : 0.2996 | Acc : 0.8636
     Batch 075 | Loss : 0.2646 | Acc : 0.8758
     Batch 080 | Loss : 0.2428 | Acc : 0.8894
     Batch 085 | Loss : 0.3166 | Acc : 0.8510
     Batch 090 | Loss : 0.2976 | Acc : 0.8582
     Batch 095 | Loss : 0.2773 | Acc : 0.8739
     Batch 100 | Loss : 0.2357 | Acc : 0.8947
     Batch 105 | Loss : 0.3021 | Acc : 0.8600
     Batch 110 | Loss : 0.3366 | Acc : 0.8417
     Batch 115 | Loss : 0.2487 | Acc : 0.8884
     Batch 120 | Loss : 0.2821 | Acc : 0.8744
     Batch 125 | Loss : 0.2624 | Acc : 0.8809
     Batch 130 | Loss : 0.2609 | Acc : 0.8840
     Batch 135 | Loss : 0.2985 | Acc : 0.8613
     Batch 140 | Loss : 0.3098 | Acc : 0.8601
     Batch 145 | Loss : 0.3115 | Acc : 0.8561
     Batch 150 | Loss : 0.3192 | Acc : 0.8515
Epoch 00112 | Train Loss : 0.2980 | Eval Loss : 0.3194 | Train acc : 0.8628 | Eval Acc : 0.8536 | Eval Log. Respected : 0.9331
     Batch 000 | Loss : 0.3110 | Acc : 0.8587
     Batch 005 | Loss : 0.3269 | Acc : 0.8458
     Batch 010 | Loss : 0.3021 | Acc : 0.8609
     Batch 015 | Loss : 0.2773 | Acc : 0.8698
     Batch 020 | Loss : 0.2962 | Acc : 0.8630
     Batch 025 | Loss : 0.2796 | Acc : 0.8701
     Batch 030 | Loss : 0.3017 | Acc : 0.8620
     Batch 035 | Loss : 0.2974 | Acc : 0.8670
     Batch 040 | Loss : 0.2621 | Acc : 0.8844
     Batch 045 | Loss : 0.2625 | Acc : 0.8763
     Batch 050 | Loss : 0.2815 | Acc : 0.8706
     Batch 055 | Loss : 0.3140 | Acc : 0.8542
     Batch 060 | Loss : 0.2717 | Acc : 0.8724
     Batch 065 | Loss : 0.2794 | Acc : 0.8728
     Batch 070 | Loss : 0.2615 | Acc : 0.8798
     Batch 075 | Loss : 0.3862 | Acc : 0.8191
     Batch 080 | Loss : 0.3436 | Acc : 0.8447
     Batch 085 | Loss : 0.3256 | Acc : 0.8480
     Batch 090 | Loss : 0.3186 | Acc : 0.8496
     Batch 095 | Loss : 0.2949 | Acc : 0.8639
     Batch 100 | Loss : 0.2734 | Acc : 0.8787
     Batch 105 | Loss : 0.2868 | Acc : 0.8696
     Batch 110 | Loss : 0.2663 | Acc : 0.8803
     Batch 115 | Loss : 0.3100 | Acc : 0.8552
     Batch 120 | Loss : 0.2590 | Acc : 0.8790
     Batch 125 | Loss : 0.3197 | Acc : 0.8482
     Batch 130 | Loss : 0.3299 | Acc : 0.8451
     Batch 135 | Loss : 0.2530 | Acc : 0.8852
     Batch 140 | Loss : 0.3509 | Acc : 0.8376
     Batch 145 | Loss : 0.3011 | Acc : 0.8583
     Batch 150 | Loss : 0.2790 | Acc : 0.8686
Epoch 00113 | Train Loss : 0.2974 | Eval Loss : 0.3182 | Train acc : 0.8628 | Eval Acc : 0.8542 | Eval Log. Respected : 0.9360
     Batch 000 | Loss : 0.3317 | Acc : 0.8419
     Batch 005 | Loss : 0.3086 | Acc : 0.8601
     Batch 010 | Loss : 0.3219 | Acc : 0.8494
     Batch 015 | Loss : 0.2538 | Acc : 0.8855
     Batch 020 | Loss : 0.2800 | Acc : 0.8708
     Batch 025 | Loss : 0.3351 | Acc : 0.8444
     Batch 030 | Loss : 0.3147 | Acc : 0.8565
     Batch 035 | Loss : 0.2960 | Acc : 0.8666
     Batch 040 | Loss : 0.2898 | Acc : 0.8633
     Batch 045 | Loss : 0.2819 | Acc : 0.8708
     Batch 050 | Loss : 0.2810 | Acc : 0.8721
     Batch 055 | Loss : 0.3182 | Acc : 0.8485
     Batch 060 | Loss : 0.2944 | Acc : 0.8665
     Batch 065 | Loss : 0.2493 | Acc : 0.8894
     Batch 070 | Loss : 0.2995 | Acc : 0.8589
     Batch 075 | Loss : 0.2642 | Acc : 0.8782
     Batch 080 | Loss : 0.2782 | Acc : 0.8691
     Batch 085 | Loss : 0.2757 | Acc : 0.8746
     Batch 090 | Loss : 0.2639 | Acc : 0.8823
     Batch 095 | Loss : 0.2698 | Acc : 0.8767
     Batch 100 | Loss : 0.2904 | Acc : 0.8638
     Batch 105 | Loss : 0.3001 | Acc : 0.8619
     Batch 110 | Loss : 0.2938 | Acc : 0.8629
     Batch 115 | Loss : 0.2664 | Acc : 0.8786
     Batch 120 | Loss : 0.2831 | Acc : 0.8665
     Batch 125 | Loss : 0.2458 | Acc : 0.8892
     Batch 130 | Loss : 0.2802 | Acc : 0.8727
     Batch 135 | Loss : 0.3047 | Acc : 0.8592
     Batch 140 | Loss : 0.3243 | Acc : 0.8485
     Batch 145 | Loss : 0.2752 | Acc : 0.8783
     Batch 150 | Loss : 0.2724 | Acc : 0.8796
Epoch 00114 | Train Loss : 0.2969 | Eval Loss : 0.3180 | Train acc : 0.8631 | Eval Acc : 0.8528 | Eval Log. Respected : 0.9233
     Batch 000 | Loss : 0.3047 | Acc : 0.8585
     Batch 005 | Loss : 0.2376 | Acc : 0.8945
     Batch 010 | Loss : 0.2773 | Acc : 0.8779
     Batch 015 | Loss : 0.2941 | Acc : 0.8630
     Batch 020 | Loss : 0.2593 | Acc : 0.8882
     Batch 025 | Loss : 0.2889 | Acc : 0.8717
     Batch 030 | Loss : 0.3012 | Acc : 0.8623
     Batch 035 | Loss : 0.2597 | Acc : 0.8811
     Batch 040 | Loss : 0.3211 | Acc : 0.8515
     Batch 045 | Loss : 0.3693 | Acc : 0.8250
     Batch 050 | Loss : 0.3124 | Acc : 0.8531
     Batch 055 | Loss : 0.2869 | Acc : 0.8703
     Batch 060 | Loss : 0.3115 | Acc : 0.8554
     Batch 065 | Loss : 0.2946 | Acc : 0.8652
     Batch 070 | Loss : 0.3067 | Acc : 0.8572
     Batch 075 | Loss : 0.2801 | Acc : 0.8732
     Batch 080 | Loss : 0.2909 | Acc : 0.8694
     Batch 085 | Loss : 0.3232 | Acc : 0.8509
     Batch 090 | Loss : 0.2712 | Acc : 0.8753
     Batch 095 | Loss : 0.2741 | Acc : 0.8731
     Batch 100 | Loss : 0.3071 | Acc : 0.8619
     Batch 105 | Loss : 0.2861 | Acc : 0.8720
     Batch 110 | Loss : 0.3293 | Acc : 0.8437
     Batch 115 | Loss : 0.2967 | Acc : 0.8626
     Batch 120 | Loss : 0.3126 | Acc : 0.8568
     Batch 125 | Loss : 0.3476 | Acc : 0.8337
     Batch 130 | Loss : 0.3174 | Acc : 0.8519
     Batch 135 | Loss : 0.3140 | Acc : 0.8527
     Batch 140 | Loss : 0.3090 | Acc : 0.8586
     Batch 145 | Loss : 0.2595 | Acc : 0.8837
     Batch 150 | Loss : 0.3220 | Acc : 0.8493
Epoch 00115 | Train Loss : 0.2974 | Eval Loss : 0.3141 | Train acc : 0.8632 | Eval Acc : 0.8536 | Eval Log. Respected : 0.9409
     Batch 000 | Loss : 0.3253 | Acc : 0.8494
     Batch 005 | Loss : 0.2646 | Acc : 0.8817
     Batch 010 | Loss : 0.2961 | Acc : 0.8659
     Batch 015 | Loss : 0.2631 | Acc : 0.8802
     Batch 020 | Loss : 0.3780 | Acc : 0.8249
     Batch 025 | Loss : 0.2756 | Acc : 0.8764
     Batch 030 | Loss : 0.3070 | Acc : 0.8590
     Batch 035 | Loss : 0.3709 | Acc : 0.8239
     Batch 040 | Loss : 0.3246 | Acc : 0.8496
     Batch 045 | Loss : 0.2957 | Acc : 0.8615
     Batch 050 | Loss : 0.2699 | Acc : 0.8773
     Batch 055 | Loss : 0.2733 | Acc : 0.8718
     Batch 060 | Loss : 0.3292 | Acc : 0.8442
     Batch 065 | Loss : 0.2939 | Acc : 0.8702
     Batch 070 | Loss : 0.2675 | Acc : 0.8764
     Batch 075 | Loss : 0.2784 | Acc : 0.8716
     Batch 080 | Loss : 0.3092 | Acc : 0.8590
     Batch 085 | Loss : 0.3562 | Acc : 0.8378
     Batch 090 | Loss : 0.2984 | Acc : 0.8675
     Batch 095 | Loss : 0.3069 | Acc : 0.8555
     Batch 100 | Loss : 0.2969 | Acc : 0.8657
     Batch 105 | Loss : 0.2662 | Acc : 0.8786
     Batch 110 | Loss : 0.2836 | Acc : 0.8689
     Batch 115 | Loss : 0.2830 | Acc : 0.8708
     Batch 120 | Loss : 0.2540 | Acc : 0.8863
     Batch 125 | Loss : 0.2870 | Acc : 0.8672
     Batch 130 | Loss : 0.2665 | Acc : 0.8776
     Batch 135 | Loss : 0.3221 | Acc : 0.8487
     Batch 140 | Loss : 0.3745 | Acc : 0.8295
     Batch 145 | Loss : 0.3036 | Acc : 0.8610
     Batch 150 | Loss : 0.2890 | Acc : 0.8641
Epoch 00116 | Train Loss : 0.2962 | Eval Loss : 0.3238 | Train acc : 0.8635 | Eval Acc : 0.8543 | Eval Log. Respected : 0.9296
     Batch 000 | Loss : 0.3550 | Acc : 0.8352
     Batch 005 | Loss : 0.2917 | Acc : 0.8674
     Batch 010 | Loss : 0.2691 | Acc : 0.8774
     Batch 015 | Loss : 0.2779 | Acc : 0.8718
     Batch 020 | Loss : 0.3523 | Acc : 0.8348
     Batch 025 | Loss : 0.3152 | Acc : 0.8539
     Batch 030 | Loss : 0.3076 | Acc : 0.8586
     Batch 035 | Loss : 0.3154 | Acc : 0.8520
     Batch 040 | Loss : 0.2840 | Acc : 0.8708
     Batch 045 | Loss : 0.2779 | Acc : 0.8714
     Batch 050 | Loss : 0.3215 | Acc : 0.8505
     Batch 055 | Loss : 0.2712 | Acc : 0.8842
     Batch 060 | Loss : 0.2884 | Acc : 0.8683
     Batch 065 | Loss : 0.3015 | Acc : 0.8611
     Batch 070 | Loss : 0.2947 | Acc : 0.8638
     Batch 075 | Loss : 0.3016 | Acc : 0.8558
     Batch 080 | Loss : 0.3191 | Acc : 0.8543
     Batch 085 | Loss : 0.2554 | Acc : 0.8837
     Batch 090 | Loss : 0.2623 | Acc : 0.8806
     Batch 095 | Loss : 0.2715 | Acc : 0.8743
     Batch 100 | Loss : 0.3173 | Acc : 0.8491
     Batch 105 | Loss : 0.2656 | Acc : 0.8840
     Batch 110 | Loss : 0.3117 | Acc : 0.8573
     Batch 115 | Loss : 0.3269 | Acc : 0.8507
     Batch 120 | Loss : 0.2928 | Acc : 0.8648
     Batch 125 | Loss : 0.2626 | Acc : 0.8827
     Batch 130 | Loss : 0.2932 | Acc : 0.8637
     Batch 135 | Loss : 0.3243 | Acc : 0.8514
     Batch 140 | Loss : 0.3304 | Acc : 0.8459
     Batch 145 | Loss : 0.2663 | Acc : 0.8849
     Batch 150 | Loss : 0.2972 | Acc : 0.8632
Epoch 00117 | Train Loss : 0.2982 | Eval Loss : 0.3203 | Train acc : 0.8630 | Eval Acc : 0.8535 | Eval Log. Respected : 0.9402
     Batch 000 | Loss : 0.3130 | Acc : 0.8582
     Batch 005 | Loss : 0.3298 | Acc : 0.8456
     Batch 010 | Loss : 0.2995 | Acc : 0.8635
     Batch 015 | Loss : 0.2784 | Acc : 0.8709
     Batch 020 | Loss : 0.2576 | Acc : 0.8862
     Batch 025 | Loss : 0.2913 | Acc : 0.8665
     Batch 030 | Loss : 0.2851 | Acc : 0.8679
     Batch 035 | Loss : 0.3309 | Acc : 0.8444
     Batch 040 | Loss : 0.3316 | Acc : 0.8462
     Batch 045 | Loss : 0.2771 | Acc : 0.8728
     Batch 050 | Loss : 0.3039 | Acc : 0.8578
     Batch 055 | Loss : 0.2978 | Acc : 0.8611
     Batch 060 | Loss : 0.2540 | Acc : 0.8887
     Batch 065 | Loss : 0.2969 | Acc : 0.8657
     Batch 070 | Loss : 0.2611 | Acc : 0.8792
     Batch 075 | Loss : 0.3189 | Acc : 0.8506
     Batch 080 | Loss : 0.2629 | Acc : 0.8829
     Batch 085 | Loss : 0.2543 | Acc : 0.8886
     Batch 090 | Loss : 0.2736 | Acc : 0.8770
     Batch 095 | Loss : 0.3260 | Acc : 0.8515
     Batch 100 | Loss : 0.2790 | Acc : 0.8801
     Batch 105 | Loss : 0.3311 | Acc : 0.8466
     Batch 110 | Loss : 0.2656 | Acc : 0.8779
     Batch 115 | Loss : 0.2592 | Acc : 0.8814
     Batch 120 | Loss : 0.2523 | Acc : 0.8875
     Batch 125 | Loss : 0.3656 | Acc : 0.8257
     Batch 130 | Loss : 0.2737 | Acc : 0.8735
     Batch 135 | Loss : 0.3156 | Acc : 0.8574
     Batch 140 | Loss : 0.3130 | Acc : 0.8538
     Batch 145 | Loss : 0.3178 | Acc : 0.8572
     Batch 150 | Loss : 0.3134 | Acc : 0.8497
Epoch 00118 | Train Loss : 0.2967 | Eval Loss : 0.3139 | Train acc : 0.8636 | Eval Acc : 0.8543 | Eval Log. Respected : 0.9278
     Batch 000 | Loss : 0.2803 | Acc : 0.8715
     Batch 005 | Loss : 0.2940 | Acc : 0.8637
     Batch 010 | Loss : 0.2898 | Acc : 0.8660
     Batch 015 | Loss : 0.2617 | Acc : 0.8796
     Batch 020 | Loss : 0.3199 | Acc : 0.8449
     Batch 025 | Loss : 0.3799 | Acc : 0.8189
     Batch 030 | Loss : 0.3187 | Acc : 0.8536
     Batch 035 | Loss : 0.3052 | Acc : 0.8563
     Batch 040 | Loss : 0.3244 | Acc : 0.8455
     Batch 045 | Loss : 0.2536 | Acc : 0.8848
     Batch 050 | Loss : 0.2981 | Acc : 0.8663
     Batch 055 | Loss : 0.2729 | Acc : 0.8755
     Batch 060 | Loss : 0.2634 | Acc : 0.8812
     Batch 065 | Loss : 0.2945 | Acc : 0.8630
     Batch 070 | Loss : 0.2600 | Acc : 0.8825
     Batch 075 | Loss : 0.3006 | Acc : 0.8583
     Batch 080 | Loss : 0.3028 | Acc : 0.8600
     Batch 085 | Loss : 0.2475 | Acc : 0.8863
     Batch 090 | Loss : 0.3207 | Acc : 0.8529
     Batch 095 | Loss : 0.2974 | Acc : 0.8636
     Batch 100 | Loss : 0.2666 | Acc : 0.8776
     Batch 105 | Loss : 0.3365 | Acc : 0.8432
     Batch 110 | Loss : 0.3533 | Acc : 0.8358
     Batch 115 | Loss : 0.3622 | Acc : 0.8276
     Batch 120 | Loss : 0.2896 | Acc : 0.8685
     Batch 125 | Loss : 0.3214 | Acc : 0.8528
     Batch 130 | Loss : 0.3255 | Acc : 0.8498
     Batch 135 | Loss : 0.2852 | Acc : 0.8682
     Batch 140 | Loss : 0.2702 | Acc : 0.8789
     Batch 145 | Loss : 0.2863 | Acc : 0.8690
     Batch 150 | Loss : 0.3141 | Acc : 0.8527
Epoch 00119 | Train Loss : 0.2944 | Eval Loss : 0.3179 | Train acc : 0.8643 | Eval Acc : 0.8528 | Eval Log. Respected : 0.9249
     Batch 000 | Loss : 0.3163 | Acc : 0.8573
     Batch 005 | Loss : 0.3528 | Acc : 0.8308
     Batch 010 | Loss : 0.3148 | Acc : 0.8524
     Batch 015 | Loss : 0.2541 | Acc : 0.8838
     Batch 020 | Loss : 0.2819 | Acc : 0.8691
     Batch 025 | Loss : 0.2659 | Acc : 0.8802
     Batch 030 | Loss : 0.2797 | Acc : 0.8714
     Batch 035 | Loss : 0.2663 | Acc : 0.8798
     Batch 040 | Loss : 0.3440 | Acc : 0.8393
     Batch 045 | Loss : 0.2921 | Acc : 0.8674
     Batch 050 | Loss : 0.2919 | Acc : 0.8643
     Batch 055 | Loss : 0.3149 | Acc : 0.8559
     Batch 060 | Loss : 0.2791 | Acc : 0.8733
     Batch 065 | Loss : 0.2805 | Acc : 0.8721
     Batch 070 | Loss : 0.2842 | Acc : 0.8699
     Batch 075 | Loss : 0.2718 | Acc : 0.8768
     Batch 080 | Loss : 0.3030 | Acc : 0.8610
     Batch 085 | Loss : 0.2510 | Acc : 0.8877
     Batch 090 | Loss : 0.2995 | Acc : 0.8616
     Batch 095 | Loss : 0.2972 | Acc : 0.8636
     Batch 100 | Loss : 0.2935 | Acc : 0.8662
     Batch 105 | Loss : 0.3144 | Acc : 0.8578
     Batch 110 | Loss : 0.3468 | Acc : 0.8393
     Batch 115 | Loss : 0.3530 | Acc : 0.8364
     Batch 120 | Loss : 0.3002 | Acc : 0.8592
     Batch 125 | Loss : 0.2990 | Acc : 0.8588
     Batch 130 | Loss : 0.3016 | Acc : 0.8633
     Batch 135 | Loss : 0.2902 | Acc : 0.8672
     Batch 140 | Loss : 0.3095 | Acc : 0.8534
     Batch 145 | Loss : 0.3090 | Acc : 0.8581
     Batch 150 | Loss : 0.2923 | Acc : 0.8643
Epoch 00120 | Train Loss : 0.2978 | Eval Loss : 0.3185 | Train acc : 0.8630 | Eval Acc : 0.8527 | Eval Log. Respected : 0.9268
     Batch 000 | Loss : 0.3225 | Acc : 0.8477
     Batch 005 | Loss : 0.2773 | Acc : 0.8707
     Batch 010 | Loss : 0.2845 | Acc : 0.8714
     Batch 015 | Loss : 0.3274 | Acc : 0.8451
     Batch 020 | Loss : 0.3004 | Acc : 0.8614
     Batch 025 | Loss : 0.2830 | Acc : 0.8711
     Batch 030 | Loss : 0.3316 | Acc : 0.8470
     Batch 035 | Loss : 0.2645 | Acc : 0.8811
     Batch 040 | Loss : 0.3097 | Acc : 0.8568
     Batch 045 | Loss : 0.2852 | Acc : 0.8735
     Batch 050 | Loss : 0.2772 | Acc : 0.8739
     Batch 055 | Loss : 0.2514 | Acc : 0.8858
     Batch 060 | Loss : 0.3586 | Acc : 0.8352
     Batch 065 | Loss : 0.2972 | Acc : 0.8634
     Batch 070 | Loss : 0.2784 | Acc : 0.8710
     Batch 075 | Loss : 0.2775 | Acc : 0.8729
     Batch 080 | Loss : 0.3008 | Acc : 0.8624
     Batch 085 | Loss : 0.2907 | Acc : 0.8696
     Batch 090 | Loss : 0.2425 | Acc : 0.8920
     Batch 095 | Loss : 0.2481 | Acc : 0.8897
     Batch 100 | Loss : 0.3881 | Acc : 0.8315
     Batch 105 | Loss : 0.2808 | Acc : 0.8744
     Batch 110 | Loss : 0.2804 | Acc : 0.8725
     Batch 115 | Loss : 0.2529 | Acc : 0.8883
     Batch 120 | Loss : 0.2708 | Acc : 0.8783
     Batch 125 | Loss : 0.3072 | Acc : 0.8566
     Batch 130 | Loss : 0.3272 | Acc : 0.8448
     Batch 135 | Loss : 0.2895 | Acc : 0.8673
     Batch 140 | Loss : 0.3256 | Acc : 0.8469
     Batch 145 | Loss : 0.3570 | Acc : 0.8355
     Batch 150 | Loss : 0.3169 | Acc : 0.8553
Epoch 00121 | Train Loss : 0.2966 | Eval Loss : 0.3189 | Train acc : 0.8635 | Eval Acc : 0.8522 | Eval Log. Respected : 0.9310
     Batch 000 | Loss : 0.3014 | Acc : 0.8595
     Batch 005 | Loss : 0.2823 | Acc : 0.8670
     Batch 010 | Loss : 0.3289 | Acc : 0.8479
     Batch 015 | Loss : 0.3196 | Acc : 0.8535
     Batch 020 | Loss : 0.2764 | Acc : 0.8734
     Batch 025 | Loss : 0.2763 | Acc : 0.8731
     Batch 030 | Loss : 0.3160 | Acc : 0.8516
     Batch 035 | Loss : 0.3091 | Acc : 0.8569
     Batch 040 | Loss : 0.3238 | Acc : 0.8463
     Batch 045 | Loss : 0.2858 | Acc : 0.8723
     Batch 050 | Loss : 0.2919 | Acc : 0.8642
     Batch 055 | Loss : 0.2931 | Acc : 0.8651
     Batch 060 | Loss : 0.2665 | Acc : 0.8782
     Batch 065 | Loss : 0.2927 | Acc : 0.8666
     Batch 070 | Loss : 0.2954 | Acc : 0.8647
     Batch 075 | Loss : 0.3607 | Acc : 0.8321
     Batch 080 | Loss : 0.2784 | Acc : 0.8730
     Batch 085 | Loss : 0.2838 | Acc : 0.8668
     Batch 090 | Loss : 0.2847 | Acc : 0.8683
     Batch 095 | Loss : 0.2819 | Acc : 0.8720
     Batch 100 | Loss : 0.3156 | Acc : 0.8525
     Batch 105 | Loss : 0.2606 | Acc : 0.8799
     Batch 110 | Loss : 0.3368 | Acc : 0.8472
     Batch 115 | Loss : 0.3033 | Acc : 0.8585
     Batch 120 | Loss : 0.3256 | Acc : 0.8415
     Batch 125 | Loss : 0.2923 | Acc : 0.8664
     Batch 130 | Loss : 0.3421 | Acc : 0.8379
     Batch 135 | Loss : 0.3215 | Acc : 0.8474
     Batch 140 | Loss : 0.3198 | Acc : 0.8460
     Batch 145 | Loss : 0.3220 | Acc : 0.8483
     Batch 150 | Loss : 0.3447 | Acc : 0.8402
Epoch 00122 | Train Loss : 0.2956 | Eval Loss : 0.3168 | Train acc : 0.8638 | Eval Acc : 0.8531 | Eval Log. Respected : 0.9423
     Batch 000 | Loss : 0.2765 | Acc : 0.8789
     Batch 005 | Loss : 0.2980 | Acc : 0.8629
     Batch 010 | Loss : 0.2727 | Acc : 0.8722
     Batch 015 | Loss : 0.3381 | Acc : 0.8387
     Batch 020 | Loss : 0.2970 | Acc : 0.8604
     Batch 025 | Loss : 0.2641 | Acc : 0.8814
     Batch 030 | Loss : 0.2922 | Acc : 0.8660
     Batch 035 | Loss : 0.2463 | Acc : 0.8869
     Batch 040 | Loss : 0.2705 | Acc : 0.8756
     Batch 045 | Loss : 0.2764 | Acc : 0.8729
     Batch 050 | Loss : 0.2837 | Acc : 0.8721
     Batch 055 | Loss : 0.2662 | Acc : 0.8814
     Batch 060 | Loss : 0.2858 | Acc : 0.8655
     Batch 065 | Loss : 0.2935 | Acc : 0.8624
     Batch 070 | Loss : 0.2902 | Acc : 0.8680
     Batch 075 | Loss : 0.2985 | Acc : 0.8641
     Batch 080 | Loss : 0.2894 | Acc : 0.8676
     Batch 085 | Loss : 0.2795 | Acc : 0.8683
     Batch 090 | Loss : 0.3190 | Acc : 0.8522
     Batch 095 | Loss : 0.2806 | Acc : 0.8731
     Batch 100 | Loss : 0.3201 | Acc : 0.8496
     Batch 105 | Loss : 0.2971 | Acc : 0.8637
     Batch 110 | Loss : 0.3401 | Acc : 0.8391
     Batch 115 | Loss : 0.2707 | Acc : 0.8788
     Batch 120 | Loss : 0.2878 | Acc : 0.8674
     Batch 125 | Loss : 0.2478 | Acc : 0.8867
     Batch 130 | Loss : 0.2917 | Acc : 0.8659
     Batch 135 | Loss : 0.2915 | Acc : 0.8696
     Batch 140 | Loss : 0.2965 | Acc : 0.8613
     Batch 145 | Loss : 0.2841 | Acc : 0.8708
     Batch 150 | Loss : 0.2966 | Acc : 0.8630
Epoch 00123 | Train Loss : 0.2949 | Eval Loss : 0.3153 | Train acc : 0.8642 | Eval Acc : 0.8544 | Eval Log. Respected : 0.9370
     Batch 000 | Loss : 0.2595 | Acc : 0.8825
     Batch 005 | Loss : 0.2910 | Acc : 0.8679
     Batch 010 | Loss : 0.3110 | Acc : 0.8515
     Batch 015 | Loss : 0.2639 | Acc : 0.8832
     Batch 020 | Loss : 0.3466 | Acc : 0.8403
     Batch 025 | Loss : 0.2789 | Acc : 0.8693
     Batch 030 | Loss : 0.3027 | Acc : 0.8607
     Batch 035 | Loss : 0.2857 | Acc : 0.8691
     Batch 040 | Loss : 0.2340 | Acc : 0.8930
     Batch 045 | Loss : 0.3514 | Acc : 0.8356
     Batch 050 | Loss : 0.2723 | Acc : 0.8757
     Batch 055 | Loss : 0.2651 | Acc : 0.8842
     Batch 060 | Loss : 0.3038 | Acc : 0.8601
     Batch 065 | Loss : 0.3140 | Acc : 0.8558
     Batch 070 | Loss : 0.2565 | Acc : 0.8841
     Batch 075 | Loss : 0.3358 | Acc : 0.8395
     Batch 080 | Loss : 0.3487 | Acc : 0.8377
     Batch 085 | Loss : 0.2733 | Acc : 0.8770
     Batch 090 | Loss : 0.3120 | Acc : 0.8542
     Batch 095 | Loss : 0.3018 | Acc : 0.8597
     Batch 100 | Loss : 0.2796 | Acc : 0.8747
     Batch 105 | Loss : 0.3119 | Acc : 0.8532
     Batch 110 | Loss : 0.3232 | Acc : 0.8499
     Batch 115 | Loss : 0.2851 | Acc : 0.8704
     Batch 120 | Loss : 0.2730 | Acc : 0.8751
     Batch 125 | Loss : 0.3260 | Acc : 0.8544
     Batch 130 | Loss : 0.3452 | Acc : 0.8307
     Batch 135 | Loss : 0.2813 | Acc : 0.8690
     Batch 140 | Loss : 0.3412 | Acc : 0.8402
     Batch 145 | Loss : 0.3153 | Acc : 0.8522
     Batch 150 | Loss : 0.2832 | Acc : 0.8686
Epoch 00124 | Train Loss : 0.2970 | Eval Loss : 0.3259 | Train acc : 0.8634 | Eval Acc : 0.8511 | Eval Log. Respected : 0.9405
     Batch 000 | Loss : 0.2907 | Acc : 0.8676
     Batch 005 | Loss : 0.2918 | Acc : 0.8682
     Batch 010 | Loss : 0.2519 | Acc : 0.8870
     Batch 015 | Loss : 0.3417 | Acc : 0.8411
     Batch 020 | Loss : 0.3483 | Acc : 0.8407
     Batch 025 | Loss : 0.2706 | Acc : 0.8809
     Batch 030 | Loss : 0.2816 | Acc : 0.8695
     Batch 035 | Loss : 0.3598 | Acc : 0.8327
     Batch 040 | Loss : 0.3189 | Acc : 0.8496
     Batch 045 | Loss : 0.3193 | Acc : 0.8516
     Batch 050 | Loss : 0.3035 | Acc : 0.8606
     Batch 055 | Loss : 0.2576 | Acc : 0.8822
     Batch 060 | Loss : 0.4063 | Acc : 0.8192
     Batch 065 | Loss : 0.2924 | Acc : 0.8645
     Batch 070 | Loss : 0.3070 | Acc : 0.8565
     Batch 075 | Loss : 0.2953 | Acc : 0.8650
     Batch 080 | Loss : 0.2980 | Acc : 0.8604
     Batch 085 | Loss : 0.2898 | Acc : 0.8685
     Batch 090 | Loss : 0.2699 | Acc : 0.8767
     Batch 095 | Loss : 0.2416 | Acc : 0.8919
     Batch 100 | Loss : 0.3873 | Acc : 0.8219
     Batch 105 | Loss : 0.2946 | Acc : 0.8618
     Batch 110 | Loss : 0.2784 | Acc : 0.8752
     Batch 115 | Loss : 0.2808 | Acc : 0.8732
     Batch 120 | Loss : 0.3377 | Acc : 0.8373
     Batch 125 | Loss : 0.2768 | Acc : 0.8719
     Batch 130 | Loss : 0.3197 | Acc : 0.8539
     Batch 135 | Loss : 0.2776 | Acc : 0.8718
     Batch 140 | Loss : 0.3039 | Acc : 0.8565
     Batch 145 | Loss : 0.2736 | Acc : 0.8807
     Batch 150 | Loss : 0.2454 | Acc : 0.8880
Epoch 00125 | Train Loss : 0.2945 | Eval Loss : 0.3184 | Train acc : 0.8643 | Eval Acc : 0.8533 | Eval Log. Respected : 0.9420
     Batch 000 | Loss : 0.2565 | Acc : 0.8820
     Batch 005 | Loss : 0.2898 | Acc : 0.8668
     Batch 010 | Loss : 0.3611 | Acc : 0.8305
     Batch 015 | Loss : 0.2550 | Acc : 0.8868
     Batch 020 | Loss : 0.3321 | Acc : 0.8403
     Batch 025 | Loss : 0.3012 | Acc : 0.8588
     Batch 030 | Loss : 0.3495 | Acc : 0.8356
     Batch 035 | Loss : 0.2824 | Acc : 0.8717
     Batch 040 | Loss : 0.2816 | Acc : 0.8681
     Batch 045 | Loss : 0.2650 | Acc : 0.8820
     Batch 050 | Loss : 0.2825 | Acc : 0.8712
     Batch 055 | Loss : 0.2974 | Acc : 0.8582
     Batch 060 | Loss : 0.3317 | Acc : 0.8452
     Batch 065 | Loss : 0.2846 | Acc : 0.8715
     Batch 070 | Loss : 0.2756 | Acc : 0.8736
     Batch 075 | Loss : 0.2661 | Acc : 0.8771
     Batch 080 | Loss : 0.4401 | Acc : 0.7894
     Batch 085 | Loss : 0.2601 | Acc : 0.8911
     Batch 090 | Loss : 0.3470 | Acc : 0.8402
     Batch 095 | Loss : 0.2677 | Acc : 0.8794
     Batch 100 | Loss : 0.3742 | Acc : 0.8194
     Batch 105 | Loss : 0.3321 | Acc : 0.8440
     Batch 110 | Loss : 0.3248 | Acc : 0.8488
     Batch 115 | Loss : 0.2717 | Acc : 0.8740
     Batch 120 | Loss : 0.2856 | Acc : 0.8696
     Batch 125 | Loss : 0.3067 | Acc : 0.8544
     Batch 130 | Loss : 0.3791 | Acc : 0.8251
     Batch 135 | Loss : 0.2656 | Acc : 0.8820
     Batch 140 | Loss : 0.2694 | Acc : 0.8790
     Batch 145 | Loss : 0.3253 | Acc : 0.8505
     Batch 150 | Loss : 0.2565 | Acc : 0.8843
Epoch 00126 | Train Loss : 0.2944 | Eval Loss : 0.3188 | Train acc : 0.8646 | Eval Acc : 0.8517 | Eval Log. Respected : 0.9513
     Batch 000 | Loss : 0.2835 | Acc : 0.8683
     Batch 005 | Loss : 0.3502 | Acc : 0.8338
     Batch 010 | Loss : 0.3144 | Acc : 0.8538
     Batch 015 | Loss : 0.2872 | Acc : 0.8677
     Batch 020 | Loss : 0.2455 | Acc : 0.8876
     Batch 025 | Loss : 0.2274 | Acc : 0.9007
     Batch 030 | Loss : 0.3191 | Acc : 0.8512
     Batch 035 | Loss : 0.2805 | Acc : 0.8710
     Batch 040 | Loss : 0.3313 | Acc : 0.8408
     Batch 045 | Loss : 0.2913 | Acc : 0.8698
     Batch 050 | Loss : 0.2705 | Acc : 0.8790
     Batch 055 | Loss : 0.2792 | Acc : 0.8727
     Batch 060 | Loss : 0.2893 | Acc : 0.8658
     Batch 065 | Loss : 0.2754 | Acc : 0.8743
     Batch 070 | Loss : 0.2772 | Acc : 0.8681
     Batch 075 | Loss : 0.2820 | Acc : 0.8776
     Batch 080 | Loss : 0.3010 | Acc : 0.8635
     Batch 085 | Loss : 0.2709 | Acc : 0.8761
     Batch 090 | Loss : 0.2819 | Acc : 0.8703
     Batch 095 | Loss : 0.2559 | Acc : 0.8827
     Batch 100 | Loss : 0.2833 | Acc : 0.8661
     Batch 105 | Loss : 0.3012 | Acc : 0.8632
     Batch 110 | Loss : 0.3026 | Acc : 0.8584
     Batch 115 | Loss : 0.2971 | Acc : 0.8611
     Batch 120 | Loss : 0.3175 | Acc : 0.8503
     Batch 125 | Loss : 0.3335 | Acc : 0.8411
     Batch 130 | Loss : 0.2931 | Acc : 0.8646
     Batch 135 | Loss : 0.3142 | Acc : 0.8524
     Batch 140 | Loss : 0.2962 | Acc : 0.8636
     Batch 145 | Loss : 0.2823 | Acc : 0.8684
     Batch 150 | Loss : 0.3198 | Acc : 0.8533
Epoch 00127 | Train Loss : 0.2947 | Eval Loss : 0.3213 | Train acc : 0.8642 | Eval Acc : 0.8504 | Eval Log. Respected : 0.9357
     Batch 000 | Loss : 0.2669 | Acc : 0.8774
     Batch 005 | Loss : 0.3445 | Acc : 0.8361
     Batch 010 | Loss : 0.3154 | Acc : 0.8520
     Batch 015 | Loss : 0.2728 | Acc : 0.8756
     Batch 020 | Loss : 0.3191 | Acc : 0.8510
     Batch 025 | Loss : 0.2946 | Acc : 0.8654
     Batch 030 | Loss : 0.3246 | Acc : 0.8500
     Batch 035 | Loss : 0.2462 | Acc : 0.8863
     Batch 040 | Loss : 0.3057 | Acc : 0.8605
     Batch 045 | Loss : 0.2842 | Acc : 0.8700
     Batch 050 | Loss : 0.3115 | Acc : 0.8597
     Batch 055 | Loss : 0.2643 | Acc : 0.8796
     Batch 060 | Loss : 0.2674 | Acc : 0.8754
     Batch 065 | Loss : 0.3245 | Acc : 0.8495
     Batch 070 | Loss : 0.2957 | Acc : 0.8647
     Batch 075 | Loss : 0.2419 | Acc : 0.8912
     Batch 080 | Loss : 0.2989 | Acc : 0.8637
     Batch 085 | Loss : 0.2963 | Acc : 0.8598
     Batch 090 | Loss : 0.3799 | Acc : 0.8249
     Batch 095 | Loss : 0.2834 | Acc : 0.8653
     Batch 100 | Loss : 0.2707 | Acc : 0.8778
     Batch 105 | Loss : 0.3309 | Acc : 0.8413
     Batch 110 | Loss : 0.2595 | Acc : 0.8804
     Batch 115 | Loss : 0.2518 | Acc : 0.8864
     Batch 120 | Loss : 0.3190 | Acc : 0.8524
     Batch 125 | Loss : 0.2718 | Acc : 0.8718
     Batch 130 | Loss : 0.3293 | Acc : 0.8462
     Batch 135 | Loss : 0.3359 | Acc : 0.8392
     Batch 140 | Loss : 0.3172 | Acc : 0.8537
     Batch 145 | Loss : 0.3293 | Acc : 0.8443
     Batch 150 | Loss : 0.3037 | Acc : 0.8570
Epoch 00128 | Train Loss : 0.2951 | Eval Loss : 0.3129 | Train acc : 0.8642 | Eval Acc : 0.8550 | Eval Log. Respected : 0.9335
     Batch 000 | Loss : 0.3003 | Acc : 0.8614
     Batch 005 | Loss : 0.2980 | Acc : 0.8616
     Batch 010 | Loss : 0.3183 | Acc : 0.8542
     Batch 015 | Loss : 0.2645 | Acc : 0.8799
     Batch 020 | Loss : 0.3071 | Acc : 0.8619
     Batch 025 | Loss : 0.3088 | Acc : 0.8532
     Batch 030 | Loss : 0.3375 | Acc : 0.8427
     Batch 035 | Loss : 0.2811 | Acc : 0.8727
     Batch 040 | Loss : 0.3661 | Acc : 0.8295
     Batch 045 | Loss : 0.3341 | Acc : 0.8433
     Batch 050 | Loss : 0.2804 | Acc : 0.8712
     Batch 055 | Loss : 0.2677 | Acc : 0.8777
     Batch 060 | Loss : 0.2945 | Acc : 0.8625
     Batch 065 | Loss : 0.2914 | Acc : 0.8671
     Batch 070 | Loss : 0.2959 | Acc : 0.8611
     Batch 075 | Loss : 0.3251 | Acc : 0.8513
     Batch 080 | Loss : 0.2614 | Acc : 0.8760
     Batch 085 | Loss : 0.2757 | Acc : 0.8722
     Batch 090 | Loss : 0.3400 | Acc : 0.8368
     Batch 095 | Loss : 0.2944 | Acc : 0.8643
     Batch 100 | Loss : 0.3370 | Acc : 0.8348
     Batch 105 | Loss : 0.2919 | Acc : 0.8687
     Batch 110 | Loss : 0.3316 | Acc : 0.8434
     Batch 115 | Loss : 0.3093 | Acc : 0.8543
     Batch 120 | Loss : 0.3251 | Acc : 0.8476
     Batch 125 | Loss : 0.3243 | Acc : 0.8489
     Batch 130 | Loss : 0.2617 | Acc : 0.8823
     Batch 135 | Loss : 0.3293 | Acc : 0.8466
     Batch 140 | Loss : 0.2663 | Acc : 0.8768
     Batch 145 | Loss : 0.2851 | Acc : 0.8693
     Batch 150 | Loss : 0.2927 | Acc : 0.8633
Epoch 00129 | Train Loss : 0.2944 | Eval Loss : 0.3190 | Train acc : 0.8642 | Eval Acc : 0.8521 | Eval Log. Respected : 0.9384
     Batch 000 | Loss : 0.2650 | Acc : 0.8795
     Batch 005 | Loss : 0.3050 | Acc : 0.8583
     Batch 010 | Loss : 0.3200 | Acc : 0.8555
     Batch 015 | Loss : 0.3043 | Acc : 0.8563
     Batch 020 | Loss : 0.2590 | Acc : 0.8838
     Batch 025 | Loss : 0.2578 | Acc : 0.8822
     Batch 030 | Loss : 0.2759 | Acc : 0.8750
     Batch 035 | Loss : 0.2441 | Acc : 0.8924
     Batch 040 | Loss : 0.2665 | Acc : 0.8785
     Batch 045 | Loss : 0.3215 | Acc : 0.8516
     Batch 050 | Loss : 0.3245 | Acc : 0.8499
     Batch 055 | Loss : 0.2871 | Acc : 0.8679
     Batch 060 | Loss : 0.2919 | Acc : 0.8622
     Batch 065 | Loss : 0.2549 | Acc : 0.8819
     Batch 070 | Loss : 0.3198 | Acc : 0.8463
     Batch 075 | Loss : 0.2772 | Acc : 0.8707
     Batch 080 | Loss : 0.2871 | Acc : 0.8678
     Batch 085 | Loss : 0.2717 | Acc : 0.8723
     Batch 090 | Loss : 0.3236 | Acc : 0.8476
     Batch 095 | Loss : 0.2892 | Acc : 0.8681
     Batch 100 | Loss : 0.3225 | Acc : 0.8484
     Batch 105 | Loss : 0.2605 | Acc : 0.8800
     Batch 110 | Loss : 0.2860 | Acc : 0.8681
     Batch 115 | Loss : 0.3403 | Acc : 0.8388
     Batch 120 | Loss : 0.3080 | Acc : 0.8559
     Batch 125 | Loss : 0.2545 | Acc : 0.8863
     Batch 130 | Loss : 0.2952 | Acc : 0.8663
     Batch 135 | Loss : 0.3049 | Acc : 0.8572
     Batch 140 | Loss : 0.3270 | Acc : 0.8478
     Batch 145 | Loss : 0.2868 | Acc : 0.8697
     Batch 150 | Loss : 0.3562 | Acc : 0.8340
Epoch 00130 | Train Loss : 0.2951 | Eval Loss : 0.3185 | Train acc : 0.8642 | Eval Acc : 0.8520 | Eval Log. Respected : 0.9388
     Batch 000 | Loss : 0.2712 | Acc : 0.8764
     Batch 005 | Loss : 0.2978 | Acc : 0.8603
     Batch 010 | Loss : 0.3293 | Acc : 0.8471
     Batch 015 | Loss : 0.2744 | Acc : 0.8738
     Batch 020 | Loss : 0.2539 | Acc : 0.8865
     Batch 025 | Loss : 0.3439 | Acc : 0.8413
     Batch 030 | Loss : 0.2989 | Acc : 0.8606
     Batch 035 | Loss : 0.2541 | Acc : 0.8871
     Batch 040 | Loss : 0.2778 | Acc : 0.8714
     Batch 045 | Loss : 0.3513 | Acc : 0.8353
     Batch 050 | Loss : 0.2633 | Acc : 0.8838
     Batch 055 | Loss : 0.2664 | Acc : 0.8790
     Batch 060 | Loss : 0.2341 | Acc : 0.8953
     Batch 065 | Loss : 0.2959 | Acc : 0.8672
     Batch 070 | Loss : 0.3083 | Acc : 0.8569
     Batch 075 | Loss : 0.2809 | Acc : 0.8723
     Batch 080 | Loss : 0.3044 | Acc : 0.8601
     Batch 085 | Loss : 0.2966 | Acc : 0.8646
     Batch 090 | Loss : 0.2582 | Acc : 0.8804
     Batch 095 | Loss : 0.3248 | Acc : 0.8479
     Batch 100 | Loss : 0.2851 | Acc : 0.8664
     Batch 105 | Loss : 0.2630 | Acc : 0.8806
     Batch 110 | Loss : 0.2804 | Acc : 0.8758
     Batch 115 | Loss : 0.2707 | Acc : 0.8735
     Batch 120 | Loss : 0.3280 | Acc : 0.8489
     Batch 125 | Loss : 0.3093 | Acc : 0.8552
     Batch 130 | Loss : 0.2981 | Acc : 0.8630
     Batch 135 | Loss : 0.2869 | Acc : 0.8676
     Batch 140 | Loss : 0.2579 | Acc : 0.8825
     Batch 145 | Loss : 0.2939 | Acc : 0.8661
     Batch 150 | Loss : 0.2752 | Acc : 0.8732
Epoch 00131 | Train Loss : 0.2943 | Eval Loss : 0.3145 | Train acc : 0.8644 | Eval Acc : 0.8543 | Eval Log. Respected : 0.9398
     Batch 000 | Loss : 0.3799 | Acc : 0.8118
     Batch 005 | Loss : 0.3016 | Acc : 0.8641
     Batch 010 | Loss : 0.2903 | Acc : 0.8666
     Batch 015 | Loss : 0.2638 | Acc : 0.8795
     Batch 020 | Loss : 0.2784 | Acc : 0.8728
     Batch 025 | Loss : 0.2855 | Acc : 0.8655
     Batch 030 | Loss : 0.2819 | Acc : 0.8697
     Batch 035 | Loss : 0.2467 | Acc : 0.8864
     Batch 040 | Loss : 0.2641 | Acc : 0.8806
     Batch 045 | Loss : 0.3514 | Acc : 0.8387
     Batch 050 | Loss : 0.2756 | Acc : 0.8748
     Batch 055 | Loss : 0.2852 | Acc : 0.8691
     Batch 060 | Loss : 0.2982 | Acc : 0.8669
     Batch 065 | Loss : 0.2794 | Acc : 0.8728
     Batch 070 | Loss : 0.2908 | Acc : 0.8682
     Batch 075 | Loss : 0.2865 | Acc : 0.8695
     Batch 080 | Loss : 0.2576 | Acc : 0.8798
     Batch 085 | Loss : 0.2623 | Acc : 0.8824
     Batch 090 | Loss : 0.3523 | Acc : 0.8393
     Batch 095 | Loss : 0.3081 | Acc : 0.8577
     Batch 100 | Loss : 0.3058 | Acc : 0.8619
     Batch 105 | Loss : 0.3182 | Acc : 0.8528
     Batch 110 | Loss : 0.3041 | Acc : 0.8614
     Batch 115 | Loss : 0.2634 | Acc : 0.8826
     Batch 120 | Loss : 0.3058 | Acc : 0.8565
     Batch 125 | Loss : 0.2402 | Acc : 0.8936
     Batch 130 | Loss : 0.3678 | Acc : 0.8362
     Batch 135 | Loss : 0.3028 | Acc : 0.8555
     Batch 140 | Loss : 0.2578 | Acc : 0.8853
     Batch 145 | Loss : 0.2913 | Acc : 0.8672
     Batch 150 | Loss : 0.3141 | Acc : 0.8554
Epoch 00132 | Train Loss : 0.2928 | Eval Loss : 0.3160 | Train acc : 0.8652 | Eval Acc : 0.8549 | Eval Log. Respected : 0.9337
     Batch 000 | Loss : 0.2596 | Acc : 0.8825
     Batch 005 | Loss : 0.2840 | Acc : 0.8678
     Batch 010 | Loss : 0.3296 | Acc : 0.8455
     Batch 015 | Loss : 0.2975 | Acc : 0.8638
     Batch 020 | Loss : 0.2704 | Acc : 0.8759
     Batch 025 | Loss : 0.3111 | Acc : 0.8554
     Batch 030 | Loss : 0.2701 | Acc : 0.8770
     Batch 035 | Loss : 0.2755 | Acc : 0.8784
     Batch 040 | Loss : 0.2527 | Acc : 0.8872
     Batch 045 | Loss : 0.3191 | Acc : 0.8524
     Batch 050 | Loss : 0.2640 | Acc : 0.8793
     Batch 055 | Loss : 0.3129 | Acc : 0.8535
     Batch 060 | Loss : 0.3017 | Acc : 0.8562
     Batch 065 | Loss : 0.2889 | Acc : 0.8664
     Batch 070 | Loss : 0.2471 | Acc : 0.8900
     Batch 075 | Loss : 0.3017 | Acc : 0.8608
     Batch 080 | Loss : 0.2526 | Acc : 0.8857
     Batch 085 | Loss : 0.3233 | Acc : 0.8510
     Batch 090 | Loss : 0.3017 | Acc : 0.8616
     Batch 095 | Loss : 0.3046 | Acc : 0.8636
     Batch 100 | Loss : 0.2859 | Acc : 0.8694
     Batch 105 | Loss : 0.2638 | Acc : 0.8827
     Batch 110 | Loss : 0.2633 | Acc : 0.8797
     Batch 115 | Loss : 0.2613 | Acc : 0.8830
     Batch 120 | Loss : 0.3330 | Acc : 0.8464
     Batch 125 | Loss : 0.3060 | Acc : 0.8583
     Batch 130 | Loss : 0.3318 | Acc : 0.8452
     Batch 135 | Loss : 0.2718 | Acc : 0.8794
     Batch 140 | Loss : 0.2870 | Acc : 0.8655
     Batch 145 | Loss : 0.2455 | Acc : 0.8914
     Batch 150 | Loss : 0.3219 | Acc : 0.8500
Epoch 00133 | Train Loss : 0.2935 | Eval Loss : 0.3171 | Train acc : 0.8647 | Eval Acc : 0.8533 | Eval Log. Respected : 0.9378
     Batch 000 | Loss : 0.3086 | Acc : 0.8559
     Batch 005 | Loss : 0.2575 | Acc : 0.8819
     Batch 010 | Loss : 0.2648 | Acc : 0.8766
     Batch 015 | Loss : 0.3061 | Acc : 0.8573
     Batch 020 | Loss : 0.3480 | Acc : 0.8363
     Batch 025 | Loss : 0.2798 | Acc : 0.8733
     Batch 030 | Loss : 0.3038 | Acc : 0.8600
     Batch 035 | Loss : 0.2813 | Acc : 0.8705
     Batch 040 | Loss : 0.3026 | Acc : 0.8583
     Batch 045 | Loss : 0.2870 | Acc : 0.8728
     Batch 050 | Loss : 0.3114 | Acc : 0.8575
     Batch 055 | Loss : 0.3490 | Acc : 0.8347
     Batch 060 | Loss : 0.3028 | Acc : 0.8597
     Batch 065 | Loss : 0.2597 | Acc : 0.8819
     Batch 070 | Loss : 0.3008 | Acc : 0.8618
     Batch 075 | Loss : 0.3550 | Acc : 0.8284
     Batch 080 | Loss : 0.3183 | Acc : 0.8486
     Batch 085 | Loss : 0.2784 | Acc : 0.8723
     Batch 090 | Loss : 0.2760 | Acc : 0.8730
     Batch 095 | Loss : 0.2516 | Acc : 0.8854
     Batch 100 | Loss : 0.2398 | Acc : 0.8908
     Batch 105 | Loss : 0.2939 | Acc : 0.8658
     Batch 110 | Loss : 0.2625 | Acc : 0.8822
     Batch 115 | Loss : 0.2896 | Acc : 0.8616
     Batch 120 | Loss : 0.2762 | Acc : 0.8742
     Batch 125 | Loss : 0.3032 | Acc : 0.8585
     Batch 130 | Loss : 0.3157 | Acc : 0.8515
     Batch 135 | Loss : 0.2599 | Acc : 0.8841
     Batch 140 | Loss : 0.3886 | Acc : 0.8285
     Batch 145 | Loss : 0.2623 | Acc : 0.8809
     Batch 150 | Loss : 0.3054 | Acc : 0.8575
Epoch 00134 | Train Loss : 0.2933 | Eval Loss : 0.3186 | Train acc : 0.8651 | Eval Acc : 0.8544 | Eval Log. Respected : 0.9364
     Batch 000 | Loss : 0.2591 | Acc : 0.8837
     Batch 005 | Loss : 0.2650 | Acc : 0.8787
     Batch 010 | Loss : 0.2489 | Acc : 0.8894
     Batch 015 | Loss : 0.2457 | Acc : 0.8891
     Batch 020 | Loss : 0.2960 | Acc : 0.8668
     Batch 025 | Loss : 0.2537 | Acc : 0.8839
     Batch 030 | Loss : 0.2985 | Acc : 0.8605
     Batch 035 | Loss : 0.2969 | Acc : 0.8635
     Batch 040 | Loss : 0.2765 | Acc : 0.8697
     Batch 045 | Loss : 0.2694 | Acc : 0.8782
     Batch 050 | Loss : 0.2787 | Acc : 0.8729
     Batch 055 | Loss : 0.2769 | Acc : 0.8704
     Batch 060 | Loss : 0.2688 | Acc : 0.8797
     Batch 065 | Loss : 0.2824 | Acc : 0.8683
     Batch 070 | Loss : 0.3363 | Acc : 0.8469
     Batch 075 | Loss : 0.3186 | Acc : 0.8521
     Batch 080 | Loss : 0.2674 | Acc : 0.8752
     Batch 085 | Loss : 0.2324 | Acc : 0.8972
     Batch 090 | Loss : 0.3079 | Acc : 0.8555
     Batch 095 | Loss : 0.2991 | Acc : 0.8603
     Batch 100 | Loss : 0.2736 | Acc : 0.8781
     Batch 105 | Loss : 0.2852 | Acc : 0.8709
     Batch 110 | Loss : 0.2429 | Acc : 0.8929
     Batch 115 | Loss : 0.2665 | Acc : 0.8788
     Batch 120 | Loss : 0.3172 | Acc : 0.8536
     Batch 125 | Loss : 0.2681 | Acc : 0.8753
     Batch 130 | Loss : 0.3189 | Acc : 0.8556
     Batch 135 | Loss : 0.2889 | Acc : 0.8664
     Batch 140 | Loss : 0.2856 | Acc : 0.8650
     Batch 145 | Loss : 0.3059 | Acc : 0.8585
     Batch 150 | Loss : 0.2554 | Acc : 0.8845
Epoch 00135 | Train Loss : 0.2921 | Eval Loss : 0.3212 | Train acc : 0.8655 | Eval Acc : 0.8538 | Eval Log. Respected : 0.9331
     Batch 000 | Loss : 0.3046 | Acc : 0.8602
     Batch 005 | Loss : 0.2915 | Acc : 0.8683
     Batch 010 | Loss : 0.3188 | Acc : 0.8506
     Batch 015 | Loss : 0.2634 | Acc : 0.8821
     Batch 020 | Loss : 0.3152 | Acc : 0.8520
     Batch 025 | Loss : 0.2767 | Acc : 0.8764
     Batch 030 | Loss : 0.2579 | Acc : 0.8818
     Batch 035 | Loss : 0.2778 | Acc : 0.8753
     Batch 040 | Loss : 0.3450 | Acc : 0.8334
     Batch 045 | Loss : 0.2586 | Acc : 0.8838
     Batch 050 | Loss : 0.3123 | Acc : 0.8537
     Batch 055 | Loss : 0.3486 | Acc : 0.8355
     Batch 060 | Loss : 0.2982 | Acc : 0.8616
     Batch 065 | Loss : 0.2995 | Acc : 0.8627
     Batch 070 | Loss : 0.2397 | Acc : 0.8908
     Batch 075 | Loss : 0.2494 | Acc : 0.8871
     Batch 080 | Loss : 0.3140 | Acc : 0.8533
     Batch 085 | Loss : 0.3027 | Acc : 0.8593
     Batch 090 | Loss : 0.2755 | Acc : 0.8721
     Batch 095 | Loss : 0.2627 | Acc : 0.8808
     Batch 100 | Loss : 0.2859 | Acc : 0.8675
     Batch 105 | Loss : 0.3409 | Acc : 0.8389
     Batch 110 | Loss : 0.3238 | Acc : 0.8496
     Batch 115 | Loss : 0.2683 | Acc : 0.8799
     Batch 120 | Loss : 0.2580 | Acc : 0.8831
     Batch 125 | Loss : 0.3592 | Acc : 0.8297
     Batch 130 | Loss : 0.3323 | Acc : 0.8436
     Batch 135 | Loss : 0.2710 | Acc : 0.8784
     Batch 140 | Loss : 0.2905 | Acc : 0.8650
     Batch 145 | Loss : 0.2819 | Acc : 0.8737
     Batch 150 | Loss : 0.3068 | Acc : 0.8647
Epoch 00136 | Train Loss : 0.2934 | Eval Loss : 0.3196 | Train acc : 0.8647 | Eval Acc : 0.8534 | Eval Log. Respected : 0.9335
     Batch 000 | Loss : 0.2703 | Acc : 0.8757
     Batch 005 | Loss : 0.2821 | Acc : 0.8706
     Batch 010 | Loss : 0.3332 | Acc : 0.8449
     Batch 015 | Loss : 0.3193 | Acc : 0.8484
     Batch 020 | Loss : 0.3106 | Acc : 0.8617
     Batch 025 | Loss : 0.2644 | Acc : 0.8785
     Batch 030 | Loss : 0.2692 | Acc : 0.8769
     Batch 035 | Loss : 0.3318 | Acc : 0.8412
     Batch 040 | Loss : 0.2753 | Acc : 0.8727
     Batch 045 | Loss : 0.2541 | Acc : 0.8852
     Batch 050 | Loss : 0.3054 | Acc : 0.8580
     Batch 055 | Loss : 0.3447 | Acc : 0.8371
     Batch 060 | Loss : 0.2784 | Acc : 0.8725
     Batch 065 | Loss : 0.2613 | Acc : 0.8805
     Batch 070 | Loss : 0.2741 | Acc : 0.8758
     Batch 075 | Loss : 0.2934 | Acc : 0.8652
     Batch 080 | Loss : 0.2933 | Acc : 0.8650
     Batch 085 | Loss : 0.2826 | Acc : 0.8711
     Batch 090 | Loss : 0.3044 | Acc : 0.8580
     Batch 095 | Loss : 0.3063 | Acc : 0.8582
     Batch 100 | Loss : 0.3001 | Acc : 0.8627
     Batch 105 | Loss : 0.2962 | Acc : 0.8652
     Batch 110 | Loss : 0.3127 | Acc : 0.8521
     Batch 115 | Loss : 0.2491 | Acc : 0.8848
     Batch 120 | Loss : 0.2866 | Acc : 0.8689
     Batch 125 | Loss : 0.2787 | Acc : 0.8723
     Batch 130 | Loss : 0.2502 | Acc : 0.8867
     Batch 135 | Loss : 0.3001 | Acc : 0.8593
     Batch 140 | Loss : 0.3203 | Acc : 0.8514
     Batch 145 | Loss : 0.3002 | Acc : 0.8618
     Batch 150 | Loss : 0.3247 | Acc : 0.8453
Epoch 00137 | Train Loss : 0.2922 | Eval Loss : 0.3178 | Train acc : 0.8655 | Eval Acc : 0.8529 | Eval Log. Respected : 0.9364
     Batch 000 | Loss : 0.2579 | Acc : 0.8853
     Batch 005 | Loss : 0.3205 | Acc : 0.8466
     Batch 010 | Loss : 0.2699 | Acc : 0.8770
     Batch 015 | Loss : 0.2485 | Acc : 0.8896
     Batch 020 | Loss : 0.3028 | Acc : 0.8599
     Batch 025 | Loss : 0.3035 | Acc : 0.8609
     Batch 030 | Loss : 0.2826 | Acc : 0.8734
     Batch 035 | Loss : 0.3385 | Acc : 0.8427
     Batch 040 | Loss : 0.2701 | Acc : 0.8782
     Batch 045 | Loss : 0.2878 | Acc : 0.8637
     Batch 050 | Loss : 0.2765 | Acc : 0.8746
     Batch 055 | Loss : 0.3094 | Acc : 0.8554
     Batch 060 | Loss : 0.3064 | Acc : 0.8582
     Batch 065 | Loss : 0.2539 | Acc : 0.8829
     Batch 070 | Loss : 0.3623 | Acc : 0.8335
     Batch 075 | Loss : 0.2714 | Acc : 0.8744
     Batch 080 | Loss : 0.3053 | Acc : 0.8576
     Batch 085 | Loss : 0.3547 | Acc : 0.8344
     Batch 090 | Loss : 0.2800 | Acc : 0.8699
     Batch 095 | Loss : 0.3259 | Acc : 0.8470
     Batch 100 | Loss : 0.3162 | Acc : 0.8533
     Batch 105 | Loss : 0.2885 | Acc : 0.8683
     Batch 110 | Loss : 0.3601 | Acc : 0.8293
     Batch 115 | Loss : 0.3477 | Acc : 0.8393
     Batch 120 | Loss : 0.2492 | Acc : 0.8891
     Batch 125 | Loss : 0.2605 | Acc : 0.8814
     Batch 130 | Loss : 0.3286 | Acc : 0.8501
     Batch 135 | Loss : 0.2636 | Acc : 0.8831
     Batch 140 | Loss : 0.3406 | Acc : 0.8488
     Batch 145 | Loss : 0.2755 | Acc : 0.8768
     Batch 150 | Loss : 0.2730 | Acc : 0.8761
Epoch 00138 | Train Loss : 0.2925 | Eval Loss : 0.3214 | Train acc : 0.8655 | Eval Acc : 0.8525 | Eval Log. Respected : 0.9307
     Batch 000 | Loss : 0.3127 | Acc : 0.8554
     Batch 005 | Loss : 0.3150 | Acc : 0.8523
     Batch 010 | Loss : 0.2533 | Acc : 0.8855
     Batch 015 | Loss : 0.2789 | Acc : 0.8729
     Batch 020 | Loss : 0.2860 | Acc : 0.8661
     Batch 025 | Loss : 0.3282 | Acc : 0.8472
     Batch 030 | Loss : 0.3249 | Acc : 0.8448
     Batch 035 | Loss : 0.2837 | Acc : 0.8688
     Batch 040 | Loss : 0.2959 | Acc : 0.8615
     Batch 045 | Loss : 0.2722 | Acc : 0.8789
     Batch 050 | Loss : 0.2465 | Acc : 0.8898
     Batch 055 | Loss : 0.2853 | Acc : 0.8718
     Batch 060 | Loss : 0.3080 | Acc : 0.8629
     Batch 065 | Loss : 0.2779 | Acc : 0.8738
     Batch 070 | Loss : 0.2870 | Acc : 0.8700
     Batch 075 | Loss : 0.2563 | Acc : 0.8861
     Batch 080 | Loss : 0.2970 | Acc : 0.8612
     Batch 085 | Loss : 0.2877 | Acc : 0.8727
     Batch 090 | Loss : 0.2865 | Acc : 0.8686
     Batch 095 | Loss : 0.2553 | Acc : 0.8859
     Batch 100 | Loss : 0.3021 | Acc : 0.8601
     Batch 105 | Loss : 0.3685 | Acc : 0.8240
     Batch 110 | Loss : 0.2632 | Acc : 0.8823
     Batch 115 | Loss : 0.2844 | Acc : 0.8670
     Batch 120 | Loss : 0.3161 | Acc : 0.8518
     Batch 125 | Loss : 0.3270 | Acc : 0.8477
     Batch 130 | Loss : 0.2893 | Acc : 0.8678
     Batch 135 | Loss : 0.2543 | Acc : 0.8863
     Batch 140 | Loss : 0.3201 | Acc : 0.8503
     Batch 145 | Loss : 0.2874 | Acc : 0.8665
     Batch 150 | Loss : 0.2814 | Acc : 0.8704
Epoch 00139 | Train Loss : 0.2930 | Eval Loss : 0.3188 | Train acc : 0.8653 | Eval Acc : 0.8538 | Eval Log. Respected : 0.9339
     Batch 000 | Loss : 0.2835 | Acc : 0.8678
     Batch 005 | Loss : 0.3030 | Acc : 0.8652
     Batch 010 | Loss : 0.2759 | Acc : 0.8773
     Batch 015 | Loss : 0.2429 | Acc : 0.8910
     Batch 020 | Loss : 0.3167 | Acc : 0.8508
     Batch 025 | Loss : 0.3632 | Acc : 0.8331
     Batch 030 | Loss : 0.3177 | Acc : 0.8496
     Batch 035 | Loss : 0.3121 | Acc : 0.8501
     Batch 040 | Loss : 0.2957 | Acc : 0.8662
     Batch 045 | Loss : 0.2490 | Acc : 0.8848
     Batch 050 | Loss : 0.2764 | Acc : 0.8728
     Batch 055 | Loss : 0.2925 | Acc : 0.8646
     Batch 060 | Loss : 0.3248 | Acc : 0.8425
     Batch 065 | Loss : 0.2585 | Acc : 0.8827
     Batch 070 | Loss : 0.2881 | Acc : 0.8685
     Batch 075 | Loss : 0.3387 | Acc : 0.8424
     Batch 080 | Loss : 0.2969 | Acc : 0.8638
     Batch 085 | Loss : 0.2631 | Acc : 0.8787
     Batch 090 | Loss : 0.2675 | Acc : 0.8768
     Batch 095 | Loss : 0.3187 | Acc : 0.8531
     Batch 100 | Loss : 0.2627 | Acc : 0.8826
     Batch 105 | Loss : 0.2791 | Acc : 0.8739
     Batch 110 | Loss : 0.3077 | Acc : 0.8563
     Batch 115 | Loss : 0.2807 | Acc : 0.8697
     Batch 120 | Loss : 0.3078 | Acc : 0.8615
     Batch 125 | Loss : 0.3096 | Acc : 0.8608
     Batch 130 | Loss : 0.2765 | Acc : 0.8729
     Batch 135 | Loss : 0.2833 | Acc : 0.8708
     Batch 140 | Loss : 0.2937 | Acc : 0.8654
     Batch 145 | Loss : 0.2522 | Acc : 0.8890
     Batch 150 | Loss : 0.2956 | Acc : 0.8645
Epoch 00140 | Train Loss : 0.2922 | Eval Loss : 0.3221 | Train acc : 0.8655 | Eval Acc : 0.8524 | Eval Log. Respected : 0.9410
     Batch 000 | Loss : 0.3268 | Acc : 0.8504
     Batch 005 | Loss : 0.2363 | Acc : 0.8943
     Batch 010 | Loss : 0.2579 | Acc : 0.8841
     Batch 015 | Loss : 0.2677 | Acc : 0.8802
     Batch 020 | Loss : 0.3076 | Acc : 0.8597
     Batch 025 | Loss : 0.3163 | Acc : 0.8562
     Batch 030 | Loss : 0.2451 | Acc : 0.8917
     Batch 035 | Loss : 0.3173 | Acc : 0.8528
     Batch 040 | Loss : 0.2618 | Acc : 0.8831
     Batch 045 | Loss : 0.3212 | Acc : 0.8465
     Batch 050 | Loss : 0.3046 | Acc : 0.8615
     Batch 055 | Loss : 0.3656 | Acc : 0.8285
     Batch 060 | Loss : 0.3240 | Acc : 0.8477
     Batch 065 | Loss : 0.2749 | Acc : 0.8780
     Batch 070 | Loss : 0.2566 | Acc : 0.8840
     Batch 075 | Loss : 0.2433 | Acc : 0.8886
     Batch 080 | Loss : 0.2957 | Acc : 0.8639
     Batch 085 | Loss : 0.2754 | Acc : 0.8784
     Batch 090 | Loss : 0.2463 | Acc : 0.8884
     Batch 095 | Loss : 0.2433 | Acc : 0.8904
     Batch 100 | Loss : 0.2855 | Acc : 0.8656
     Batch 105 | Loss : 0.2619 | Acc : 0.8807
     Batch 110 | Loss : 0.2960 | Acc : 0.8608
     Batch 115 | Loss : 0.2871 | Acc : 0.8711
     Batch 120 | Loss : 0.3020 | Acc : 0.8593
     Batch 125 | Loss : 0.2862 | Acc : 0.8712
     Batch 130 | Loss : 0.2814 | Acc : 0.8731
     Batch 135 | Loss : 0.2992 | Acc : 0.8621
     Batch 140 | Loss : 0.3318 | Acc : 0.8436
     Batch 145 | Loss : 0.2941 | Acc : 0.8641
     Batch 150 | Loss : 0.3075 | Acc : 0.8561
Epoch 00141 | Train Loss : 0.2918 | Eval Loss : 0.3182 | Train acc : 0.8656 | Eval Acc : 0.8536 | Eval Log. Respected : 0.9474
     Batch 000 | Loss : 0.3314 | Acc : 0.8453
     Batch 005 | Loss : 0.2591 | Acc : 0.8817
     Batch 010 | Loss : 0.2504 | Acc : 0.8897
     Batch 015 | Loss : 0.3079 | Acc : 0.8559
     Batch 020 | Loss : 0.2618 | Acc : 0.8826
     Batch 025 | Loss : 0.2778 | Acc : 0.8730
     Batch 030 | Loss : 0.2496 | Acc : 0.8889
     Batch 035 | Loss : 0.2968 | Acc : 0.8651
     Batch 040 | Loss : 0.2489 | Acc : 0.8891
     Batch 045 | Loss : 0.3124 | Acc : 0.8544
     Batch 050 | Loss : 0.2519 | Acc : 0.8851
     Batch 055 | Loss : 0.2764 | Acc : 0.8688
     Batch 060 | Loss : 0.3175 | Acc : 0.8495
     Batch 065 | Loss : 0.3698 | Acc : 0.8216
     Batch 070 | Loss : 0.2727 | Acc : 0.8792
     Batch 075 | Loss : 0.2456 | Acc : 0.8883
     Batch 080 | Loss : 0.3039 | Acc : 0.8607
     Batch 085 | Loss : 0.2718 | Acc : 0.8776
     Batch 090 | Loss : 0.3360 | Acc : 0.8514
     Batch 095 | Loss : 0.2836 | Acc : 0.8700
     Batch 100 | Loss : 0.2966 | Acc : 0.8657
     Batch 105 | Loss : 0.2649 | Acc : 0.8793
     Batch 110 | Loss : 0.2681 | Acc : 0.8757
     Batch 115 | Loss : 0.2686 | Acc : 0.8800
     Batch 120 | Loss : 0.2595 | Acc : 0.8831
     Batch 125 | Loss : 0.2980 | Acc : 0.8636
     Batch 130 | Loss : 0.2842 | Acc : 0.8708
     Batch 135 | Loss : 0.3271 | Acc : 0.8460
     Batch 140 | Loss : 0.3500 | Acc : 0.8339
     Batch 145 | Loss : 0.2740 | Acc : 0.8733
     Batch 150 | Loss : 0.2956 | Acc : 0.8634
Epoch 00142 | Train Loss : 0.2913 | Eval Loss : 0.3177 | Train acc : 0.8658 | Eval Acc : 0.8533 | Eval Log. Respected : 0.9406
     Batch 000 | Loss : 0.3150 | Acc : 0.8501
     Batch 005 | Loss : 0.3481 | Acc : 0.8404
     Batch 010 | Loss : 0.2624 | Acc : 0.8804
     Batch 015 | Loss : 0.3461 | Acc : 0.8355
     Batch 020 | Loss : 0.2937 | Acc : 0.8605
     Batch 025 | Loss : 0.2925 | Acc : 0.8668
     Batch 030 | Loss : 0.2795 | Acc : 0.8698
     Batch 035 | Loss : 0.3201 | Acc : 0.8517
     Batch 040 | Loss : 0.2987 | Acc : 0.8587
     Batch 045 | Loss : 0.3241 | Acc : 0.8462
     Batch 050 | Loss : 0.2564 | Acc : 0.8835
     Batch 055 | Loss : 0.2540 | Acc : 0.8835
     Batch 060 | Loss : 0.2858 | Acc : 0.8663
     Batch 065 | Loss : 0.3189 | Acc : 0.8497
     Batch 070 | Loss : 0.3348 | Acc : 0.8394
     Batch 075 | Loss : 0.2662 | Acc : 0.8797
     Batch 080 | Loss : 0.2645 | Acc : 0.8767
     Batch 085 | Loss : 0.2631 | Acc : 0.8810
     Batch 090 | Loss : 0.2564 | Acc : 0.8863
     Batch 095 | Loss : 0.3163 | Acc : 0.8491
     Batch 100 | Loss : 0.3191 | Acc : 0.8510
     Batch 105 | Loss : 0.3140 | Acc : 0.8535
     Batch 110 | Loss : 0.2864 | Acc : 0.8687
     Batch 115 | Loss : 0.2834 | Acc : 0.8686
     Batch 120 | Loss : 0.2298 | Acc : 0.8962
     Batch 125 | Loss : 0.3063 | Acc : 0.8557
     Batch 130 | Loss : 0.3404 | Acc : 0.8391
     Batch 135 | Loss : 0.3067 | Acc : 0.8596
     Batch 140 | Loss : 0.3259 | Acc : 0.8526
     Batch 145 | Loss : 0.2796 | Acc : 0.8703
     Batch 150 | Loss : 0.2401 | Acc : 0.8939
Epoch 00143 | Train Loss : 0.2919 | Eval Loss : 0.3177 | Train acc : 0.8656 | Eval Acc : 0.8525 | Eval Log. Respected : 0.9420
     Batch 000 | Loss : 0.2451 | Acc : 0.8898
     Batch 005 | Loss : 0.2610 | Acc : 0.8795
     Batch 010 | Loss : 0.2735 | Acc : 0.8759
     Batch 015 | Loss : 0.2903 | Acc : 0.8639
     Batch 020 | Loss : 0.2953 | Acc : 0.8605
     Batch 025 | Loss : 0.2714 | Acc : 0.8749
     Batch 030 | Loss : 0.2863 | Acc : 0.8674
     Batch 035 | Loss : 0.2651 | Acc : 0.8776
     Batch 040 | Loss : 0.3082 | Acc : 0.8608
     Batch 045 | Loss : 0.2880 | Acc : 0.8645
     Batch 050 | Loss : 0.2978 | Acc : 0.8663
     Batch 055 | Loss : 0.2373 | Acc : 0.8956
     Batch 060 | Loss : 0.3275 | Acc : 0.8531
     Batch 065 | Loss : 0.3058 | Acc : 0.8586
     Batch 070 | Loss : 0.3450 | Acc : 0.8368
     Batch 075 | Loss : 0.2766 | Acc : 0.8736
     Batch 080 | Loss : 0.3111 | Acc : 0.8551
     Batch 085 | Loss : 0.2478 | Acc : 0.8906
     Batch 090 | Loss : 0.2758 | Acc : 0.8691
     Batch 095 | Loss : 0.2740 | Acc : 0.8741
     Batch 100 | Loss : 0.3517 | Acc : 0.8314
     Batch 105 | Loss : 0.2586 | Acc : 0.8798
     Batch 110 | Loss : 0.2794 | Acc : 0.8766
     Batch 115 | Loss : 0.2526 | Acc : 0.8848
     Batch 120 | Loss : 0.3117 | Acc : 0.8497
     Batch 125 | Loss : 0.3351 | Acc : 0.8456
     Batch 130 | Loss : 0.2677 | Acc : 0.8767
     Batch 135 | Loss : 0.2921 | Acc : 0.8628
     Batch 140 | Loss : 0.2387 | Acc : 0.8972
     Batch 145 | Loss : 0.3413 | Acc : 0.8406
     Batch 150 | Loss : 0.2657 | Acc : 0.8814
Epoch 00144 | Train Loss : 0.2916 | Eval Loss : 0.3170 | Train acc : 0.8658 | Eval Acc : 0.8532 | Eval Log. Respected : 0.9392
     Batch 000 | Loss : 0.2783 | Acc : 0.8793
     Batch 005 | Loss : 0.3023 | Acc : 0.8593
     Batch 010 | Loss : 0.2485 | Acc : 0.8898
     Batch 015 | Loss : 0.2753 | Acc : 0.8761
     Batch 020 | Loss : 0.2822 | Acc : 0.8700
     Batch 025 | Loss : 0.2825 | Acc : 0.8717
     Batch 030 | Loss : 0.2558 | Acc : 0.8838
     Batch 035 | Loss : 0.2528 | Acc : 0.8874
     Batch 040 | Loss : 0.2785 | Acc : 0.8693
     Batch 045 | Loss : 0.3196 | Acc : 0.8525
     Batch 050 | Loss : 0.2599 | Acc : 0.8818
     Batch 055 | Loss : 0.3283 | Acc : 0.8443
     Batch 060 | Loss : 0.2729 | Acc : 0.8760
     Batch 065 | Loss : 0.3110 | Acc : 0.8580
     Batch 070 | Loss : 0.2862 | Acc : 0.8693
     Batch 075 | Loss : 0.2628 | Acc : 0.8827
     Batch 080 | Loss : 0.2924 | Acc : 0.8651
     Batch 085 | Loss : 0.2597 | Acc : 0.8809
     Batch 090 | Loss : 0.2923 | Acc : 0.8645
     Batch 095 | Loss : 0.2625 | Acc : 0.8795
     Batch 100 | Loss : 0.2730 | Acc : 0.8758
     Batch 105 | Loss : 0.2729 | Acc : 0.8737
     Batch 110 | Loss : 0.2547 | Acc : 0.8867
     Batch 115 | Loss : 0.3178 | Acc : 0.8522
     Batch 120 | Loss : 0.2602 | Acc : 0.8842
     Batch 125 | Loss : 0.2596 | Acc : 0.8805
     Batch 130 | Loss : 0.2647 | Acc : 0.8774
     Batch 135 | Loss : 0.2761 | Acc : 0.8698
     Batch 140 | Loss : 0.3019 | Acc : 0.8590
     Batch 145 | Loss : 0.2853 | Acc : 0.8708
     Batch 150 | Loss : 0.2873 | Acc : 0.8706
Epoch 00145 | Train Loss : 0.2909 | Eval Loss : 0.3162 | Train acc : 0.8659 | Eval Acc : 0.8530 | Eval Log. Respected : 0.9406
     Batch 000 | Loss : 0.2876 | Acc : 0.8662
     Batch 005 | Loss : 0.2858 | Acc : 0.8686
     Batch 010 | Loss : 0.2989 | Acc : 0.8612
     Batch 015 | Loss : 0.2647 | Acc : 0.8813
     Batch 020 | Loss : 0.2989 | Acc : 0.8606
     Batch 025 | Loss : 0.2921 | Acc : 0.8594
     Batch 030 | Loss : 0.2662 | Acc : 0.8752
     Batch 035 | Loss : 0.3339 | Acc : 0.8410
     Batch 040 | Loss : 0.2621 | Acc : 0.8819
     Batch 045 | Loss : 0.2528 | Acc : 0.8851
     Batch 050 | Loss : 0.3224 | Acc : 0.8521
     Batch 055 | Loss : 0.3856 | Acc : 0.8278
     Batch 060 | Loss : 0.2862 | Acc : 0.8695
     Batch 065 | Loss : 0.2922 | Acc : 0.8640
     Batch 070 | Loss : 0.2840 | Acc : 0.8667
     Batch 075 | Loss : 0.2854 | Acc : 0.8677
     Batch 080 | Loss : 0.2846 | Acc : 0.8697
     Batch 085 | Loss : 0.3325 | Acc : 0.8459
     Batch 090 | Loss : 0.2640 | Acc : 0.8814
     Batch 095 | Loss : 0.2661 | Acc : 0.8809
     Batch 100 | Loss : 0.3246 | Acc : 0.8499
     Batch 105 | Loss : 0.2662 | Acc : 0.8792
     Batch 110 | Loss : 0.3353 | Acc : 0.8401
     Batch 115 | Loss : 0.3537 | Acc : 0.8334
     Batch 120 | Loss : 0.2470 | Acc : 0.8851
     Batch 125 | Loss : 0.3205 | Acc : 0.8520
     Batch 130 | Loss : 0.3648 | Acc : 0.8222
     Batch 135 | Loss : 0.3112 | Acc : 0.8522
     Batch 140 | Loss : 0.2986 | Acc : 0.8650
     Batch 145 | Loss : 0.2666 | Acc : 0.8760
     Batch 150 | Loss : 0.2654 | Acc : 0.8796
Epoch 00146 | Train Loss : 0.2905 | Eval Loss : 0.3215 | Train acc : 0.8664 | Eval Acc : 0.8533 | Eval Log. Respected : 0.9345
     Batch 000 | Loss : 0.2815 | Acc : 0.8726
     Batch 005 | Loss : 0.2580 | Acc : 0.8822
     Batch 010 | Loss : 0.3172 | Acc : 0.8526
     Batch 015 | Loss : 0.3040 | Acc : 0.8609
     Batch 020 | Loss : 0.2922 | Acc : 0.8645
     Batch 025 | Loss : 0.2956 | Acc : 0.8654
     Batch 030 | Loss : 0.2926 | Acc : 0.8705
     Batch 035 | Loss : 0.3895 | Acc : 0.8171
     Batch 040 | Loss : 0.2617 | Acc : 0.8812
     Batch 045 | Loss : 0.2441 | Acc : 0.8915
     Batch 050 | Loss : 0.2869 | Acc : 0.8657
     Batch 055 | Loss : 0.2691 | Acc : 0.8783
     Batch 060 | Loss : 0.3659 | Acc : 0.8228
     Batch 065 | Loss : 0.2988 | Acc : 0.8610
     Batch 070 | Loss : 0.2767 | Acc : 0.8737
     Batch 075 | Loss : 0.2891 | Acc : 0.8636
     Batch 080 | Loss : 0.3188 | Acc : 0.8514
     Batch 085 | Loss : 0.3095 | Acc : 0.8562
     Batch 090 | Loss : 0.3668 | Acc : 0.8338
     Batch 095 | Loss : 0.2819 | Acc : 0.8726
     Batch 100 | Loss : 0.2821 | Acc : 0.8727
     Batch 105 | Loss : 0.2406 | Acc : 0.8913
     Batch 110 | Loss : 0.3055 | Acc : 0.8593
     Batch 115 | Loss : 0.3129 | Acc : 0.8540
     Batch 120 | Loss : 0.2708 | Acc : 0.8810
     Batch 125 | Loss : 0.2531 | Acc : 0.8829
     Batch 130 | Loss : 0.2911 | Acc : 0.8630
     Batch 135 | Loss : 0.2851 | Acc : 0.8681
     Batch 140 | Loss : 0.2556 | Acc : 0.8830
     Batch 145 | Loss : 0.2574 | Acc : 0.8815
     Batch 150 | Loss : 0.2736 | Acc : 0.8725
Epoch 00147 | Train Loss : 0.2911 | Eval Loss : 0.3204 | Train acc : 0.8661 | Eval Acc : 0.8530 | Eval Log. Respected : 0.9416
     Batch 000 | Loss : 0.2545 | Acc : 0.8849
     Batch 005 | Loss : 0.3313 | Acc : 0.8435
     Batch 010 | Loss : 0.3092 | Acc : 0.8576
     Batch 015 | Loss : 0.3006 | Acc : 0.8593
     Batch 020 | Loss : 0.2483 | Acc : 0.8863
     Batch 025 | Loss : 0.2728 | Acc : 0.8740
     Batch 030 | Loss : 0.2733 | Acc : 0.8780
     Batch 035 | Loss : 0.3041 | Acc : 0.8657
     Batch 040 | Loss : 0.2976 | Acc : 0.8615
     Batch 045 | Loss : 0.3101 | Acc : 0.8548
     Batch 050 | Loss : 0.2706 | Acc : 0.8787
     Batch 055 | Loss : 0.3505 | Acc : 0.8374
     Batch 060 | Loss : 0.3224 | Acc : 0.8490
     Batch 065 | Loss : 0.2731 | Acc : 0.8738
     Batch 070 | Loss : 0.2836 | Acc : 0.8744
     Batch 075 | Loss : 0.2647 | Acc : 0.8795
     Batch 080 | Loss : 0.2999 | Acc : 0.8576
     Batch 085 | Loss : 0.2891 | Acc : 0.8661
     Batch 090 | Loss : 0.2663 | Acc : 0.8784
     Batch 095 | Loss : 0.3194 | Acc : 0.8472
     Batch 100 | Loss : 0.3231 | Acc : 0.8467
     Batch 105 | Loss : 0.2576 | Acc : 0.8850
     Batch 110 | Loss : 0.2511 | Acc : 0.8892
     Batch 115 | Loss : 0.3121 | Acc : 0.8550
     Batch 120 | Loss : 0.2750 | Acc : 0.8756
     Batch 125 | Loss : 0.2854 | Acc : 0.8687
     Batch 130 | Loss : 0.3058 | Acc : 0.8594
     Batch 135 | Loss : 0.3067 | Acc : 0.8593
     Batch 140 | Loss : 0.2511 | Acc : 0.8895
     Batch 145 | Loss : 0.3167 | Acc : 0.8539
     Batch 150 | Loss : 0.2916 | Acc : 0.8629
Epoch 00148 | Train Loss : 0.2902 | Eval Loss : 0.3172 | Train acc : 0.8665 | Eval Acc : 0.8530 | Eval Log. Respected : 0.9477
     Batch 000 | Loss : 0.2618 | Acc : 0.8812
     Batch 005 | Loss : 0.2877 | Acc : 0.8690
     Batch 010 | Loss : 0.2713 | Acc : 0.8799
     Batch 015 | Loss : 0.2714 | Acc : 0.8735
     Batch 020 | Loss : 0.3285 | Acc : 0.8450
     Batch 025 | Loss : 0.3199 | Acc : 0.8560
     Batch 030 | Loss : 0.2800 | Acc : 0.8732
     Batch 035 | Loss : 0.2942 | Acc : 0.8645
     Batch 040 | Loss : 0.2896 | Acc : 0.8647
     Batch 045 | Loss : 0.2675 | Acc : 0.8819
     Batch 050 | Loss : 0.3134 | Acc : 0.8513
     Batch 055 | Loss : 0.2353 | Acc : 0.8985
     Batch 060 | Loss : 0.2903 | Acc : 0.8653
     Batch 065 | Loss : 0.2985 | Acc : 0.8589
     Batch 070 | Loss : 0.2855 | Acc : 0.8669
     Batch 075 | Loss : 0.2955 | Acc : 0.8661
     Batch 080 | Loss : 0.2691 | Acc : 0.8750
     Batch 085 | Loss : 0.3147 | Acc : 0.8527
     Batch 090 | Loss : 0.3791 | Acc : 0.8259
     Batch 095 | Loss : 0.2580 | Acc : 0.8839
     Batch 100 | Loss : 0.2755 | Acc : 0.8749
     Batch 105 | Loss : 0.2717 | Acc : 0.8791
     Batch 110 | Loss : 0.2668 | Acc : 0.8785
     Batch 115 | Loss : 0.2664 | Acc : 0.8810
     Batch 120 | Loss : 0.2870 | Acc : 0.8641
     Batch 125 | Loss : 0.3479 | Acc : 0.8310
     Batch 130 | Loss : 0.3132 | Acc : 0.8554
     Batch 135 | Loss : 0.2575 | Acc : 0.8813
     Batch 140 | Loss : 0.2792 | Acc : 0.8766
     Batch 145 | Loss : 0.2742 | Acc : 0.8758
     Batch 150 | Loss : 0.3152 | Acc : 0.8552
Epoch 00149 | Train Loss : 0.2899 | Eval Loss : 0.3209 | Train acc : 0.8665 | Eval Acc : 0.8514 | Eval Log. Respected : 0.9361
     Batch 000 | Loss : 0.2749 | Acc : 0.8755
     Batch 005 | Loss : 0.2702 | Acc : 0.8740
     Batch 010 | Loss : 0.2720 | Acc : 0.8742
     Batch 015 | Loss : 0.2663 | Acc : 0.8791
     Batch 020 | Loss : 0.2765 | Acc : 0.8713
     Batch 025 | Loss : 0.3217 | Acc : 0.8482
     Batch 030 | Loss : 0.2985 | Acc : 0.8598
     Batch 035 | Loss : 0.2449 | Acc : 0.8922
     Batch 040 | Loss : 0.2474 | Acc : 0.8869
     Batch 045 | Loss : 0.3366 | Acc : 0.8396
     Batch 050 | Loss : 0.2933 | Acc : 0.8632
     Batch 055 | Loss : 0.3304 | Acc : 0.8443
     Batch 060 | Loss : 0.3301 | Acc : 0.8466
     Batch 065 | Loss : 0.3043 | Acc : 0.8567
     Batch 070 | Loss : 0.2967 | Acc : 0.8673
     Batch 075 | Loss : 0.2546 | Acc : 0.8884
     Batch 080 | Loss : 0.2523 | Acc : 0.8838
     Batch 085 | Loss : 0.2660 | Acc : 0.8775
     Batch 090 | Loss : 0.2499 | Acc : 0.8927
     Batch 095 | Loss : 0.2663 | Acc : 0.8746
     Batch 100 | Loss : 0.3151 | Acc : 0.8564
     Batch 105 | Loss : 0.2433 | Acc : 0.8888
     Batch 110 | Loss : 0.2894 | Acc : 0.8654
     Batch 115 | Loss : 0.2948 | Acc : 0.8631
     Batch 120 | Loss : 0.2924 | Acc : 0.8663
     Batch 125 | Loss : 0.2827 | Acc : 0.8721
     Batch 130 | Loss : 0.3136 | Acc : 0.8568
     Batch 135 | Loss : 0.3117 | Acc : 0.8540
     Batch 140 | Loss : 0.2817 | Acc : 0.8718
     Batch 145 | Loss : 0.3161 | Acc : 0.8523
     Batch 150 | Loss : 0.2679 | Acc : 0.8792
Epoch 00150 | Train Loss : 0.2901 | Eval Loss : 0.3207 | Train acc : 0.8666 | Eval Acc : 0.8532 | Eval Log. Respected : 0.9353
     Batch 000 | Loss : 0.2372 | Acc : 0.8942
     Batch 005 | Loss : 0.3135 | Acc : 0.8545
     Batch 010 | Loss : 0.2674 | Acc : 0.8803
     Batch 015 | Loss : 0.2851 | Acc : 0.8679
     Batch 020 | Loss : 0.3101 | Acc : 0.8581
     Batch 025 | Loss : 0.2438 | Acc : 0.8884
     Batch 030 | Loss : 0.3171 | Acc : 0.8570
     Batch 035 | Loss : 0.3154 | Acc : 0.8514
     Batch 040 | Loss : 0.2973 | Acc : 0.8599
     Batch 045 | Loss : 0.2592 | Acc : 0.8817
     Batch 050 | Loss : 0.3369 | Acc : 0.8453
     Batch 055 | Loss : 0.3122 | Acc : 0.8606
     Batch 060 | Loss : 0.3271 | Acc : 0.8460
     Batch 065 | Loss : 0.2702 | Acc : 0.8772
     Batch 070 | Loss : 0.3228 | Acc : 0.8494
     Batch 075 | Loss : 0.2669 | Acc : 0.8783
     Batch 080 | Loss : 0.2559 | Acc : 0.8837
     Batch 085 | Loss : 0.2897 | Acc : 0.8647
     Batch 090 | Loss : 0.2614 | Acc : 0.8787
     Batch 095 | Loss : 0.2904 | Acc : 0.8645
     Batch 100 | Loss : 0.2817 | Acc : 0.8725
     Batch 105 | Loss : 0.2695 | Acc : 0.8775
     Batch 110 | Loss : 0.2922 | Acc : 0.8678
     Batch 115 | Loss : 0.2742 | Acc : 0.8740
     Batch 120 | Loss : 0.2765 | Acc : 0.8711
     Batch 125 | Loss : 0.2591 | Acc : 0.8799
     Batch 130 | Loss : 0.2940 | Acc : 0.8660
     Batch 135 | Loss : 0.2896 | Acc : 0.8674
     Batch 140 | Loss : 0.2833 | Acc : 0.8678
     Batch 145 | Loss : 0.3234 | Acc : 0.8473
     Batch 150 | Loss : 0.2984 | Acc : 0.8598
Epoch 00151 | Train Loss : 0.2910 | Eval Loss : 0.3152 | Train acc : 0.8662 | Eval Acc : 0.8540 | Eval Log. Respected : 0.9435
     Batch 000 | Loss : 0.2944 | Acc : 0.8636
     Batch 005 | Loss : 0.2580 | Acc : 0.8853
     Batch 010 | Loss : 0.2625 | Acc : 0.8803
     Batch 015 | Loss : 0.3913 | Acc : 0.8270
     Batch 020 | Loss : 0.2859 | Acc : 0.8686
     Batch 025 | Loss : 0.3137 | Acc : 0.8536
     Batch 030 | Loss : 0.3070 | Acc : 0.8585
     Batch 035 | Loss : 0.3259 | Acc : 0.8467
     Batch 040 | Loss : 0.3102 | Acc : 0.8528
     Batch 045 | Loss : 0.2763 | Acc : 0.8732
     Batch 050 | Loss : 0.2602 | Acc : 0.8833
     Batch 055 | Loss : 0.2799 | Acc : 0.8711
     Batch 060 | Loss : 0.2752 | Acc : 0.8724
     Batch 065 | Loss : 0.2540 | Acc : 0.8865
     Batch 070 | Loss : 0.2747 | Acc : 0.8771
     Batch 075 | Loss : 0.2875 | Acc : 0.8665
     Batch 080 | Loss : 0.3029 | Acc : 0.8579
     Batch 085 | Loss : 0.2743 | Acc : 0.8773
     Batch 090 | Loss : 0.2982 | Acc : 0.8610
     Batch 095 | Loss : 0.2562 | Acc : 0.8836
     Batch 100 | Loss : 0.2763 | Acc : 0.8709
     Batch 105 | Loss : 0.3799 | Acc : 0.8236
     Batch 110 | Loss : 0.2883 | Acc : 0.8716
     Batch 115 | Loss : 0.2842 | Acc : 0.8690
     Batch 120 | Loss : 0.3444 | Acc : 0.8391
     Batch 125 | Loss : 0.2685 | Acc : 0.8747
     Batch 130 | Loss : 0.2690 | Acc : 0.8770
     Batch 135 | Loss : 0.2703 | Acc : 0.8765
     Batch 140 | Loss : 0.2417 | Acc : 0.8920
     Batch 145 | Loss : 0.3567 | Acc : 0.8290
     Batch 150 | Loss : 0.3098 | Acc : 0.8564
Epoch 00152 | Train Loss : 0.2894 | Eval Loss : 0.3183 | Train acc : 0.8670 | Eval Acc : 0.8522 | Eval Log. Respected : 0.9443
     Batch 000 | Loss : 0.2555 | Acc : 0.8856
     Batch 005 | Loss : 0.3006 | Acc : 0.8585
     Batch 010 | Loss : 0.2855 | Acc : 0.8699
     Batch 015 | Loss : 0.2870 | Acc : 0.8675
     Batch 020 | Loss : 0.2463 | Acc : 0.8908
     Batch 025 | Loss : 0.3779 | Acc : 0.8331
     Batch 030 | Loss : 0.3566 | Acc : 0.8305
     Batch 035 | Loss : 0.2711 | Acc : 0.8792
     Batch 040 | Loss : 0.2690 | Acc : 0.8779
     Batch 045 | Loss : 0.2787 | Acc : 0.8702
     Batch 050 | Loss : 0.2577 | Acc : 0.8806
     Batch 055 | Loss : 0.2880 | Acc : 0.8654
     Batch 060 | Loss : 0.2957 | Acc : 0.8635
     Batch 065 | Loss : 0.2976 | Acc : 0.8571
     Batch 070 | Loss : 0.3125 | Acc : 0.8558
     Batch 075 | Loss : 0.3276 | Acc : 0.8439
     Batch 080 | Loss : 0.3004 | Acc : 0.8610
     Batch 085 | Loss : 0.2701 | Acc : 0.8753
     Batch 090 | Loss : 0.3372 | Acc : 0.8451
     Batch 095 | Loss : 0.2896 | Acc : 0.8656
     Batch 100 | Loss : 0.2724 | Acc : 0.8757
     Batch 105 | Loss : 0.3033 | Acc : 0.8618
     Batch 110 | Loss : 0.2995 | Acc : 0.8598
     Batch 115 | Loss : 0.2645 | Acc : 0.8781
     Batch 120 | Loss : 0.3174 | Acc : 0.8519
     Batch 125 | Loss : 0.2532 | Acc : 0.8849
     Batch 130 | Loss : 0.3212 | Acc : 0.8560
     Batch 135 | Loss : 0.2805 | Acc : 0.8711
     Batch 140 | Loss : 0.3491 | Acc : 0.8381
     Batch 145 | Loss : 0.3158 | Acc : 0.8527
     Batch 150 | Loss : 0.2370 | Acc : 0.8980
Epoch 00153 | Train Loss : 0.2892 | Eval Loss : 0.3223 | Train acc : 0.8670 | Eval Acc : 0.8533 | Eval Log. Respected : 0.9398
     Batch 000 | Loss : 0.3558 | Acc : 0.8345
     Batch 005 | Loss : 0.2592 | Acc : 0.8830
     Batch 010 | Loss : 0.2823 | Acc : 0.8706
     Batch 015 | Loss : 0.3054 | Acc : 0.8589
     Batch 020 | Loss : 0.2668 | Acc : 0.8810
     Batch 025 | Loss : 0.2420 | Acc : 0.8903
     Batch 030 | Loss : 0.3338 | Acc : 0.8420
     Batch 035 | Loss : 0.2819 | Acc : 0.8717
     Batch 040 | Loss : 0.2821 | Acc : 0.8728
     Batch 045 | Loss : 0.2680 | Acc : 0.8761
     Batch 050 | Loss : 0.3088 | Acc : 0.8535
     Batch 055 | Loss : 0.3014 | Acc : 0.8600
     Batch 060 | Loss : 0.2573 | Acc : 0.8833
     Batch 065 | Loss : 0.3036 | Acc : 0.8576
     Batch 070 | Loss : 0.2770 | Acc : 0.8688
     Batch 075 | Loss : 0.3696 | Acc : 0.8268
     Batch 080 | Loss : 0.2732 | Acc : 0.8769
     Batch 085 | Loss : 0.2607 | Acc : 0.8792
     Batch 090 | Loss : 0.2659 | Acc : 0.8789
     Batch 095 | Loss : 0.3184 | Acc : 0.8529
     Batch 100 | Loss : 0.3100 | Acc : 0.8560
     Batch 105 | Loss : 0.3321 | Acc : 0.8412
     Batch 110 | Loss : 0.2869 | Acc : 0.8676
     Batch 115 | Loss : 0.2804 | Acc : 0.8735
     Batch 120 | Loss : 0.3597 | Acc : 0.8368
     Batch 125 | Loss : 0.2425 | Acc : 0.8920
     Batch 130 | Loss : 0.2924 | Acc : 0.8657
     Batch 135 | Loss : 0.2832 | Acc : 0.8699
     Batch 140 | Loss : 0.2164 | Acc : 0.9055
     Batch 145 | Loss : 0.2643 | Acc : 0.8793
     Batch 150 | Loss : 0.2984 | Acc : 0.8630
Epoch 00154 | Train Loss : 0.2892 | Eval Loss : 0.3196 | Train acc : 0.8668 | Eval Acc : 0.8516 | Eval Log. Respected : 0.9368
     Batch 000 | Loss : 0.2825 | Acc : 0.8707
     Batch 005 | Loss : 0.2740 | Acc : 0.8746
     Batch 010 | Loss : 0.3140 | Acc : 0.8570
     Batch 015 | Loss : 0.2888 | Acc : 0.8673
     Batch 020 | Loss : 0.2690 | Acc : 0.8768
     Batch 025 | Loss : 0.2880 | Acc : 0.8663
     Batch 030 | Loss : 0.2594 | Acc : 0.8808
     Batch 035 | Loss : 0.3212 | Acc : 0.8507
     Batch 040 | Loss : 0.2708 | Acc : 0.8776
     Batch 045 | Loss : 0.4154 | Acc : 0.8096
     Batch 050 | Loss : 0.2968 | Acc : 0.8612
     Batch 055 | Loss : 0.2833 | Acc : 0.8708
     Batch 060 | Loss : 0.3141 | Acc : 0.8534
     Batch 065 | Loss : 0.3279 | Acc : 0.8462
     Batch 070 | Loss : 0.2944 | Acc : 0.8641
     Batch 075 | Loss : 0.2922 | Acc : 0.8661
     Batch 080 | Loss : 0.3304 | Acc : 0.8441
     Batch 085 | Loss : 0.2877 | Acc : 0.8698
     Batch 090 | Loss : 0.3179 | Acc : 0.8543
     Batch 095 | Loss : 0.2527 | Acc : 0.8859
     Batch 100 | Loss : 0.2509 | Acc : 0.8846
     Batch 105 | Loss : 0.2788 | Acc : 0.8722
     Batch 110 | Loss : 0.2960 | Acc : 0.8659
     Batch 115 | Loss : 0.3142 | Acc : 0.8550
     Batch 120 | Loss : 0.2813 | Acc : 0.8667
     Batch 125 | Loss : 0.2958 | Acc : 0.8636
     Batch 130 | Loss : 0.2974 | Acc : 0.8631
     Batch 135 | Loss : 0.2797 | Acc : 0.8754
     Batch 140 | Loss : 0.2667 | Acc : 0.8776
     Batch 145 | Loss : 0.3172 | Acc : 0.8531
     Batch 150 | Loss : 0.2558 | Acc : 0.8867
Epoch 00155 | Train Loss : 0.2886 | Eval Loss : 0.3214 | Train acc : 0.8670 | Eval Acc : 0.8539 | Eval Log. Respected : 0.9345
     Batch 000 | Loss : 0.3008 | Acc : 0.8602
     Batch 005 | Loss : 0.3566 | Acc : 0.8352
     Batch 010 | Loss : 0.2697 | Acc : 0.8794
     Batch 015 | Loss : 0.2451 | Acc : 0.8894
     Batch 020 | Loss : 0.3016 | Acc : 0.8588
     Batch 025 | Loss : 0.2684 | Acc : 0.8776
     Batch 030 | Loss : 0.3025 | Acc : 0.8597
     Batch 035 | Loss : 0.3135 | Acc : 0.8547
     Batch 040 | Loss : 0.3250 | Acc : 0.8497
     Batch 045 | Loss : 0.2910 | Acc : 0.8679
     Batch 050 | Loss : 0.2745 | Acc : 0.8740
     Batch 055 | Loss : 0.3155 | Acc : 0.8539
     Batch 060 | Loss : 0.2970 | Acc : 0.8613
     Batch 065 | Loss : 0.2915 | Acc : 0.8675
     Batch 070 | Loss : 0.2931 | Acc : 0.8639
     Batch 075 | Loss : 0.2670 | Acc : 0.8778
     Batch 080 | Loss : 0.3229 | Acc : 0.8509
     Batch 085 | Loss : 0.2554 | Acc : 0.8858
     Batch 090 | Loss : 0.2684 | Acc : 0.8766
     Batch 095 | Loss : 0.3137 | Acc : 0.8592
     Batch 100 | Loss : 0.2819 | Acc : 0.8676
     Batch 105 | Loss : 0.3111 | Acc : 0.8562
     Batch 110 | Loss : 0.2553 | Acc : 0.8810
     Batch 115 | Loss : 0.2699 | Acc : 0.8739
     Batch 120 | Loss : 0.2871 | Acc : 0.8696
     Batch 125 | Loss : 0.3093 | Acc : 0.8587
     Batch 130 | Loss : 0.2699 | Acc : 0.8758
     Batch 135 | Loss : 0.2796 | Acc : 0.8724
     Batch 140 | Loss : 0.2471 | Acc : 0.8866
     Batch 145 | Loss : 0.2762 | Acc : 0.8713
     Batch 150 | Loss : 0.2783 | Acc : 0.8710
Epoch 00156 | Train Loss : 0.2886 | Eval Loss : 0.3226 | Train acc : 0.8674 | Eval Acc : 0.8526 | Eval Log. Respected : 0.9392
     Batch 000 | Loss : 0.2879 | Acc : 0.8688
     Batch 005 | Loss : 0.2604 | Acc : 0.8818
     Batch 010 | Loss : 0.3440 | Acc : 0.8390
     Batch 015 | Loss : 0.2712 | Acc : 0.8762
     Batch 020 | Loss : 0.2862 | Acc : 0.8643
     Batch 025 | Loss : 0.2861 | Acc : 0.8684
     Batch 030 | Loss : 0.2859 | Acc : 0.8708
     Batch 035 | Loss : 0.2777 | Acc : 0.8712
     Batch 040 | Loss : 0.3284 | Acc : 0.8442
     Batch 045 | Loss : 0.3040 | Acc : 0.8566
     Batch 050 | Loss : 0.2681 | Acc : 0.8834
     Batch 055 | Loss : 0.2454 | Acc : 0.8883
     Batch 060 | Loss : 0.2612 | Acc : 0.8790
     Batch 065 | Loss : 0.2794 | Acc : 0.8712
     Batch 070 | Loss : 0.2882 | Acc : 0.8699
     Batch 075 | Loss : 0.2823 | Acc : 0.8721
     Batch 080 | Loss : 0.3069 | Acc : 0.8603
     Batch 085 | Loss : 0.3038 | Acc : 0.8586
     Batch 090 | Loss : 0.3209 | Acc : 0.8494
     Batch 095 | Loss : 0.3066 | Acc : 0.8572
     Batch 100 | Loss : 0.2811 | Acc : 0.8689
     Batch 105 | Loss : 0.2527 | Acc : 0.8860
     Batch 110 | Loss : 0.2927 | Acc : 0.8652
     Batch 115 | Loss : 0.2928 | Acc : 0.8645
     Batch 120 | Loss : 0.2441 | Acc : 0.8881
     Batch 125 | Loss : 0.2774 | Acc : 0.8742
     Batch 130 | Loss : 0.2826 | Acc : 0.8713
     Batch 135 | Loss : 0.2511 | Acc : 0.8874
     Batch 140 | Loss : 0.3145 | Acc : 0.8545
     Batch 145 | Loss : 0.2501 | Acc : 0.8887
     Batch 150 | Loss : 0.2801 | Acc : 0.8704
Epoch 00157 | Train Loss : 0.2884 | Eval Loss : 0.3190 | Train acc : 0.8674 | Eval Acc : 0.8525 | Eval Log. Respected : 0.9335
     Batch 000 | Loss : 0.2940 | Acc : 0.8625
     Batch 005 | Loss : 0.2712 | Acc : 0.8750
     Batch 010 | Loss : 0.2887 | Acc : 0.8688
     Batch 015 | Loss : 0.2917 | Acc : 0.8657
     Batch 020 | Loss : 0.2867 | Acc : 0.8685
     Batch 025 | Loss : 0.2649 | Acc : 0.8759
     Batch 030 | Loss : 0.2530 | Acc : 0.8876
     Batch 035 | Loss : 0.2732 | Acc : 0.8770
     Batch 040 | Loss : 0.2750 | Acc : 0.8704
     Batch 045 | Loss : 0.2831 | Acc : 0.8745
     Batch 050 | Loss : 0.2737 | Acc : 0.8743
     Batch 055 | Loss : 0.2828 | Acc : 0.8686
     Batch 060 | Loss : 0.3245 | Acc : 0.8512
     Batch 065 | Loss : 0.2841 | Acc : 0.8695
     Batch 070 | Loss : 0.2745 | Acc : 0.8717
     Batch 075 | Loss : 0.2601 | Acc : 0.8806
     Batch 080 | Loss : 0.2703 | Acc : 0.8811
     Batch 085 | Loss : 0.2774 | Acc : 0.8750
     Batch 090 | Loss : 0.3120 | Acc : 0.8557
     Batch 095 | Loss : 0.2796 | Acc : 0.8757
     Batch 100 | Loss : 0.2491 | Acc : 0.8879
     Batch 105 | Loss : 0.2875 | Acc : 0.8681
     Batch 110 | Loss : 0.2740 | Acc : 0.8752
     Batch 115 | Loss : 0.3054 | Acc : 0.8583
     Batch 120 | Loss : 0.2427 | Acc : 0.8909
     Batch 125 | Loss : 0.3355 | Acc : 0.8419
     Batch 130 | Loss : 0.3107 | Acc : 0.8527
     Batch 135 | Loss : 0.3371 | Acc : 0.8440
     Batch 140 | Loss : 0.2726 | Acc : 0.8756
     Batch 145 | Loss : 0.3454 | Acc : 0.8419
     Batch 150 | Loss : 0.2636 | Acc : 0.8811
Epoch 00158 | Train Loss : 0.2884 | Eval Loss : 0.3163 | Train acc : 0.8674 | Eval Acc : 0.8536 | Eval Log. Respected : 0.9289
     Batch 000 | Loss : 0.2889 | Acc : 0.8696
     Batch 005 | Loss : 0.3054 | Acc : 0.8585
     Batch 010 | Loss : 0.2655 | Acc : 0.8783
     Batch 015 | Loss : 0.3294 | Acc : 0.8483
     Batch 020 | Loss : 0.2835 | Acc : 0.8719
     Batch 025 | Loss : 0.2969 | Acc : 0.8644
     Batch 030 | Loss : 0.2783 | Acc : 0.8731
     Batch 035 | Loss : 0.2689 | Acc : 0.8776
     Batch 040 | Loss : 0.3041 | Acc : 0.8551
     Batch 045 | Loss : 0.2882 | Acc : 0.8661
     Batch 050 | Loss : 0.3130 | Acc : 0.8497
     Batch 055 | Loss : 0.2625 | Acc : 0.8870
     Batch 060 | Loss : 0.2748 | Acc : 0.8751
     Batch 065 | Loss : 0.2757 | Acc : 0.8715
     Batch 070 | Loss : 0.2483 | Acc : 0.8862
     Batch 075 | Loss : 0.3179 | Acc : 0.8468
     Batch 080 | Loss : 0.3022 | Acc : 0.8625
     Batch 085 | Loss : 0.2974 | Acc : 0.8602
     Batch 090 | Loss : 0.3127 | Acc : 0.8515
     Batch 095 | Loss : 0.2765 | Acc : 0.8704
     Batch 100 | Loss : 0.3138 | Acc : 0.8566
     Batch 105 | Loss : 0.3272 | Acc : 0.8517
     Batch 110 | Loss : 0.2509 | Acc : 0.8882
     Batch 115 | Loss : 0.2782 | Acc : 0.8740
     Batch 120 | Loss : 0.2714 | Acc : 0.8749
     Batch 125 | Loss : 0.2488 | Acc : 0.8864
     Batch 130 | Loss : 0.2657 | Acc : 0.8786
     Batch 135 | Loss : 0.3195 | Acc : 0.8497
     Batch 140 | Loss : 0.2496 | Acc : 0.8862
     Batch 145 | Loss : 0.3509 | Acc : 0.8286
     Batch 150 | Loss : 0.2680 | Acc : 0.8785
Epoch 00159 | Train Loss : 0.2890 | Eval Loss : 0.3159 | Train acc : 0.8673 | Eval Acc : 0.8539 | Eval Log. Respected : 0.9327
     Batch 000 | Loss : 0.3249 | Acc : 0.8480
     Batch 005 | Loss : 0.3201 | Acc : 0.8507
     Batch 010 | Loss : 0.2927 | Acc : 0.8652
     Batch 015 | Loss : 0.2855 | Acc : 0.8695
     Batch 020 | Loss : 0.2427 | Acc : 0.8916
     Batch 025 | Loss : 0.3102 | Acc : 0.8552
     Batch 030 | Loss : 0.2749 | Acc : 0.8730
     Batch 035 | Loss : 0.2676 | Acc : 0.8776
     Batch 040 | Loss : 0.2924 | Acc : 0.8654
     Batch 045 | Loss : 0.3218 | Acc : 0.8498
     Batch 050 | Loss : 0.3179 | Acc : 0.8511
     Batch 055 | Loss : 0.2735 | Acc : 0.8727
     Batch 060 | Loss : 0.2643 | Acc : 0.8796
     Batch 065 | Loss : 0.2913 | Acc : 0.8670
     Batch 070 | Loss : 0.2822 | Acc : 0.8689
     Batch 075 | Loss : 0.3625 | Acc : 0.8370
     Batch 080 | Loss : 0.3564 | Acc : 0.8318
     Batch 085 | Loss : 0.2794 | Acc : 0.8745
     Batch 090 | Loss : 0.2491 | Acc : 0.8878
     Batch 095 | Loss : 0.3109 | Acc : 0.8563
     Batch 100 | Loss : 0.2499 | Acc : 0.8859
     Batch 105 | Loss : 0.3171 | Acc : 0.8557
     Batch 110 | Loss : 0.3099 | Acc : 0.8614
     Batch 115 | Loss : 0.2564 | Acc : 0.8853
     Batch 120 | Loss : 0.2398 | Acc : 0.8923
     Batch 125 | Loss : 0.2538 | Acc : 0.8845
     Batch 130 | Loss : 0.2981 | Acc : 0.8644
     Batch 135 | Loss : 0.2778 | Acc : 0.8723
     Batch 140 | Loss : 0.2894 | Acc : 0.8682
     Batch 145 | Loss : 0.3020 | Acc : 0.8602
     Batch 150 | Loss : 0.2764 | Acc : 0.8708
Epoch 00160 | Train Loss : 0.2893 | Eval Loss : 0.3184 | Train acc : 0.8668 | Eval Acc : 0.8525 | Eval Log. Respected : 0.9413
     Batch 000 | Loss : 0.2942 | Acc : 0.8633
     Batch 005 | Loss : 0.2969 | Acc : 0.8624
     Batch 010 | Loss : 0.2887 | Acc : 0.8649
     Batch 015 | Loss : 0.2725 | Acc : 0.8708
     Batch 020 | Loss : 0.3107 | Acc : 0.8533
     Batch 025 | Loss : 0.3883 | Acc : 0.8225
     Batch 030 | Loss : 0.2893 | Acc : 0.8666
     Batch 035 | Loss : 0.3096 | Acc : 0.8562
     Batch 040 | Loss : 0.2915 | Acc : 0.8670
     Batch 045 | Loss : 0.3120 | Acc : 0.8533
     Batch 050 | Loss : 0.3276 | Acc : 0.8453
     Batch 055 | Loss : 0.2646 | Acc : 0.8806
     Batch 060 | Loss : 0.2682 | Acc : 0.8798
     Batch 065 | Loss : 0.2902 | Acc : 0.8674
     Batch 070 | Loss : 0.2818 | Acc : 0.8637
     Batch 075 | Loss : 0.2424 | Acc : 0.8925
     Batch 080 | Loss : 0.2642 | Acc : 0.8792
     Batch 085 | Loss : 0.2326 | Acc : 0.8931
     Batch 090 | Loss : 0.3128 | Acc : 0.8590
     Batch 095 | Loss : 0.2960 | Acc : 0.8632
     Batch 100 | Loss : 0.2561 | Acc : 0.8860
     Batch 105 | Loss : 0.3161 | Acc : 0.8582
     Batch 110 | Loss : 0.2960 | Acc : 0.8623
     Batch 115 | Loss : 0.2407 | Acc : 0.8968
     Batch 120 | Loss : 0.2889 | Acc : 0.8648
     Batch 125 | Loss : 0.2644 | Acc : 0.8775
     Batch 130 | Loss : 0.3382 | Acc : 0.8437
     Batch 135 | Loss : 0.2813 | Acc : 0.8705
     Batch 140 | Loss : 0.2507 | Acc : 0.8856
     Batch 145 | Loss : 0.2876 | Acc : 0.8697
     Batch 150 | Loss : 0.2753 | Acc : 0.8751
Epoch 00161 | Train Loss : 0.2878 | Eval Loss : 0.3157 | Train acc : 0.8677 | Eval Acc : 0.8539 | Eval Log. Respected : 0.9395
     Batch 000 | Loss : 0.2841 | Acc : 0.8688
     Batch 005 | Loss : 0.2819 | Acc : 0.8714
     Batch 010 | Loss : 0.3088 | Acc : 0.8600
     Batch 015 | Loss : 0.2850 | Acc : 0.8673
     Batch 020 | Loss : 0.2484 | Acc : 0.8873
     Batch 025 | Loss : 0.2636 | Acc : 0.8766
     Batch 030 | Loss : 0.2906 | Acc : 0.8680
     Batch 035 | Loss : 0.2731 | Acc : 0.8741
     Batch 040 | Loss : 0.3462 | Acc : 0.8394
     Batch 045 | Loss : 0.3050 | Acc : 0.8604
     Batch 050 | Loss : 0.2674 | Acc : 0.8769
     Batch 055 | Loss : 0.2976 | Acc : 0.8626
     Batch 060 | Loss : 0.2559 | Acc : 0.8828
     Batch 065 | Loss : 0.2691 | Acc : 0.8748
     Batch 070 | Loss : 0.3207 | Acc : 0.8492
     Batch 075 | Loss : 0.3040 | Acc : 0.8557
     Batch 080 | Loss : 0.2979 | Acc : 0.8649
     Batch 085 | Loss : 0.3024 | Acc : 0.8613
     Batch 090 | Loss : 0.2355 | Acc : 0.8997
     Batch 095 | Loss : 0.2991 | Acc : 0.8622
     Batch 100 | Loss : 0.2493 | Acc : 0.8894
     Batch 105 | Loss : 0.3292 | Acc : 0.8486
     Batch 110 | Loss : 0.2982 | Acc : 0.8606
     Batch 115 | Loss : 0.3214 | Acc : 0.8491
     Batch 120 | Loss : 0.2819 | Acc : 0.8694
     Batch 125 | Loss : 0.3361 | Acc : 0.8488
     Batch 130 | Loss : 0.2676 | Acc : 0.8769
     Batch 135 | Loss : 0.2644 | Acc : 0.8813
     Batch 140 | Loss : 0.2994 | Acc : 0.8646
     Batch 145 | Loss : 0.2734 | Acc : 0.8761
     Batch 150 | Loss : 0.2620 | Acc : 0.8841
Epoch 00162 | Train Loss : 0.2877 | Eval Loss : 0.3162 | Train acc : 0.8678 | Eval Acc : 0.8540 | Eval Log. Respected : 0.9404
     Batch 000 | Loss : 0.3075 | Acc : 0.8573
     Batch 005 | Loss : 0.2568 | Acc : 0.8829
     Batch 010 | Loss : 0.2786 | Acc : 0.8739
     Batch 015 | Loss : 0.2488 | Acc : 0.8837
     Batch 020 | Loss : 0.2695 | Acc : 0.8786
     Batch 025 | Loss : 0.2711 | Acc : 0.8773
     Batch 030 | Loss : 0.2432 | Acc : 0.8900
     Batch 035 | Loss : 0.3150 | Acc : 0.8573
     Batch 040 | Loss : 0.2608 | Acc : 0.8810
     Batch 045 | Loss : 0.2818 | Acc : 0.8701
     Batch 050 | Loss : 0.3105 | Acc : 0.8571
     Batch 055 | Loss : 0.3163 | Acc : 0.8502
     Batch 060 | Loss : 0.3042 | Acc : 0.8599
     Batch 065 | Loss : 0.2833 | Acc : 0.8702
     Batch 070 | Loss : 0.2700 | Acc : 0.8757
     Batch 075 | Loss : 0.2603 | Acc : 0.8807
     Batch 080 | Loss : 0.2568 | Acc : 0.8849
     Batch 085 | Loss : 0.2857 | Acc : 0.8647
     Batch 090 | Loss : 0.3012 | Acc : 0.8613
     Batch 095 | Loss : 0.3018 | Acc : 0.8618
     Batch 100 | Loss : 0.2998 | Acc : 0.8607
     Batch 105 | Loss : 0.3257 | Acc : 0.8490
     Batch 110 | Loss : 0.2530 | Acc : 0.8861
     Batch 115 | Loss : 0.4070 | Acc : 0.8105
     Batch 120 | Loss : 0.2889 | Acc : 0.8700
     Batch 125 | Loss : 0.2916 | Acc : 0.8653
     Batch 130 | Loss : 0.2553 | Acc : 0.8859
     Batch 135 | Loss : 0.2928 | Acc : 0.8631
     Batch 140 | Loss : 0.3512 | Acc : 0.8315
     Batch 145 | Loss : 0.2967 | Acc : 0.8618
     Batch 150 | Loss : 0.2754 | Acc : 0.8713
Epoch 00163 | Train Loss : 0.2883 | Eval Loss : 0.3216 | Train acc : 0.8674 | Eval Acc : 0.8511 | Eval Log. Respected : 0.9456
     Batch 000 | Loss : 0.2395 | Acc : 0.8954
     Batch 005 | Loss : 0.2702 | Acc : 0.8794
     Batch 010 | Loss : 0.2912 | Acc : 0.8660
     Batch 015 | Loss : 0.2556 | Acc : 0.8828
     Batch 020 | Loss : 0.2498 | Acc : 0.8866
     Batch 025 | Loss : 0.2560 | Acc : 0.8805
     Batch 030 | Loss : 0.3002 | Acc : 0.8595
     Batch 035 | Loss : 0.2684 | Acc : 0.8787
     Batch 040 | Loss : 0.2628 | Acc : 0.8783
     Batch 045 | Loss : 0.2644 | Acc : 0.8825
     Batch 050 | Loss : 0.2611 | Acc : 0.8819
     Batch 055 | Loss : 0.2623 | Acc : 0.8803
     Batch 060 | Loss : 0.3247 | Acc : 0.8509
     Batch 065 | Loss : 0.2450 | Acc : 0.8908
     Batch 070 | Loss : 0.2800 | Acc : 0.8681
     Batch 075 | Loss : 0.3300 | Acc : 0.8453
     Batch 080 | Loss : 0.2629 | Acc : 0.8827
     Batch 085 | Loss : 0.3702 | Acc : 0.8372
     Batch 090 | Loss : 0.2570 | Acc : 0.8860
     Batch 095 | Loss : 0.2894 | Acc : 0.8685
     Batch 100 | Loss : 0.2979 | Acc : 0.8615
     Batch 105 | Loss : 0.2986 | Acc : 0.8639
     Batch 110 | Loss : 0.3006 | Acc : 0.8591
     Batch 115 | Loss : 0.3042 | Acc : 0.8584
     Batch 120 | Loss : 0.2616 | Acc : 0.8860
     Batch 125 | Loss : 0.3031 | Acc : 0.8570
     Batch 130 | Loss : 0.2813 | Acc : 0.8721
     Batch 135 | Loss : 0.2911 | Acc : 0.8658
     Batch 140 | Loss : 0.2949 | Acc : 0.8623
     Batch 145 | Loss : 0.3158 | Acc : 0.8575
     Batch 150 | Loss : 0.3322 | Acc : 0.8453
Epoch 00164 | Train Loss : 0.2869 | Eval Loss : 0.3207 | Train acc : 0.8680 | Eval Acc : 0.8510 | Eval Log. Respected : 0.9301
     Batch 000 | Loss : 0.3013 | Acc : 0.8579
     Batch 005 | Loss : 0.2640 | Acc : 0.8818
     Batch 010 | Loss : 0.2741 | Acc : 0.8738
     Batch 015 | Loss : 0.2949 | Acc : 0.8621
     Batch 020 | Loss : 0.3002 | Acc : 0.8641
     Batch 025 | Loss : 0.3186 | Acc : 0.8519
     Batch 030 | Loss : 0.2580 | Acc : 0.8833
     Batch 035 | Loss : 0.2688 | Acc : 0.8770
     Batch 040 | Loss : 0.2711 | Acc : 0.8772
     Batch 045 | Loss : 0.3318 | Acc : 0.8432
     Batch 050 | Loss : 0.2455 | Acc : 0.8901
     Batch 055 | Loss : 0.2721 | Acc : 0.8751
     Batch 060 | Loss : 0.3335 | Acc : 0.8448
     Batch 065 | Loss : 0.2854 | Acc : 0.8675
     Batch 070 | Loss : 0.3032 | Acc : 0.8615
     Batch 075 | Loss : 0.3064 | Acc : 0.8616
     Batch 080 | Loss : 0.2621 | Acc : 0.8824
     Batch 085 | Loss : 0.2560 | Acc : 0.8839
     Batch 090 | Loss : 0.2601 | Acc : 0.8809
     Batch 095 | Loss : 0.2754 | Acc : 0.8769
     Batch 100 | Loss : 0.3191 | Acc : 0.8529
     Batch 105 | Loss : 0.2603 | Acc : 0.8841
     Batch 110 | Loss : 0.2631 | Acc : 0.8820
     Batch 115 | Loss : 0.2935 | Acc : 0.8630
     Batch 120 | Loss : 0.3129 | Acc : 0.8555
     Batch 125 | Loss : 0.3274 | Acc : 0.8505
     Batch 130 | Loss : 0.2863 | Acc : 0.8688
     Batch 135 | Loss : 0.2791 | Acc : 0.8724
     Batch 140 | Loss : 0.2645 | Acc : 0.8794
     Batch 145 | Loss : 0.2785 | Acc : 0.8712
     Batch 150 | Loss : 0.3097 | Acc : 0.8560
Epoch 00165 | Train Loss : 0.2877 | Eval Loss : 0.3224 | Train acc : 0.8677 | Eval Acc : 0.8525 | Eval Log. Respected : 0.9369
     Batch 000 | Loss : 0.2521 | Acc : 0.8858
     Batch 005 | Loss : 0.2675 | Acc : 0.8785
     Batch 010 | Loss : 0.2737 | Acc : 0.8773
     Batch 015 | Loss : 0.2848 | Acc : 0.8666
     Batch 020 | Loss : 0.2959 | Acc : 0.8641
     Batch 025 | Loss : 0.2825 | Acc : 0.8711
     Batch 030 | Loss : 0.2497 | Acc : 0.8874
     Batch 035 | Loss : 0.2917 | Acc : 0.8633
     Batch 040 | Loss : 0.2973 | Acc : 0.8607
     Batch 045 | Loss : 0.2852 | Acc : 0.8707
     Batch 050 | Loss : 0.3202 | Acc : 0.8526
     Batch 055 | Loss : 0.3293 | Acc : 0.8459
     Batch 060 | Loss : 0.3015 | Acc : 0.8600
     Batch 065 | Loss : 0.2558 | Acc : 0.8852
     Batch 070 | Loss : 0.2773 | Acc : 0.8734
     Batch 075 | Loss : 0.3065 | Acc : 0.8618
     Batch 080 | Loss : 0.2781 | Acc : 0.8677
     Batch 085 | Loss : 0.3370 | Acc : 0.8409
     Batch 090 | Loss : 0.3094 | Acc : 0.8523
     Batch 095 | Loss : 0.2596 | Acc : 0.8787
     Batch 100 | Loss : 0.3679 | Acc : 0.8317
     Batch 105 | Loss : 0.3142 | Acc : 0.8513
     Batch 110 | Loss : 0.2750 | Acc : 0.8732
     Batch 115 | Loss : 0.2935 | Acc : 0.8654
     Batch 120 | Loss : 0.3088 | Acc : 0.8537
     Batch 125 | Loss : 0.3360 | Acc : 0.8449
     Batch 130 | Loss : 0.2511 | Acc : 0.8884
     Batch 135 | Loss : 0.3085 | Acc : 0.8558
     Batch 140 | Loss : 0.2883 | Acc : 0.8690
     Batch 145 | Loss : 0.3091 | Acc : 0.8585
     Batch 150 | Loss : 0.2674 | Acc : 0.8818
Epoch 00166 | Train Loss : 0.2870 | Eval Loss : 0.3182 | Train acc : 0.8679 | Eval Acc : 0.8528 | Eval Log. Respected : 0.9313
     Batch 000 | Loss : 0.3006 | Acc : 0.8595
     Batch 005 | Loss : 0.2788 | Acc : 0.8727
     Batch 010 | Loss : 0.2923 | Acc : 0.8665
     Batch 015 | Loss : 0.2792 | Acc : 0.8736
     Batch 020 | Loss : 0.3313 | Acc : 0.8459
     Batch 025 | Loss : 0.2867 | Acc : 0.8694
     Batch 030 | Loss : 0.2743 | Acc : 0.8747
     Batch 035 | Loss : 0.2250 | Acc : 0.9002
     Batch 040 | Loss : 0.2377 | Acc : 0.8909
     Batch 045 | Loss : 0.3418 | Acc : 0.8437
     Batch 050 | Loss : 0.2752 | Acc : 0.8725
     Batch 055 | Loss : 0.3386 | Acc : 0.8387
     Batch 060 | Loss : 0.2848 | Acc : 0.8658
     Batch 065 | Loss : 0.2850 | Acc : 0.8677
     Batch 070 | Loss : 0.2920 | Acc : 0.8633
     Batch 075 | Loss : 0.2886 | Acc : 0.8663
     Batch 080 | Loss : 0.2634 | Acc : 0.8797
     Batch 085 | Loss : 0.3347 | Acc : 0.8425
     Batch 090 | Loss : 0.2405 | Acc : 0.8902
     Batch 095 | Loss : 0.2893 | Acc : 0.8693
     Batch 100 | Loss : 0.2676 | Acc : 0.8774
     Batch 105 | Loss : 0.2709 | Acc : 0.8761
     Batch 110 | Loss : 0.2653 | Acc : 0.8805
     Batch 115 | Loss : 0.2670 | Acc : 0.8774
     Batch 120 | Loss : 0.2671 | Acc : 0.8777
     Batch 125 | Loss : 0.2788 | Acc : 0.8726
     Batch 130 | Loss : 0.2941 | Acc : 0.8657
     Batch 135 | Loss : 0.2607 | Acc : 0.8791
     Batch 140 | Loss : 0.3319 | Acc : 0.8422
     Batch 145 | Loss : 0.2838 | Acc : 0.8704
     Batch 150 | Loss : 0.3182 | Acc : 0.8523
Epoch 00167 | Train Loss : 0.2865 | Eval Loss : 0.3261 | Train acc : 0.8681 | Eval Acc : 0.8528 | Eval Log. Respected : 0.9430
     Batch 000 | Loss : 0.2655 | Acc : 0.8818
     Batch 005 | Loss : 0.3299 | Acc : 0.8504
     Batch 010 | Loss : 0.2591 | Acc : 0.8808
     Batch 015 | Loss : 0.2864 | Acc : 0.8696
     Batch 020 | Loss : 0.2951 | Acc : 0.8620
     Batch 025 | Loss : 0.2918 | Acc : 0.8629
     Batch 030 | Loss : 0.3020 | Acc : 0.8611
     Batch 035 | Loss : 0.2788 | Acc : 0.8729
     Batch 040 | Loss : 0.2710 | Acc : 0.8792
     Batch 045 | Loss : 0.2534 | Acc : 0.8879
     Batch 050 | Loss : 0.2913 | Acc : 0.8640
     Batch 055 | Loss : 0.2649 | Acc : 0.8790
     Batch 060 | Loss : 0.2441 | Acc : 0.8892
     Batch 065 | Loss : 0.2546 | Acc : 0.8858
     Batch 070 | Loss : 0.2581 | Acc : 0.8824
     Batch 075 | Loss : 0.2507 | Acc : 0.8842
     Batch 080 | Loss : 0.2922 | Acc : 0.8666
     Batch 085 | Loss : 0.2965 | Acc : 0.8621
     Batch 090 | Loss : 0.2484 | Acc : 0.8872
     Batch 095 | Loss : 0.2497 | Acc : 0.8876
     Batch 100 | Loss : 0.2752 | Acc : 0.8721
     Batch 105 | Loss : 0.2707 | Acc : 0.8769
     Batch 110 | Loss : 0.3264 | Acc : 0.8427
     Batch 115 | Loss : 0.2902 | Acc : 0.8657
     Batch 120 | Loss : 0.2941 | Acc : 0.8640
     Batch 125 | Loss : 0.2871 | Acc : 0.8666
     Batch 130 | Loss : 0.2761 | Acc : 0.8725
     Batch 135 | Loss : 0.2896 | Acc : 0.8658
     Batch 140 | Loss : 0.2937 | Acc : 0.8642
     Batch 145 | Loss : 0.3053 | Acc : 0.8600
     Batch 150 | Loss : 0.2643 | Acc : 0.8817
Epoch 00168 | Train Loss : 0.2866 | Eval Loss : 0.3211 | Train acc : 0.8682 | Eval Acc : 0.8512 | Eval Log. Respected : 0.9456
     Batch 000 | Loss : 0.2725 | Acc : 0.8787
     Batch 005 | Loss : 0.2821 | Acc : 0.8685
     Batch 010 | Loss : 0.3231 | Acc : 0.8473
     Batch 015 | Loss : 0.2554 | Acc : 0.8859
     Batch 020 | Loss : 0.2392 | Acc : 0.8936
     Batch 025 | Loss : 0.3341 | Acc : 0.8491
     Batch 030 | Loss : 0.3219 | Acc : 0.8507
     Batch 035 | Loss : 0.3125 | Acc : 0.8512
     Batch 040 | Loss : 0.3170 | Acc : 0.8497
     Batch 045 | Loss : 0.3137 | Acc : 0.8526
     Batch 050 | Loss : 0.2627 | Acc : 0.8779
     Batch 055 | Loss : 0.2599 | Acc : 0.8837
     Batch 060 | Loss : 0.2757 | Acc : 0.8764
     Batch 065 | Loss : 0.2691 | Acc : 0.8776
     Batch 070 | Loss : 0.2999 | Acc : 0.8601
     Batch 075 | Loss : 0.2613 | Acc : 0.8811
     Batch 080 | Loss : 0.2930 | Acc : 0.8653
     Batch 085 | Loss : 0.2650 | Acc : 0.8790
     Batch 090 | Loss : 0.2905 | Acc : 0.8686
     Batch 095 | Loss : 0.2695 | Acc : 0.8762
     Batch 100 | Loss : 0.2796 | Acc : 0.8740
     Batch 105 | Loss : 0.2508 | Acc : 0.8853
     Batch 110 | Loss : 0.2826 | Acc : 0.8744
     Batch 115 | Loss : 0.2974 | Acc : 0.8605
     Batch 120 | Loss : 0.2714 | Acc : 0.8774
     Batch 125 | Loss : 0.2642 | Acc : 0.8770
     Batch 130 | Loss : 0.2802 | Acc : 0.8729
     Batch 135 | Loss : 0.3725 | Acc : 0.8291
     Batch 140 | Loss : 0.2949 | Acc : 0.8672
     Batch 145 | Loss : 0.2825 | Acc : 0.8701
     Batch 150 | Loss : 0.3127 | Acc : 0.8613
Epoch 00169 | Train Loss : 0.2867 | Eval Loss : 0.3190 | Train acc : 0.8683 | Eval Acc : 0.8545 | Eval Log. Respected : 0.9330
     Batch 000 | Loss : 0.2446 | Acc : 0.8887
     Batch 005 | Loss : 0.3227 | Acc : 0.8487
     Batch 010 | Loss : 0.3326 | Acc : 0.8418
     Batch 015 | Loss : 0.3026 | Acc : 0.8585
     Batch 020 | Loss : 0.2769 | Acc : 0.8717
     Batch 025 | Loss : 0.2774 | Acc : 0.8698
     Batch 030 | Loss : 0.2759 | Acc : 0.8705
     Batch 035 | Loss : 0.2642 | Acc : 0.8821
     Batch 040 | Loss : 0.2595 | Acc : 0.8833
     Batch 045 | Loss : 0.2786 | Acc : 0.8717
     Batch 050 | Loss : 0.3012 | Acc : 0.8591
     Batch 055 | Loss : 0.3044 | Acc : 0.8590
     Batch 060 | Loss : 0.3080 | Acc : 0.8540
     Batch 065 | Loss : 0.2892 | Acc : 0.8679
     Batch 070 | Loss : 0.2960 | Acc : 0.8581
     Batch 075 | Loss : 0.2955 | Acc : 0.8616
     Batch 080 | Loss : 0.3231 | Acc : 0.8500
     Batch 085 | Loss : 0.2489 | Acc : 0.8875
     Batch 090 | Loss : 0.2615 | Acc : 0.8800
     Batch 095 | Loss : 0.2756 | Acc : 0.8770
     Batch 100 | Loss : 0.2659 | Acc : 0.8801
     Batch 105 | Loss : 0.3445 | Acc : 0.8402
     Batch 110 | Loss : 0.2378 | Acc : 0.8934
     Batch 115 | Loss : 0.3013 | Acc : 0.8596
     Batch 120 | Loss : 0.2842 | Acc : 0.8749
     Batch 125 | Loss : 0.2752 | Acc : 0.8722
     Batch 130 | Loss : 0.2503 | Acc : 0.8818
     Batch 135 | Loss : 0.2490 | Acc : 0.8897
     Batch 140 | Loss : 0.2797 | Acc : 0.8700
     Batch 145 | Loss : 0.3102 | Acc : 0.8573
     Batch 150 | Loss : 0.3000 | Acc : 0.8573
Epoch 00170 | Train Loss : 0.2880 | Eval Loss : 0.3192 | Train acc : 0.8675 | Eval Acc : 0.8533 | Eval Log. Respected : 0.9344
     Batch 000 | Loss : 0.3203 | Acc : 0.8542
     Batch 005 | Loss : 0.3046 | Acc : 0.8608
     Batch 010 | Loss : 0.3214 | Acc : 0.8447
     Batch 015 | Loss : 0.2690 | Acc : 0.8776
     Batch 020 | Loss : 0.2893 | Acc : 0.8670
     Batch 025 | Loss : 0.2550 | Acc : 0.8829
     Batch 030 | Loss : 0.2790 | Acc : 0.8729
     Batch 035 | Loss : 0.2986 | Acc : 0.8610
     Batch 040 | Loss : 0.2867 | Acc : 0.8667
     Batch 045 | Loss : 0.3164 | Acc : 0.8516
     Batch 050 | Loss : 0.2270 | Acc : 0.8992
     Batch 055 | Loss : 0.3601 | Acc : 0.8329
     Batch 060 | Loss : 0.3018 | Acc : 0.8562
     Batch 065 | Loss : 0.3013 | Acc : 0.8600
     Batch 070 | Loss : 0.2565 | Acc : 0.8824
     Batch 075 | Loss : 0.2777 | Acc : 0.8731
     Batch 080 | Loss : 0.3171 | Acc : 0.8493
     Batch 085 | Loss : 0.2934 | Acc : 0.8689
     Batch 090 | Loss : 0.3214 | Acc : 0.8470
     Batch 095 | Loss : 0.2596 | Acc : 0.8832
     Batch 100 | Loss : 0.3080 | Acc : 0.8582
     Batch 105 | Loss : 0.2780 | Acc : 0.8721
     Batch 110 | Loss : 0.2527 | Acc : 0.8870
     Batch 115 | Loss : 0.3234 | Acc : 0.8496
     Batch 120 | Loss : 0.3594 | Acc : 0.8302
     Batch 125 | Loss : 0.2861 | Acc : 0.8694
     Batch 130 | Loss : 0.3025 | Acc : 0.8607
     Batch 135 | Loss : 0.2954 | Acc : 0.8634
     Batch 140 | Loss : 0.2447 | Acc : 0.8904
     Batch 145 | Loss : 0.2917 | Acc : 0.8632
     Batch 150 | Loss : 0.2708 | Acc : 0.8762
Epoch 00171 | Train Loss : 0.2859 | Eval Loss : 0.3195 | Train acc : 0.8687 | Eval Acc : 0.8533 | Eval Log. Respected : 0.9394
     Batch 000 | Loss : 0.3067 | Acc : 0.8579
     Batch 005 | Loss : 0.3138 | Acc : 0.8520
     Batch 010 | Loss : 0.2951 | Acc : 0.8651
     Batch 015 | Loss : 0.2579 | Acc : 0.8810
     Batch 020 | Loss : 0.2427 | Acc : 0.8902
     Batch 025 | Loss : 0.3205 | Acc : 0.8495
     Batch 030 | Loss : 0.2830 | Acc : 0.8679
     Batch 035 | Loss : 0.2504 | Acc : 0.8818
     Batch 040 | Loss : 0.3016 | Acc : 0.8591
     Batch 045 | Loss : 0.2542 | Acc : 0.8833
     Batch 050 | Loss : 0.3502 | Acc : 0.8373
     Batch 055 | Loss : 0.2817 | Acc : 0.8726
     Batch 060 | Loss : 0.3360 | Acc : 0.8462
     Batch 065 | Loss : 0.2650 | Acc : 0.8825
     Batch 070 | Loss : 0.2514 | Acc : 0.8883
     Batch 075 | Loss : 0.2615 | Acc : 0.8814
     Batch 080 | Loss : 0.2915 | Acc : 0.8649
     Batch 085 | Loss : 0.2636 | Acc : 0.8827
     Batch 090 | Loss : 0.3011 | Acc : 0.8625
     Batch 095 | Loss : 0.2447 | Acc : 0.8885
     Batch 100 | Loss : 0.3142 | Acc : 0.8526
     Batch 105 | Loss : 0.2739 | Acc : 0.8761
     Batch 110 | Loss : 0.2447 | Acc : 0.8909
     Batch 115 | Loss : 0.2908 | Acc : 0.8664
     Batch 120 | Loss : 0.2821 | Acc : 0.8712
     Batch 125 | Loss : 0.2875 | Acc : 0.8675
     Batch 130 | Loss : 0.2673 | Acc : 0.8784
     Batch 135 | Loss : 0.2807 | Acc : 0.8696
     Batch 140 | Loss : 0.2957 | Acc : 0.8638
     Batch 145 | Loss : 0.3003 | Acc : 0.8630
     Batch 150 | Loss : 0.2675 | Acc : 0.8773
Epoch 00172 | Train Loss : 0.2846 | Eval Loss : 0.3184 | Train acc : 0.8690 | Eval Acc : 0.8528 | Eval Log. Respected : 0.9303
     Batch 000 | Loss : 0.3220 | Acc : 0.8489
     Batch 005 | Loss : 0.2446 | Acc : 0.8903
     Batch 010 | Loss : 0.2921 | Acc : 0.8672
     Batch 015 | Loss : 0.2653 | Acc : 0.8778
     Batch 020 | Loss : 0.3046 | Acc : 0.8585
     Batch 025 | Loss : 0.2568 | Acc : 0.8897
     Batch 030 | Loss : 0.2450 | Acc : 0.8896
     Batch 035 | Loss : 0.3068 | Acc : 0.8590
     Batch 040 | Loss : 0.3381 | Acc : 0.8429
     Batch 045 | Loss : 0.3164 | Acc : 0.8546
     Batch 050 | Loss : 0.2684 | Acc : 0.8766
     Batch 055 | Loss : 0.2741 | Acc : 0.8731
     Batch 060 | Loss : 0.2582 | Acc : 0.8823
     Batch 065 | Loss : 0.3315 | Acc : 0.8425
     Batch 070 | Loss : 0.3058 | Acc : 0.8596
     Batch 075 | Loss : 0.2802 | Acc : 0.8713
     Batch 080 | Loss : 0.2692 | Acc : 0.8785
     Batch 085 | Loss : 0.3075 | Acc : 0.8564
     Batch 090 | Loss : 0.2887 | Acc : 0.8670
     Batch 095 | Loss : 0.3187 | Acc : 0.8561
     Batch 100 | Loss : 0.3194 | Acc : 0.8503
     Batch 105 | Loss : 0.2614 | Acc : 0.8813
     Batch 110 | Loss : 0.2740 | Acc : 0.8784
     Batch 115 | Loss : 0.3638 | Acc : 0.8309
     Batch 120 | Loss : 0.2943 | Acc : 0.8622
     Batch 125 | Loss : 0.2431 | Acc : 0.8917
     Batch 130 | Loss : 0.3128 | Acc : 0.8548
     Batch 135 | Loss : 0.3088 | Acc : 0.8577
     Batch 140 | Loss : 0.2909 | Acc : 0.8633
     Batch 145 | Loss : 0.2296 | Acc : 0.8980
     Batch 150 | Loss : 0.3084 | Acc : 0.8599
Epoch 00173 | Train Loss : 0.2867 | Eval Loss : 0.3233 | Train acc : 0.8681 | Eval Acc : 0.8514 | Eval Log. Respected : 0.9451
     Batch 000 | Loss : 0.3020 | Acc : 0.8573
     Batch 005 | Loss : 0.2712 | Acc : 0.8752
     Batch 010 | Loss : 0.2599 | Acc : 0.8832
     Batch 015 | Loss : 0.2906 | Acc : 0.8655
     Batch 020 | Loss : 0.3021 | Acc : 0.8602
     Batch 025 | Loss : 0.2825 | Acc : 0.8719
     Batch 030 | Loss : 0.2676 | Acc : 0.8793
     Batch 035 | Loss : 0.2979 | Acc : 0.8616
     Batch 040 | Loss : 0.2608 | Acc : 0.8830
     Batch 045 | Loss : 0.2574 | Acc : 0.8811
     Batch 050 | Loss : 0.3029 | Acc : 0.8616
     Batch 055 | Loss : 0.3405 | Acc : 0.8407
     Batch 060 | Loss : 0.3093 | Acc : 0.8525
     Batch 065 | Loss : 0.2788 | Acc : 0.8745
     Batch 070 | Loss : 0.2680 | Acc : 0.8765
     Batch 075 | Loss : 0.2678 | Acc : 0.8750
     Batch 080 | Loss : 0.2996 | Acc : 0.8630
     Batch 085 | Loss : 0.2800 | Acc : 0.8687
     Batch 090 | Loss : 0.3058 | Acc : 0.8599
     Batch 095 | Loss : 0.2708 | Acc : 0.8762
     Batch 100 | Loss : 0.2862 | Acc : 0.8698
     Batch 105 | Loss : 0.2617 | Acc : 0.8794
     Batch 110 | Loss : 0.2758 | Acc : 0.8695
     Batch 115 | Loss : 0.2821 | Acc : 0.8730
     Batch 120 | Loss : 0.2391 | Acc : 0.8918
     Batch 125 | Loss : 0.2748 | Acc : 0.8741
     Batch 130 | Loss : 0.2820 | Acc : 0.8754
     Batch 135 | Loss : 0.2449 | Acc : 0.8945
     Batch 140 | Loss : 0.2216 | Acc : 0.9017
     Batch 145 | Loss : 0.2807 | Acc : 0.8723
     Batch 150 | Loss : 0.2906 | Acc : 0.8640
Epoch 00174 | Train Loss : 0.2856 | Eval Loss : 0.3203 | Train acc : 0.8685 | Eval Acc : 0.8519 | Eval Log. Respected : 0.9339
     Batch 000 | Loss : 0.2618 | Acc : 0.8789
     Batch 005 | Loss : 0.2426 | Acc : 0.8930
     Batch 010 | Loss : 0.3180 | Acc : 0.8525
     Batch 015 | Loss : 0.3029 | Acc : 0.8603
     Batch 020 | Loss : 0.2835 | Acc : 0.8724
     Batch 025 | Loss : 0.2390 | Acc : 0.8943
     Batch 030 | Loss : 0.3407 | Acc : 0.8436
     Batch 035 | Loss : 0.3147 | Acc : 0.8538
     Batch 040 | Loss : 0.3061 | Acc : 0.8617
     Batch 045 | Loss : 0.2719 | Acc : 0.8753
     Batch 050 | Loss : 0.2661 | Acc : 0.8805
     Batch 055 | Loss : 0.3284 | Acc : 0.8429
     Batch 060 | Loss : 0.3052 | Acc : 0.8561
     Batch 065 | Loss : 0.2729 | Acc : 0.8801
     Batch 070 | Loss : 0.2713 | Acc : 0.8765
     Batch 075 | Loss : 0.2567 | Acc : 0.8852
     Batch 080 | Loss : 0.2845 | Acc : 0.8700
     Batch 085 | Loss : 0.2433 | Acc : 0.8912
     Batch 090 | Loss : 0.2920 | Acc : 0.8658
     Batch 095 | Loss : 0.2611 | Acc : 0.8822
     Batch 100 | Loss : 0.3220 | Acc : 0.8469
     Batch 105 | Loss : 0.2511 | Acc : 0.8860
     Batch 110 | Loss : 0.3308 | Acc : 0.8457
     Batch 115 | Loss : 0.2753 | Acc : 0.8732
     Batch 120 | Loss : 0.2996 | Acc : 0.8648
     Batch 125 | Loss : 0.2470 | Acc : 0.8872
     Batch 130 | Loss : 0.3030 | Acc : 0.8581
     Batch 135 | Loss : 0.2749 | Acc : 0.8733
     Batch 140 | Loss : 0.2685 | Acc : 0.8777
     Batch 145 | Loss : 0.2513 | Acc : 0.8822
     Batch 150 | Loss : 0.2931 | Acc : 0.8665
Epoch 00175 | Train Loss : 0.2867 | Eval Loss : 0.3210 | Train acc : 0.8681 | Eval Acc : 0.8535 | Eval Log. Respected : 0.9332
     Batch 000 | Loss : 0.2833 | Acc : 0.8669
     Batch 005 | Loss : 0.3054 | Acc : 0.8556
     Batch 010 | Loss : 0.2879 | Acc : 0.8661
     Batch 015 | Loss : 0.3145 | Acc : 0.8617
     Batch 020 | Loss : 0.2762 | Acc : 0.8755
     Batch 025 | Loss : 0.2692 | Acc : 0.8776
     Batch 030 | Loss : 0.2641 | Acc : 0.8777
     Batch 035 | Loss : 0.3030 | Acc : 0.8570
     Batch 040 | Loss : 0.2722 | Acc : 0.8749
     Batch 045 | Loss : 0.2560 | Acc : 0.8852
     Batch 050 | Loss : 0.2916 | Acc : 0.8683
     Batch 055 | Loss : 0.3010 | Acc : 0.8617
     Batch 060 | Loss : 0.2573 | Acc : 0.8834
     Batch 065 | Loss : 0.3068 | Acc : 0.8551
     Batch 070 | Loss : 0.2529 | Acc : 0.8840
     Batch 075 | Loss : 0.3227 | Acc : 0.8510
     Batch 080 | Loss : 0.2854 | Acc : 0.8710
     Batch 085 | Loss : 0.2653 | Acc : 0.8800
     Batch 090 | Loss : 0.3335 | Acc : 0.8380
     Batch 095 | Loss : 0.3330 | Acc : 0.8417
     Batch 100 | Loss : 0.2731 | Acc : 0.8755
     Batch 105 | Loss : 0.2507 | Acc : 0.8860
     Batch 110 | Loss : 0.2493 | Acc : 0.8858
     Batch 115 | Loss : 0.2625 | Acc : 0.8810
     Batch 120 | Loss : 0.2867 | Acc : 0.8719
     Batch 125 | Loss : 0.2743 | Acc : 0.8758
     Batch 130 | Loss : 0.2602 | Acc : 0.8805
     Batch 135 | Loss : 0.3075 | Acc : 0.8577
     Batch 140 | Loss : 0.2730 | Acc : 0.8741
     Batch 145 | Loss : 0.3218 | Acc : 0.8483
     Batch 150 | Loss : 0.3395 | Acc : 0.8467
Epoch 00176 | Train Loss : 0.2863 | Eval Loss : 0.3229 | Train acc : 0.8685 | Eval Acc : 0.8520 | Eval Log. Respected : 0.9350
     Batch 000 | Loss : 0.2798 | Acc : 0.8673
     Batch 005 | Loss : 0.3130 | Acc : 0.8528
     Batch 010 | Loss : 0.3183 | Acc : 0.8480
     Batch 015 | Loss : 0.2518 | Acc : 0.8876
     Batch 020 | Loss : 0.3017 | Acc : 0.8624
     Batch 025 | Loss : 0.2841 | Acc : 0.8685
     Batch 030 | Loss : 0.2385 | Acc : 0.8886
     Batch 035 | Loss : 0.3462 | Acc : 0.8436
     Batch 040 | Loss : 0.2638 | Acc : 0.8824
     Batch 045 | Loss : 0.2759 | Acc : 0.8747
     Batch 050 | Loss : 0.3210 | Acc : 0.8507
     Batch 055 | Loss : 0.2837 | Acc : 0.8651
     Batch 060 | Loss : 0.2790 | Acc : 0.8716
     Batch 065 | Loss : 0.3005 | Acc : 0.8598
     Batch 070 | Loss : 0.2892 | Acc : 0.8673
     Batch 075 | Loss : 0.2890 | Acc : 0.8663
     Batch 080 | Loss : 0.2586 | Acc : 0.8807
     Batch 085 | Loss : 0.2689 | Acc : 0.8770
     Batch 090 | Loss : 0.2595 | Acc : 0.8825
     Batch 095 | Loss : 0.2601 | Acc : 0.8805
     Batch 100 | Loss : 0.2777 | Acc : 0.8709
     Batch 105 | Loss : 0.2412 | Acc : 0.8918
     Batch 110 | Loss : 0.3022 | Acc : 0.8590
     Batch 115 | Loss : 0.2749 | Acc : 0.8726
     Batch 120 | Loss : 0.2989 | Acc : 0.8634
     Batch 125 | Loss : 0.2884 | Acc : 0.8680
     Batch 130 | Loss : 0.3403 | Acc : 0.8393
     Batch 135 | Loss : 0.2704 | Acc : 0.8767
     Batch 140 | Loss : 0.3078 | Acc : 0.8598
     Batch 145 | Loss : 0.2580 | Acc : 0.8844
     Batch 150 | Loss : 0.2967 | Acc : 0.8620
Epoch 00177 | Train Loss : 0.2856 | Eval Loss : 0.3191 | Train acc : 0.8687 | Eval Acc : 0.8535 | Eval Log. Respected : 0.9295
     Batch 000 | Loss : 0.3109 | Acc : 0.8560
     Batch 005 | Loss : 0.2576 | Acc : 0.8847
     Batch 010 | Loss : 0.2894 | Acc : 0.8666
     Batch 015 | Loss : 0.2713 | Acc : 0.8753
     Batch 020 | Loss : 0.2891 | Acc : 0.8666
     Batch 025 | Loss : 0.2569 | Acc : 0.8807
     Batch 030 | Loss : 0.2640 | Acc : 0.8782
     Batch 035 | Loss : 0.3065 | Acc : 0.8548
     Batch 040 | Loss : 0.2916 | Acc : 0.8683
     Batch 045 | Loss : 0.2718 | Acc : 0.8747
     Batch 050 | Loss : 0.3137 | Acc : 0.8593
     Batch 055 | Loss : 0.3021 | Acc : 0.8619
     Batch 060 | Loss : 0.2825 | Acc : 0.8737
     Batch 065 | Loss : 0.2620 | Acc : 0.8791
     Batch 070 | Loss : 0.3000 | Acc : 0.8602
     Batch 075 | Loss : 0.2547 | Acc : 0.8868
     Batch 080 | Loss : 0.3314 | Acc : 0.8467
     Batch 085 | Loss : 0.2755 | Acc : 0.8711
     Batch 090 | Loss : 0.2552 | Acc : 0.8844
     Batch 095 | Loss : 0.2478 | Acc : 0.8876
     Batch 100 | Loss : 0.2773 | Acc : 0.8756
     Batch 105 | Loss : 0.2599 | Acc : 0.8816
     Batch 110 | Loss : 0.3553 | Acc : 0.8376
     Batch 115 | Loss : 0.3165 | Acc : 0.8495
     Batch 120 | Loss : 0.2807 | Acc : 0.8703
     Batch 125 | Loss : 0.3135 | Acc : 0.8550
     Batch 130 | Loss : 0.2784 | Acc : 0.8709
     Batch 135 | Loss : 0.2561 | Acc : 0.8806
     Batch 140 | Loss : 0.2493 | Acc : 0.8911
     Batch 145 | Loss : 0.3125 | Acc : 0.8606
     Batch 150 | Loss : 0.2507 | Acc : 0.8871
Epoch 00178 | Train Loss : 0.2841 | Eval Loss : 0.3220 | Train acc : 0.8693 | Eval Acc : 0.8520 | Eval Log. Respected : 0.9456
     Batch 000 | Loss : 0.2595 | Acc : 0.8815
     Batch 005 | Loss : 0.2298 | Acc : 0.8980
     Batch 010 | Loss : 0.2804 | Acc : 0.8703
     Batch 015 | Loss : 0.2484 | Acc : 0.8876
     Batch 020 | Loss : 0.2554 | Acc : 0.8853
     Batch 025 | Loss : 0.2669 | Acc : 0.8787
     Batch 030 | Loss : 0.2923 | Acc : 0.8660
     Batch 035 | Loss : 0.3149 | Acc : 0.8548
     Batch 040 | Loss : 0.3479 | Acc : 0.8413
     Batch 045 | Loss : 0.2697 | Acc : 0.8767
     Batch 050 | Loss : 0.3077 | Acc : 0.8596
     Batch 055 | Loss : 0.3171 | Acc : 0.8534
     Batch 060 | Loss : 0.2956 | Acc : 0.8651
     Batch 065 | Loss : 0.3094 | Acc : 0.8536
     Batch 070 | Loss : 0.2717 | Acc : 0.8742
     Batch 075 | Loss : 0.2566 | Acc : 0.8834
     Batch 080 | Loss : 0.3024 | Acc : 0.8587
     Batch 085 | Loss : 0.2561 | Acc : 0.8855
     Batch 090 | Loss : 0.2906 | Acc : 0.8628
     Batch 095 | Loss : 0.3349 | Acc : 0.8469
     Batch 100 | Loss : 0.2614 | Acc : 0.8817
     Batch 105 | Loss : 0.2723 | Acc : 0.8773
     Batch 110 | Loss : 0.3134 | Acc : 0.8521
     Batch 115 | Loss : 0.3152 | Acc : 0.8542
     Batch 120 | Loss : 0.2869 | Acc : 0.8678
     Batch 125 | Loss : 0.2565 | Acc : 0.8849
     Batch 130 | Loss : 0.2827 | Acc : 0.8701
     Batch 135 | Loss : 0.3241 | Acc : 0.8515
     Batch 140 | Loss : 0.2523 | Acc : 0.8881
     Batch 145 | Loss : 0.3110 | Acc : 0.8548
     Batch 150 | Loss : 0.3282 | Acc : 0.8437
Epoch 00179 | Train Loss : 0.2845 | Eval Loss : 0.3183 | Train acc : 0.8692 | Eval Acc : 0.8523 | Eval Log. Respected : 0.9336
     Batch 000 | Loss : 0.2685 | Acc : 0.8789
     Batch 005 | Loss : 0.2623 | Acc : 0.8785
     Batch 010 | Loss : 0.2960 | Acc : 0.8640
     Batch 015 | Loss : 0.2893 | Acc : 0.8667
     Batch 020 | Loss : 0.2198 | Acc : 0.9040
     Batch 025 | Loss : 0.2794 | Acc : 0.8701
     Batch 030 | Loss : 0.2578 | Acc : 0.8848
     Batch 035 | Loss : 0.3020 | Acc : 0.8591
     Batch 040 | Loss : 0.3540 | Acc : 0.8382
     Batch 045 | Loss : 0.3091 | Acc : 0.8543
     Batch 050 | Loss : 0.2872 | Acc : 0.8684
     Batch 055 | Loss : 0.3085 | Acc : 0.8600
     Batch 060 | Loss : 0.3062 | Acc : 0.8531
     Batch 065 | Loss : 0.2655 | Acc : 0.8828
     Batch 070 | Loss : 0.2697 | Acc : 0.8786
     Batch 075 | Loss : 0.3406 | Acc : 0.8404
     Batch 080 | Loss : 0.2580 | Acc : 0.8846
     Batch 085 | Loss : 0.3094 | Acc : 0.8549
     Batch 090 | Loss : 0.2743 | Acc : 0.8722
     Batch 095 | Loss : 0.3100 | Acc : 0.8559
     Batch 100 | Loss : 0.3313 | Acc : 0.8450
     Batch 105 | Loss : 0.2612 | Acc : 0.8823
     Batch 110 | Loss : 0.2974 | Acc : 0.8649
     Batch 115 | Loss : 0.2552 | Acc : 0.8868
     Batch 120 | Loss : 0.2934 | Acc : 0.8619
     Batch 125 | Loss : 0.2677 | Acc : 0.8773
     Batch 130 | Loss : 0.2436 | Acc : 0.8897
     Batch 135 | Loss : 0.2796 | Acc : 0.8718
     Batch 140 | Loss : 0.3468 | Acc : 0.8362
     Batch 145 | Loss : 0.2695 | Acc : 0.8766
     Batch 150 | Loss : 0.2719 | Acc : 0.8762
Epoch 00180 | Train Loss : 0.2861 | Eval Loss : 0.3234 | Train acc : 0.8682 | Eval Acc : 0.8534 | Eval Log. Respected : 0.9382
     Batch 000 | Loss : 0.2589 | Acc : 0.8786
     Batch 005 | Loss : 0.2880 | Acc : 0.8644
     Batch 010 | Loss : 0.2630 | Acc : 0.8778
     Batch 015 | Loss : 0.2889 | Acc : 0.8691
     Batch 020 | Loss : 0.2992 | Acc : 0.8636
     Batch 025 | Loss : 0.2995 | Acc : 0.8618
     Batch 030 | Loss : 0.2591 | Acc : 0.8821
     Batch 035 | Loss : 0.2866 | Acc : 0.8659
     Batch 040 | Loss : 0.2857 | Acc : 0.8701
     Batch 045 | Loss : 0.3006 | Acc : 0.8625
     Batch 050 | Loss : 0.2536 | Acc : 0.8863
     Batch 055 | Loss : 0.2615 | Acc : 0.8822
     Batch 060 | Loss : 0.2454 | Acc : 0.8903
     Batch 065 | Loss : 0.2647 | Acc : 0.8817
     Batch 070 | Loss : 0.2802 | Acc : 0.8697
     Batch 075 | Loss : 0.2762 | Acc : 0.8723
     Batch 080 | Loss : 0.3075 | Acc : 0.8564
     Batch 085 | Loss : 0.3226 | Acc : 0.8482
     Batch 090 | Loss : 0.2906 | Acc : 0.8648
     Batch 095 | Loss : 0.2632 | Acc : 0.8799
     Batch 100 | Loss : 0.2823 | Acc : 0.8717
     Batch 105 | Loss : 0.2838 | Acc : 0.8696
     Batch 110 | Loss : 0.2663 | Acc : 0.8766
     Batch 115 | Loss : 0.3164 | Acc : 0.8542
     Batch 120 | Loss : 0.2858 | Acc : 0.8677
     Batch 125 | Loss : 0.2927 | Acc : 0.8652
     Batch 130 | Loss : 0.2463 | Acc : 0.8918
     Batch 135 | Loss : 0.2817 | Acc : 0.8689
     Batch 140 | Loss : 0.3236 | Acc : 0.8475
     Batch 145 | Loss : 0.2705 | Acc : 0.8783
     Batch 150 | Loss : 0.3218 | Acc : 0.8517
Epoch 00181 | Train Loss : 0.2842 | Eval Loss : 0.3236 | Train acc : 0.8693 | Eval Acc : 0.8518 | Eval Log. Respected : 0.9448
     Batch 000 | Loss : 0.2785 | Acc : 0.8719
     Batch 005 | Loss : 0.2604 | Acc : 0.8786
     Batch 010 | Loss : 0.3246 | Acc : 0.8475
     Batch 015 | Loss : 0.2525 | Acc : 0.8862
     Batch 020 | Loss : 0.3014 | Acc : 0.8590
     Batch 025 | Loss : 0.2820 | Acc : 0.8692
     Batch 030 | Loss : 0.3144 | Acc : 0.8536
     Batch 035 | Loss : 0.2692 | Acc : 0.8759
     Batch 040 | Loss : 0.2414 | Acc : 0.8909
     Batch 045 | Loss : 0.2690 | Acc : 0.8785
     Batch 050 | Loss : 0.2855 | Acc : 0.8686
     Batch 055 | Loss : 0.2804 | Acc : 0.8692
     Batch 060 | Loss : 0.2790 | Acc : 0.8732
     Batch 065 | Loss : 0.2444 | Acc : 0.8895
     Batch 070 | Loss : 0.3535 | Acc : 0.8405
     Batch 075 | Loss : 0.2811 | Acc : 0.8729
     Batch 080 | Loss : 0.2763 | Acc : 0.8727
     Batch 085 | Loss : 0.2740 | Acc : 0.8751
     Batch 090 | Loss : 0.3058 | Acc : 0.8611
     Batch 095 | Loss : 0.2800 | Acc : 0.8701
     Batch 100 | Loss : 0.2938 | Acc : 0.8630
     Batch 105 | Loss : 0.2264 | Acc : 0.9019
     Batch 110 | Loss : 0.3231 | Acc : 0.8542
     Batch 115 | Loss : 0.3362 | Acc : 0.8438
     Batch 120 | Loss : 0.2893 | Acc : 0.8644
     Batch 125 | Loss : 0.3107 | Acc : 0.8535
     Batch 130 | Loss : 0.2698 | Acc : 0.8747
     Batch 135 | Loss : 0.2854 | Acc : 0.8665
     Batch 140 | Loss : 0.3568 | Acc : 0.8320
     Batch 145 | Loss : 0.3150 | Acc : 0.8521
     Batch 150 | Loss : 0.2789 | Acc : 0.8706
Epoch 00182 | Train Loss : 0.2839 | Eval Loss : 0.3225 | Train acc : 0.8696 | Eval Acc : 0.8524 | Eval Log. Respected : 0.9346
     Batch 000 | Loss : 0.2441 | Acc : 0.8871
     Batch 005 | Loss : 0.2968 | Acc : 0.8647
     Batch 010 | Loss : 0.2876 | Acc : 0.8655
     Batch 015 | Loss : 0.2855 | Acc : 0.8676
     Batch 020 | Loss : 0.2960 | Acc : 0.8622
     Batch 025 | Loss : 0.2961 | Acc : 0.8634
     Batch 030 | Loss : 0.2680 | Acc : 0.8778
     Batch 035 | Loss : 0.2699 | Acc : 0.8791
     Batch 040 | Loss : 0.3015 | Acc : 0.8557
     Batch 045 | Loss : 0.2891 | Acc : 0.8674
     Batch 050 | Loss : 0.3002 | Acc : 0.8581
     Batch 055 | Loss : 0.2756 | Acc : 0.8744
     Batch 060 | Loss : 0.3093 | Acc : 0.8566
     Batch 065 | Loss : 0.2634 | Acc : 0.8800
     Batch 070 | Loss : 0.2886 | Acc : 0.8681
     Batch 075 | Loss : 0.3698 | Acc : 0.8339
     Batch 080 | Loss : 0.3020 | Acc : 0.8575
     Batch 085 | Loss : 0.2642 | Acc : 0.8814
     Batch 090 | Loss : 0.3337 | Acc : 0.8444
     Batch 095 | Loss : 0.2412 | Acc : 0.8905
     Batch 100 | Loss : 0.2716 | Acc : 0.8740
     Batch 105 | Loss : 0.3090 | Acc : 0.8564
     Batch 110 | Loss : 0.2964 | Acc : 0.8664
     Batch 115 | Loss : 0.2570 | Acc : 0.8858
     Batch 120 | Loss : 0.2663 | Acc : 0.8809
     Batch 125 | Loss : 0.2721 | Acc : 0.8774
     Batch 130 | Loss : 0.3040 | Acc : 0.8605
     Batch 135 | Loss : 0.2797 | Acc : 0.8716
     Batch 140 | Loss : 0.2503 | Acc : 0.8876
     Batch 145 | Loss : 0.3249 | Acc : 0.8565
     Batch 150 | Loss : 0.3279 | Acc : 0.8475
Epoch 00183 | Train Loss : 0.2838 | Eval Loss : 0.3185 | Train acc : 0.8695 | Eval Acc : 0.8526 | Eval Log. Respected : 0.9395
     Batch 000 | Loss : 0.3020 | Acc : 0.8610
     Batch 005 | Loss : 0.2584 | Acc : 0.8825
     Batch 010 | Loss : 0.2453 | Acc : 0.8899
     Batch 015 | Loss : 0.2473 | Acc : 0.8890
     Batch 020 | Loss : 0.2943 | Acc : 0.8632
     Batch 025 | Loss : 0.2772 | Acc : 0.8729
     Batch 030 | Loss : 0.2911 | Acc : 0.8682
     Batch 035 | Loss : 0.2610 | Acc : 0.8788
     Batch 040 | Loss : 0.3053 | Acc : 0.8609
     Batch 045 | Loss : 0.2651 | Acc : 0.8796
     Batch 050 | Loss : 0.3332 | Acc : 0.8420
     Batch 055 | Loss : 0.2455 | Acc : 0.8907
     Batch 060 | Loss : 0.3113 | Acc : 0.8561
     Batch 065 | Loss : 0.2534 | Acc : 0.8849
     Batch 070 | Loss : 0.3216 | Acc : 0.8528
     Batch 075 | Loss : 0.2853 | Acc : 0.8721
     Batch 080 | Loss : 0.3068 | Acc : 0.8594
     Batch 085 | Loss : 0.2633 | Acc : 0.8848
     Batch 090 | Loss : 0.2933 | Acc : 0.8619
     Batch 095 | Loss : 0.3084 | Acc : 0.8548
     Batch 100 | Loss : 0.2918 | Acc : 0.8632
     Batch 105 | Loss : 0.2849 | Acc : 0.8706
     Batch 110 | Loss : 0.2464 | Acc : 0.8897
     Batch 115 | Loss : 0.2601 | Acc : 0.8798
     Batch 120 | Loss : 0.3050 | Acc : 0.8531
     Batch 125 | Loss : 0.3089 | Acc : 0.8566
     Batch 130 | Loss : 0.2827 | Acc : 0.8667
     Batch 135 | Loss : 0.2292 | Acc : 0.8989
     Batch 140 | Loss : 0.2598 | Acc : 0.8782
     Batch 145 | Loss : 0.2749 | Acc : 0.8762
     Batch 150 | Loss : 0.2750 | Acc : 0.8749
Epoch 00184 | Train Loss : 0.2844 | Eval Loss : 0.3251 | Train acc : 0.8694 | Eval Acc : 0.8521 | Eval Log. Respected : 0.9320
     Batch 000 | Loss : 0.2342 | Acc : 0.8947
     Batch 005 | Loss : 0.3202 | Acc : 0.8546
     Batch 010 | Loss : 0.2462 | Acc : 0.8948
     Batch 015 | Loss : 0.3468 | Acc : 0.8387
     Batch 020 | Loss : 0.2690 | Acc : 0.8787
     Batch 025 | Loss : 0.2572 | Acc : 0.8811
     Batch 030 | Loss : 0.2848 | Acc : 0.8701
     Batch 035 | Loss : 0.2667 | Acc : 0.8792
     Batch 040 | Loss : 0.3075 | Acc : 0.8597
     Batch 045 | Loss : 0.3079 | Acc : 0.8585
     Batch 050 | Loss : 0.2566 | Acc : 0.8850
     Batch 055 | Loss : 0.2634 | Acc : 0.8790
     Batch 060 | Loss : 0.2852 | Acc : 0.8683
     Batch 065 | Loss : 0.2820 | Acc : 0.8687
     Batch 070 | Loss : 0.2821 | Acc : 0.8715
     Batch 075 | Loss : 0.2625 | Acc : 0.8822
     Batch 080 | Loss : 0.3043 | Acc : 0.8584
     Batch 085 | Loss : 0.2477 | Acc : 0.8861
     Batch 090 | Loss : 0.3032 | Acc : 0.8576
     Batch 095 | Loss : 0.3005 | Acc : 0.8596
     Batch 100 | Loss : 0.2727 | Acc : 0.8769
     Batch 105 | Loss : 0.2883 | Acc : 0.8620
     Batch 110 | Loss : 0.2988 | Acc : 0.8585
     Batch 115 | Loss : 0.2819 | Acc : 0.8684
     Batch 120 | Loss : 0.3173 | Acc : 0.8557
     Batch 125 | Loss : 0.3089 | Acc : 0.8589
     Batch 130 | Loss : 0.2724 | Acc : 0.8745
     Batch 135 | Loss : 0.2874 | Acc : 0.8670
     Batch 140 | Loss : 0.3422 | Acc : 0.8408
     Batch 145 | Loss : 0.3030 | Acc : 0.8627
     Batch 150 | Loss : 0.2527 | Acc : 0.8888
Epoch 00185 | Train Loss : 0.2835 | Eval Loss : 0.3240 | Train acc : 0.8697 | Eval Acc : 0.8539 | Eval Log. Respected : 0.9351
     Batch 000 | Loss : 0.2654 | Acc : 0.8806
     Batch 005 | Loss : 0.2471 | Acc : 0.8894
     Batch 010 | Loss : 0.2756 | Acc : 0.8707
     Batch 015 | Loss : 0.3126 | Acc : 0.8553
     Batch 020 | Loss : 0.2804 | Acc : 0.8686
     Batch 025 | Loss : 0.2319 | Acc : 0.8942
     Batch 030 | Loss : 0.2913 | Acc : 0.8657
     Batch 035 | Loss : 0.2667 | Acc : 0.8804
     Batch 040 | Loss : 0.2534 | Acc : 0.8842
     Batch 045 | Loss : 0.2572 | Acc : 0.8864
     Batch 050 | Loss : 0.3034 | Acc : 0.8616
     Batch 055 | Loss : 0.2897 | Acc : 0.8641
     Batch 060 | Loss : 0.2847 | Acc : 0.8676
     Batch 065 | Loss : 0.3688 | Acc : 0.8383
     Batch 070 | Loss : 0.2743 | Acc : 0.8738
     Batch 075 | Loss : 0.2818 | Acc : 0.8731
     Batch 080 | Loss : 0.2974 | Acc : 0.8634
     Batch 085 | Loss : 0.2670 | Acc : 0.8791
     Batch 090 | Loss : 0.2680 | Acc : 0.8770
     Batch 095 | Loss : 0.3252 | Acc : 0.8503
     Batch 100 | Loss : 0.2917 | Acc : 0.8642
     Batch 105 | Loss : 0.3807 | Acc : 0.8267
     Batch 110 | Loss : 0.2940 | Acc : 0.8634
     Batch 115 | Loss : 0.3034 | Acc : 0.8584
     Batch 120 | Loss : 0.2774 | Acc : 0.8732
     Batch 125 | Loss : 0.3281 | Acc : 0.8464
     Batch 130 | Loss : 0.2738 | Acc : 0.8730
     Batch 135 | Loss : 0.3328 | Acc : 0.8428
     Batch 140 | Loss : 0.2911 | Acc : 0.8676
     Batch 145 | Loss : 0.2641 | Acc : 0.8763
     Batch 150 | Loss : 0.2541 | Acc : 0.8858
Epoch 00186 | Train Loss : 0.2838 | Eval Loss : 0.3250 | Train acc : 0.8696 | Eval Acc : 0.8524 | Eval Log. Respected : 0.9333
     Batch 000 | Loss : 0.2680 | Acc : 0.8755
     Batch 005 | Loss : 0.2544 | Acc : 0.8849
     Batch 010 | Loss : 0.3367 | Acc : 0.8424
     Batch 015 | Loss : 0.2917 | Acc : 0.8687
     Batch 020 | Loss : 0.3111 | Acc : 0.8551
     Batch 025 | Loss : 0.2929 | Acc : 0.8653
     Batch 030 | Loss : 0.2600 | Acc : 0.8812
     Batch 035 | Loss : 0.2497 | Acc : 0.8875
     Batch 040 | Loss : 0.2771 | Acc : 0.8753
     Batch 045 | Loss : 0.2960 | Acc : 0.8619
     Batch 050 | Loss : 0.2815 | Acc : 0.8719
     Batch 055 | Loss : 0.3238 | Acc : 0.8482
     Batch 060 | Loss : 0.2999 | Acc : 0.8620
     Batch 065 | Loss : 0.2626 | Acc : 0.8810
     Batch 070 | Loss : 0.3068 | Acc : 0.8574
     Batch 075 | Loss : 0.2881 | Acc : 0.8652
     Batch 080 | Loss : 0.3206 | Acc : 0.8519
     Batch 085 | Loss : 0.2562 | Acc : 0.8814
     Batch 090 | Loss : 0.2613 | Acc : 0.8830
     Batch 095 | Loss : 0.2652 | Acc : 0.8813
     Batch 100 | Loss : 0.2798 | Acc : 0.8735
     Batch 105 | Loss : 0.3089 | Acc : 0.8526
     Batch 110 | Loss : 0.2390 | Acc : 0.8938
     Batch 115 | Loss : 0.2734 | Acc : 0.8759
     Batch 120 | Loss : 0.2755 | Acc : 0.8754
     Batch 125 | Loss : 0.2903 | Acc : 0.8640
     Batch 130 | Loss : 0.2834 | Acc : 0.8703
     Batch 135 | Loss : 0.2868 | Acc : 0.8685
     Batch 140 | Loss : 0.3186 | Acc : 0.8521
     Batch 145 | Loss : 0.2777 | Acc : 0.8750
     Batch 150 | Loss : 0.2855 | Acc : 0.8671
Epoch 00187 | Train Loss : 0.2843 | Eval Loss : 0.3186 | Train acc : 0.8693 | Eval Acc : 0.8536 | Eval Log. Respected : 0.9378
     Batch 000 | Loss : 0.2881 | Acc : 0.8676
     Batch 005 | Loss : 0.2734 | Acc : 0.8763
     Batch 010 | Loss : 0.2441 | Acc : 0.8880
     Batch 015 | Loss : 0.2899 | Acc : 0.8673
     Batch 020 | Loss : 0.2802 | Acc : 0.8706
     Batch 025 | Loss : 0.2639 | Acc : 0.8795
     Batch 030 | Loss : 0.2807 | Acc : 0.8706
     Batch 035 | Loss : 0.2539 | Acc : 0.8882
     Batch 040 | Loss : 0.2833 | Acc : 0.8678
     Batch 045 | Loss : 0.2982 | Acc : 0.8636
     Batch 050 | Loss : 0.2886 | Acc : 0.8661
     Batch 055 | Loss : 0.2818 | Acc : 0.8715
     Batch 060 | Loss : 0.3351 | Acc : 0.8457
     Batch 065 | Loss : 0.2765 | Acc : 0.8741
     Batch 070 | Loss : 0.2891 | Acc : 0.8653
     Batch 075 | Loss : 0.2842 | Acc : 0.8687
     Batch 080 | Loss : 0.2595 | Acc : 0.8855
     Batch 085 | Loss : 0.2633 | Acc : 0.8774
     Batch 090 | Loss : 0.3188 | Acc : 0.8539
     Batch 095 | Loss : 0.2539 | Acc : 0.8863
     Batch 100 | Loss : 0.2414 | Acc : 0.8921
     Batch 105 | Loss : 0.2903 | Acc : 0.8643
     Batch 110 | Loss : 0.2257 | Acc : 0.8995
     Batch 115 | Loss : 0.2771 | Acc : 0.8696
     Batch 120 | Loss : 0.2886 | Acc : 0.8666
     Batch 125 | Loss : 0.3014 | Acc : 0.8579
     Batch 130 | Loss : 0.3561 | Acc : 0.8380
     Batch 135 | Loss : 0.2928 | Acc : 0.8639
     Batch 140 | Loss : 0.2939 | Acc : 0.8612
     Batch 145 | Loss : 0.2702 | Acc : 0.8765
     Batch 150 | Loss : 0.2739 | Acc : 0.8756
Epoch 00188 | Train Loss : 0.2825 | Eval Loss : 0.3188 | Train acc : 0.8702 | Eval Acc : 0.8536 | Eval Log. Respected : 0.9370
     Batch 000 | Loss : 0.2897 | Acc : 0.8658
     Batch 005 | Loss : 0.2930 | Acc : 0.8671
     Batch 010 | Loss : 0.3069 | Acc : 0.8565
     Batch 015 | Loss : 0.2884 | Acc : 0.8660
     Batch 020 | Loss : 0.2843 | Acc : 0.8697
     Batch 025 | Loss : 0.3185 | Acc : 0.8524
     Batch 030 | Loss : 0.2885 | Acc : 0.8661
     Batch 035 | Loss : 0.3090 | Acc : 0.8581
     Batch 040 | Loss : 0.2997 | Acc : 0.8615
     Batch 045 | Loss : 0.2733 | Acc : 0.8735
     Batch 050 | Loss : 0.2735 | Acc : 0.8751
     Batch 055 | Loss : 0.2706 | Acc : 0.8772
     Batch 060 | Loss : 0.2962 | Acc : 0.8645
     Batch 065 | Loss : 0.2290 | Acc : 0.8971
     Batch 070 | Loss : 0.2675 | Acc : 0.8782
     Batch 075 | Loss : 0.3212 | Acc : 0.8482
     Batch 080 | Loss : 0.2543 | Acc : 0.8850
     Batch 085 | Loss : 0.2882 | Acc : 0.8648
     Batch 090 | Loss : 0.2717 | Acc : 0.8739
     Batch 095 | Loss : 0.2934 | Acc : 0.8641
     Batch 100 | Loss : 0.3412 | Acc : 0.8364
     Batch 105 | Loss : 0.3212 | Acc : 0.8516
     Batch 110 | Loss : 0.2938 | Acc : 0.8664
     Batch 115 | Loss : 0.2616 | Acc : 0.8780
     Batch 120 | Loss : 0.2766 | Acc : 0.8748
     Batch 125 | Loss : 0.2480 | Acc : 0.8920
     Batch 130 | Loss : 0.3035 | Acc : 0.8608
     Batch 135 | Loss : 0.3097 | Acc : 0.8532
     Batch 140 | Loss : 0.2611 | Acc : 0.8796
     Batch 145 | Loss : 0.3128 | Acc : 0.8533
     Batch 150 | Loss : 0.2657 | Acc : 0.8808
Epoch 00189 | Train Loss : 0.2835 | Eval Loss : 0.3206 | Train acc : 0.8696 | Eval Acc : 0.8527 | Eval Log. Respected : 0.9336
     Batch 000 | Loss : 0.2588 | Acc : 0.8818
     Batch 005 | Loss : 0.2838 | Acc : 0.8712
     Batch 010 | Loss : 0.3233 | Acc : 0.8512
     Batch 015 | Loss : 0.2620 | Acc : 0.8791
     Batch 020 | Loss : 0.3217 | Acc : 0.8520
     Batch 025 | Loss : 0.2703 | Acc : 0.8758
     Batch 030 | Loss : 0.2832 | Acc : 0.8717
     Batch 035 | Loss : 0.2988 | Acc : 0.8651
     Batch 040 | Loss : 0.3044 | Acc : 0.8584
     Batch 045 | Loss : 0.2686 | Acc : 0.8790
     Batch 050 | Loss : 0.2461 | Acc : 0.8865
     Batch 055 | Loss : 0.2669 | Acc : 0.8787
     Batch 060 | Loss : 0.3039 | Acc : 0.8598
     Batch 065 | Loss : 0.3142 | Acc : 0.8534
     Batch 070 | Loss : 0.2723 | Acc : 0.8755
     Batch 075 | Loss : 0.3192 | Acc : 0.8528
     Batch 080 | Loss : 0.2567 | Acc : 0.8851
     Batch 085 | Loss : 0.2828 | Acc : 0.8697
     Batch 090 | Loss : 0.2634 | Acc : 0.8813
     Batch 095 | Loss : 0.2918 | Acc : 0.8639
     Batch 100 | Loss : 0.2875 | Acc : 0.8665
     Batch 105 | Loss : 0.2329 | Acc : 0.8966
     Batch 110 | Loss : 0.2570 | Acc : 0.8831
     Batch 115 | Loss : 0.3348 | Acc : 0.8423
     Batch 120 | Loss : 0.2666 | Acc : 0.8772
     Batch 125 | Loss : 0.3034 | Acc : 0.8644
     Batch 130 | Loss : 0.3170 | Acc : 0.8525
     Batch 135 | Loss : 0.2729 | Acc : 0.8742
     Batch 140 | Loss : 0.2628 | Acc : 0.8828
     Batch 145 | Loss : 0.2666 | Acc : 0.8791
     Batch 150 | Loss : 0.2747 | Acc : 0.8747
Epoch 00190 | Train Loss : 0.2823 | Eval Loss : 0.3274 | Train acc : 0.8702 | Eval Acc : 0.8507 | Eval Log. Respected : 0.9276
     Batch 000 | Loss : 0.2523 | Acc : 0.8846
     Batch 005 | Loss : 0.3633 | Acc : 0.8357
     Batch 010 | Loss : 0.2977 | Acc : 0.8620
     Batch 015 | Loss : 0.3110 | Acc : 0.8562
     Batch 020 | Loss : 0.2898 | Acc : 0.8642
     Batch 025 | Loss : 0.3337 | Acc : 0.8418
     Batch 030 | Loss : 0.2612 | Acc : 0.8858
     Batch 035 | Loss : 0.2852 | Acc : 0.8678
     Batch 040 | Loss : 0.2843 | Acc : 0.8705
     Batch 045 | Loss : 0.2975 | Acc : 0.8607
     Batch 050 | Loss : 0.2569 | Acc : 0.8832
     Batch 055 | Loss : 0.2453 | Acc : 0.8912
     Batch 060 | Loss : 0.3200 | Acc : 0.8484
     Batch 065 | Loss : 0.2747 | Acc : 0.8735
     Batch 070 | Loss : 0.2956 | Acc : 0.8650
     Batch 075 | Loss : 0.3035 | Acc : 0.8586
     Batch 080 | Loss : 0.2924 | Acc : 0.8645
     Batch 085 | Loss : 0.2818 | Acc : 0.8695
     Batch 090 | Loss : 0.2721 | Acc : 0.8741
     Batch 095 | Loss : 0.2904 | Acc : 0.8679
     Batch 100 | Loss : 0.3389 | Acc : 0.8419
     Batch 105 | Loss : 0.2537 | Acc : 0.8860
     Batch 110 | Loss : 0.2683 | Acc : 0.8760
     Batch 115 | Loss : 0.2785 | Acc : 0.8681
     Batch 120 | Loss : 0.3432 | Acc : 0.8376
     Batch 125 | Loss : 0.2737 | Acc : 0.8758
     Batch 130 | Loss : 0.2613 | Acc : 0.8822
     Batch 135 | Loss : 0.2599 | Acc : 0.8826
     Batch 140 | Loss : 0.3325 | Acc : 0.8456
     Batch 145 | Loss : 0.2956 | Acc : 0.8625
     Batch 150 | Loss : 0.2574 | Acc : 0.8853
Epoch 00191 | Train Loss : 0.2845 | Eval Loss : 0.3300 | Train acc : 0.8692 | Eval Acc : 0.8504 | Eval Log. Respected : 0.9355
     Batch 000 | Loss : 0.3183 | Acc : 0.8531
     Batch 005 | Loss : 0.2886 | Acc : 0.8659
     Batch 010 | Loss : 0.2511 | Acc : 0.8874
     Batch 015 | Loss : 0.2588 | Acc : 0.8822
     Batch 020 | Loss : 0.2957 | Acc : 0.8618
     Batch 025 | Loss : 0.2677 | Acc : 0.8773
     Batch 030 | Loss : 0.3021 | Acc : 0.8617
     Batch 035 | Loss : 0.3359 | Acc : 0.8395
     Batch 040 | Loss : 0.2851 | Acc : 0.8673
     Batch 045 | Loss : 0.2634 | Acc : 0.8814
     Batch 050 | Loss : 0.3775 | Acc : 0.8232
     Batch 055 | Loss : 0.3158 | Acc : 0.8511
     Batch 060 | Loss : 0.2315 | Acc : 0.8950
     Batch 065 | Loss : 0.2476 | Acc : 0.8879
     Batch 070 | Loss : 0.2714 | Acc : 0.8775
     Batch 075 | Loss : 0.2947 | Acc : 0.8633
     Batch 080 | Loss : 0.2734 | Acc : 0.8755
     Batch 085 | Loss : 0.2967 | Acc : 0.8613
     Batch 090 | Loss : 0.3018 | Acc : 0.8557
     Batch 095 | Loss : 0.2496 | Acc : 0.8923
     Batch 100 | Loss : 0.2636 | Acc : 0.8803
     Batch 105 | Loss : 0.3165 | Acc : 0.8519
     Batch 110 | Loss : 0.2566 | Acc : 0.8847
     Batch 115 | Loss : 0.2749 | Acc : 0.8748
     Batch 120 | Loss : 0.3072 | Acc : 0.8582
     Batch 125 | Loss : 0.2569 | Acc : 0.8875
     Batch 130 | Loss : 0.2686 | Acc : 0.8768
     Batch 135 | Loss : 0.2625 | Acc : 0.8780
     Batch 140 | Loss : 0.2778 | Acc : 0.8719
     Batch 145 | Loss : 0.2648 | Acc : 0.8778
     Batch 150 | Loss : 0.3034 | Acc : 0.8607
Epoch 00192 | Train Loss : 0.2841 | Eval Loss : 0.3219 | Train acc : 0.8696 | Eval Acc : 0.8531 | Eval Log. Respected : 0.9368
     Batch 000 | Loss : 0.2650 | Acc : 0.8778
     Batch 005 | Loss : 0.2785 | Acc : 0.8742
     Batch 010 | Loss : 0.3136 | Acc : 0.8554
     Batch 015 | Loss : 0.2970 | Acc : 0.8626
     Batch 020 | Loss : 0.2925 | Acc : 0.8663
     Batch 025 | Loss : 0.2936 | Acc : 0.8624
     Batch 030 | Loss : 0.2223 | Acc : 0.8987
     Batch 035 | Loss : 0.2947 | Acc : 0.8644
     Batch 040 | Loss : 0.2561 | Acc : 0.8885
     Batch 045 | Loss : 0.2887 | Acc : 0.8655
     Batch 050 | Loss : 0.2915 | Acc : 0.8652
     Batch 055 | Loss : 0.2599 | Acc : 0.8780
     Batch 060 | Loss : 0.2592 | Acc : 0.8826
     Batch 065 | Loss : 0.2745 | Acc : 0.8716
     Batch 070 | Loss : 0.2615 | Acc : 0.8804
     Batch 075 | Loss : 0.2580 | Acc : 0.8812
     Batch 080 | Loss : 0.2927 | Acc : 0.8666
     Batch 085 | Loss : 0.2782 | Acc : 0.8746
     Batch 090 | Loss : 0.2921 | Acc : 0.8674
     Batch 095 | Loss : 0.2927 | Acc : 0.8641
     Batch 100 | Loss : 0.2735 | Acc : 0.8767
     Batch 105 | Loss : 0.2693 | Acc : 0.8746
     Batch 110 | Loss : 0.2636 | Acc : 0.8810
     Batch 115 | Loss : 0.3159 | Acc : 0.8514
     Batch 120 | Loss : 0.2882 | Acc : 0.8665
     Batch 125 | Loss : 0.2987 | Acc : 0.8621
     Batch 130 | Loss : 0.2734 | Acc : 0.8756
     Batch 135 | Loss : 0.2939 | Acc : 0.8632
     Batch 140 | Loss : 0.3018 | Acc : 0.8588
     Batch 145 | Loss : 0.2491 | Acc : 0.8854
     Batch 150 | Loss : 0.3313 | Acc : 0.8448
Epoch 00193 | Train Loss : 0.2820 | Eval Loss : 0.3208 | Train acc : 0.8704 | Eval Acc : 0.8530 | Eval Log. Respected : 0.9334
     Batch 000 | Loss : 0.2683 | Acc : 0.8769
     Batch 005 | Loss : 0.3066 | Acc : 0.8597
     Batch 010 | Loss : 0.3233 | Acc : 0.8482
     Batch 015 | Loss : 0.2734 | Acc : 0.8760
     Batch 020 | Loss : 0.2821 | Acc : 0.8694
     Batch 025 | Loss : 0.2638 | Acc : 0.8791
     Batch 030 | Loss : 0.2626 | Acc : 0.8833
     Batch 035 | Loss : 0.2562 | Acc : 0.8842
     Batch 040 | Loss : 0.3040 | Acc : 0.8598
     Batch 045 | Loss : 0.3281 | Acc : 0.8452
     Batch 050 | Loss : 0.3163 | Acc : 0.8548
     Batch 055 | Loss : 0.2504 | Acc : 0.8852
     Batch 060 | Loss : 0.2575 | Acc : 0.8827
     Batch 065 | Loss : 0.2832 | Acc : 0.8697
     Batch 070 | Loss : 0.2555 | Acc : 0.8859
     Batch 075 | Loss : 0.3029 | Acc : 0.8599
     Batch 080 | Loss : 0.2820 | Acc : 0.8745
     Batch 085 | Loss : 0.2616 | Acc : 0.8797
     Batch 090 | Loss : 0.2977 | Acc : 0.8615
     Batch 095 | Loss : 0.2987 | Acc : 0.8602
     Batch 100 | Loss : 0.2461 | Acc : 0.8865
     Batch 105 | Loss : 0.2861 | Acc : 0.8685
     Batch 110 | Loss : 0.3131 | Acc : 0.8543
     Batch 115 | Loss : 0.2760 | Acc : 0.8735
     Batch 120 | Loss : 0.3068 | Acc : 0.8551
     Batch 125 | Loss : 0.3260 | Acc : 0.8512
     Batch 130 | Loss : 0.2878 | Acc : 0.8702
     Batch 135 | Loss : 0.2684 | Acc : 0.8766
     Batch 140 | Loss : 0.3113 | Acc : 0.8565
     Batch 145 | Loss : 0.2659 | Acc : 0.8780
     Batch 150 | Loss : 0.2701 | Acc : 0.8786
Epoch 00194 | Train Loss : 0.2826 | Eval Loss : 0.3213 | Train acc : 0.8703 | Eval Acc : 0.8541 | Eval Log. Respected : 0.9329
     Batch 000 | Loss : 0.2536 | Acc : 0.8839
     Batch 005 | Loss : 0.2862 | Acc : 0.8680
     Batch 010 | Loss : 0.2794 | Acc : 0.8713
     Batch 015 | Loss : 0.2715 | Acc : 0.8742
     Batch 020 | Loss : 0.2554 | Acc : 0.8833
     Batch 025 | Loss : 0.2664 | Acc : 0.8805
     Batch 030 | Loss : 0.3078 | Acc : 0.8550
     Batch 035 | Loss : 0.2746 | Acc : 0.8745
     Batch 040 | Loss : 0.2919 | Acc : 0.8644
     Batch 045 | Loss : 0.2568 | Acc : 0.8820
     Batch 050 | Loss : 0.3269 | Acc : 0.8469
     Batch 055 | Loss : 0.2863 | Acc : 0.8658
     Batch 060 | Loss : 0.2470 | Acc : 0.8903
     Batch 065 | Loss : 0.2491 | Acc : 0.8860
     Batch 070 | Loss : 0.2920 | Acc : 0.8634
     Batch 075 | Loss : 0.2798 | Acc : 0.8748
     Batch 080 | Loss : 0.2940 | Acc : 0.8663
     Batch 085 | Loss : 0.3131 | Acc : 0.8571
     Batch 090 | Loss : 0.2799 | Acc : 0.8719
     Batch 095 | Loss : 0.2687 | Acc : 0.8747
     Batch 100 | Loss : 0.2876 | Acc : 0.8693
     Batch 105 | Loss : 0.3415 | Acc : 0.8389
     Batch 110 | Loss : 0.3045 | Acc : 0.8614
     Batch 115 | Loss : 0.2384 | Acc : 0.8948
     Batch 120 | Loss : 0.2863 | Acc : 0.8680
     Batch 125 | Loss : 0.2679 | Acc : 0.8758
     Batch 130 | Loss : 0.2599 | Acc : 0.8822
     Batch 135 | Loss : 0.3338 | Acc : 0.8467
     Batch 140 | Loss : 0.2798 | Acc : 0.8696
     Batch 145 | Loss : 0.2368 | Acc : 0.8969
     Batch 150 | Loss : 0.2735 | Acc : 0.8748
Epoch 00195 | Train Loss : 0.2835 | Eval Loss : 0.3205 | Train acc : 0.8699 | Eval Acc : 0.8535 | Eval Log. Respected : 0.9348
     Batch 000 | Loss : 0.2512 | Acc : 0.8868
     Batch 005 | Loss : 0.3181 | Acc : 0.8489
     Batch 010 | Loss : 0.2505 | Acc : 0.8872
     Batch 015 | Loss : 0.2866 | Acc : 0.8659
     Batch 020 | Loss : 0.2620 | Acc : 0.8810
     Batch 025 | Loss : 0.3092 | Acc : 0.8546
     Batch 030 | Loss : 0.2493 | Acc : 0.8864
     Batch 035 | Loss : 0.3172 | Acc : 0.8530
     Batch 040 | Loss : 0.2960 | Acc : 0.8659
     Batch 045 | Loss : 0.2403 | Acc : 0.8919
     Batch 050 | Loss : 0.2985 | Acc : 0.8642
     Batch 055 | Loss : 0.2892 | Acc : 0.8666
     Batch 060 | Loss : 0.2814 | Acc : 0.8720
     Batch 065 | Loss : 0.2641 | Acc : 0.8796
     Batch 070 | Loss : 0.2576 | Acc : 0.8825
     Batch 075 | Loss : 0.2629 | Acc : 0.8819
     Batch 080 | Loss : 0.2585 | Acc : 0.8825
     Batch 085 | Loss : 0.2613 | Acc : 0.8801
     Batch 090 | Loss : 0.2714 | Acc : 0.8749
     Batch 095 | Loss : 0.2763 | Acc : 0.8708
     Batch 100 | Loss : 0.2595 | Acc : 0.8824
     Batch 105 | Loss : 0.2934 | Acc : 0.8614
     Batch 110 | Loss : 0.2426 | Acc : 0.8917
     Batch 115 | Loss : 0.2905 | Acc : 0.8651
     Batch 120 | Loss : 0.2700 | Acc : 0.8766
     Batch 125 | Loss : 0.2677 | Acc : 0.8800
     Batch 130 | Loss : 0.3341 | Acc : 0.8415
     Batch 135 | Loss : 0.2805 | Acc : 0.8692
     Batch 140 | Loss : 0.3593 | Acc : 0.8338
     Batch 145 | Loss : 0.2877 | Acc : 0.8674
     Batch 150 | Loss : 0.3151 | Acc : 0.8524
Epoch 00196 | Train Loss : 0.2827 | Eval Loss : 0.3242 | Train acc : 0.8702 | Eval Acc : 0.8515 | Eval Log. Respected : 0.9288
     Batch 000 | Loss : 0.2574 | Acc : 0.8861
     Batch 005 | Loss : 0.2806 | Acc : 0.8695
     Batch 010 | Loss : 0.2619 | Acc : 0.8800
     Batch 015 | Loss : 0.2896 | Acc : 0.8668
     Batch 020 | Loss : 0.3094 | Acc : 0.8586
     Batch 025 | Loss : 0.2778 | Acc : 0.8707
     Batch 030 | Loss : 0.2436 | Acc : 0.8911
     Batch 035 | Loss : 0.2796 | Acc : 0.8729
     Batch 040 | Loss : 0.2788 | Acc : 0.8704
     Batch 045 | Loss : 0.2520 | Acc : 0.8868
     Batch 050 | Loss : 0.2691 | Acc : 0.8762
     Batch 055 | Loss : 0.2370 | Acc : 0.8943
     Batch 060 | Loss : 0.2431 | Acc : 0.8891
     Batch 065 | Loss : 0.2582 | Acc : 0.8811
     Batch 070 | Loss : 0.2767 | Acc : 0.8700
     Batch 075 | Loss : 0.2885 | Acc : 0.8685
     Batch 080 | Loss : 0.2754 | Acc : 0.8706
     Batch 085 | Loss : 0.3368 | Acc : 0.8419
     Batch 090 | Loss : 0.3489 | Acc : 0.8380
     Batch 095 | Loss : 0.3265 | Acc : 0.8492
     Batch 100 | Loss : 0.2681 | Acc : 0.8772
     Batch 105 | Loss : 0.2423 | Acc : 0.8897
     Batch 110 | Loss : 0.2756 | Acc : 0.8733
     Batch 115 | Loss : 0.2459 | Acc : 0.8908
     Batch 120 | Loss : 0.2905 | Acc : 0.8649
     Batch 125 | Loss : 0.3318 | Acc : 0.8436
     Batch 130 | Loss : 0.2940 | Acc : 0.8642
     Batch 135 | Loss : 0.2887 | Acc : 0.8676
     Batch 140 | Loss : 0.3405 | Acc : 0.8407
     Batch 145 | Loss : 0.2622 | Acc : 0.8827
     Batch 150 | Loss : 0.2657 | Acc : 0.8814
Epoch 00197 | Train Loss : 0.2822 | Eval Loss : 0.3217 | Train acc : 0.8703 | Eval Acc : 0.8536 | Eval Log. Respected : 0.9399
     Batch 000 | Loss : 0.2626 | Acc : 0.8811
     Batch 005 | Loss : 0.2728 | Acc : 0.8731
     Batch 010 | Loss : 0.2876 | Acc : 0.8651
     Batch 015 | Loss : 0.3076 | Acc : 0.8565
     Batch 020 | Loss : 0.2663 | Acc : 0.8811
     Batch 025 | Loss : 0.2985 | Acc : 0.8614
     Batch 030 | Loss : 0.2805 | Acc : 0.8727
     Batch 035 | Loss : 0.3151 | Acc : 0.8540
     Batch 040 | Loss : 0.3256 | Acc : 0.8468
     Batch 045 | Loss : 0.2902 | Acc : 0.8628
     Batch 050 | Loss : 0.3064 | Acc : 0.8576
     Batch 055 | Loss : 0.3010 | Acc : 0.8579
     Batch 060 | Loss : 0.2460 | Acc : 0.8891
     Batch 065 | Loss : 0.3266 | Acc : 0.8525
     Batch 070 | Loss : 0.2726 | Acc : 0.8722
     Batch 075 | Loss : 0.3053 | Acc : 0.8562
     Batch 080 | Loss : 0.2689 | Acc : 0.8748
     Batch 085 | Loss : 0.2691 | Acc : 0.8752
     Batch 090 | Loss : 0.2974 | Acc : 0.8635
     Batch 095 | Loss : 0.3091 | Acc : 0.8584
     Batch 100 | Loss : 0.2900 | Acc : 0.8650
     Batch 105 | Loss : 0.2705 | Acc : 0.8741
     Batch 110 | Loss : 0.2862 | Acc : 0.8665
     Batch 115 | Loss : 0.3362 | Acc : 0.8449
     Batch 120 | Loss : 0.2368 | Acc : 0.8955
     Batch 125 | Loss : 0.2758 | Acc : 0.8742
     Batch 130 | Loss : 0.2698 | Acc : 0.8790
     Batch 135 | Loss : 0.2917 | Acc : 0.8670
     Batch 140 | Loss : 0.2875 | Acc : 0.8648
     Batch 145 | Loss : 0.3079 | Acc : 0.8571
     Batch 150 | Loss : 0.3229 | Acc : 0.8512
Epoch 00198 | Train Loss : 0.2812 | Eval Loss : 0.3205 | Train acc : 0.8709 | Eval Acc : 0.8522 | Eval Log. Respected : 0.9302
     Batch 000 | Loss : 0.2757 | Acc : 0.8724
     Batch 005 | Loss : 0.2585 | Acc : 0.8812
     Batch 010 | Loss : 0.2206 | Acc : 0.9012
     Batch 015 | Loss : 0.2726 | Acc : 0.8728
     Batch 020 | Loss : 0.2703 | Acc : 0.8765
     Batch 025 | Loss : 0.3250 | Acc : 0.8496
     Batch 030 | Loss : 0.2795 | Acc : 0.8721
     Batch 035 | Loss : 0.3026 | Acc : 0.8590
     Batch 040 | Loss : 0.2589 | Acc : 0.8820
     Batch 045 | Loss : 0.3051 | Acc : 0.8656
     Batch 050 | Loss : 0.2586 | Acc : 0.8811
     Batch 055 | Loss : 0.2936 | Acc : 0.8706
     Batch 060 | Loss : 0.2373 | Acc : 0.8938
     Batch 065 | Loss : 0.2250 | Acc : 0.8978
     Batch 070 | Loss : 0.2979 | Acc : 0.8647
     Batch 075 | Loss : 0.2667 | Acc : 0.8795
     Batch 080 | Loss : 0.2477 | Acc : 0.8896
     Batch 085 | Loss : 0.2871 | Acc : 0.8699
     Batch 090 | Loss : 0.3084 | Acc : 0.8567
     Batch 095 | Loss : 0.3125 | Acc : 0.8560
     Batch 100 | Loss : 0.3174 | Acc : 0.8516
     Batch 105 | Loss : 0.2803 | Acc : 0.8705
     Batch 110 | Loss : 0.2838 | Acc : 0.8687
     Batch 115 | Loss : 0.2801 | Acc : 0.8689
     Batch 120 | Loss : 0.3041 | Acc : 0.8690
     Batch 125 | Loss : 0.2716 | Acc : 0.8735
     Batch 130 | Loss : 0.2698 | Acc : 0.8778
     Batch 135 | Loss : 0.3183 | Acc : 0.8481
     Batch 140 | Loss : 0.2589 | Acc : 0.8838
     Batch 145 | Loss : 0.2801 | Acc : 0.8755
     Batch 150 | Loss : 0.2774 | Acc : 0.8707
Epoch 00199 | Train Loss : 0.2818 | Eval Loss : 0.3201 | Train acc : 0.8704 | Eval Acc : 0.8524 | Eval Log. Respected : 0.9366
Testing...
Test Loss 0.5859 | Test Acc 0.8427 | Test Log. Res. 0.9351
