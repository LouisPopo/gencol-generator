cuda is available
USING : cuda
Done loading data from cached files.
Training ...
     Batch 000 | Loss : 0.6934 | Acc : 0.4851
     Batch 005 | Loss : 0.6931 | Acc : 0.5132
     Batch 010 | Loss : 0.6929 | Acc : 0.5130
     Batch 015 | Loss : 0.6928 | Acc : 0.5132
     Batch 020 | Loss : 0.6927 | Acc : 0.5119
     Batch 025 | Loss : 0.6923 | Acc : 0.5133
     Batch 030 | Loss : 0.6913 | Acc : 0.5144
     Batch 035 | Loss : 0.6899 | Acc : 0.5120
     Batch 040 | Loss : 0.6851 | Acc : 0.5134
     Batch 045 | Loss : 0.6778 | Acc : 0.5123
     Batch 050 | Loss : 0.6596 | Acc : 0.5116
     Batch 055 | Loss : 0.6299 | Acc : 0.6815
     Batch 060 | Loss : 0.6008 | Acc : 0.7171
Epoch 00000 | Train Loss : 0.6759 | Eval Loss : 0.6094 | Train acc : 0.5405 | Eval Acc : 0.7083 | Eval Log. Respected : 0.0097
     Batch 000 | Loss : 0.5919 | Acc : 0.7288
     Batch 005 | Loss : 0.5700 | Acc : 0.7808
     Batch 010 | Loss : 0.5750 | Acc : 0.7656
     Batch 015 | Loss : 0.5369 | Acc : 0.8034
     Batch 020 | Loss : 0.5453 | Acc : 0.7877
     Batch 025 | Loss : 0.5301 | Acc : 0.7955
     Batch 030 | Loss : 0.5251 | Acc : 0.7889
     Batch 035 | Loss : 0.5090 | Acc : 0.7941
     Batch 040 | Loss : 0.4863 | Acc : 0.7917
     Batch 045 | Loss : 0.4731 | Acc : 0.7810
     Batch 050 | Loss : 0.4209 | Acc : 0.8115
     Batch 055 | Loss : 0.4211 | Acc : 0.8034
     Batch 060 | Loss : 0.4494 | Acc : 0.7861
Epoch 00001 | Train Loss : 0.5107 | Eval Loss : 0.4423 | Train acc : 0.7861 | Eval Acc : 0.7869 | Eval Log. Respected : 0.0416
     Batch 000 | Loss : 0.4310 | Acc : 0.7939
     Batch 005 | Loss : 0.4486 | Acc : 0.7876
     Batch 010 | Loss : 0.4512 | Acc : 0.7793
     Batch 015 | Loss : 0.4537 | Acc : 0.7792
     Batch 020 | Loss : 0.4038 | Acc : 0.8106
     Batch 025 | Loss : 0.4157 | Acc : 0.7983
     Batch 030 | Loss : 0.3901 | Acc : 0.8150
     Batch 035 | Loss : 0.4044 | Acc : 0.8061
     Batch 040 | Loss : 0.4452 | Acc : 0.7917
     Batch 045 | Loss : 0.3950 | Acc : 0.8115
     Batch 050 | Loss : 0.4050 | Acc : 0.8060
     Batch 055 | Loss : 0.3643 | Acc : 0.8292
     Batch 060 | Loss : 0.4219 | Acc : 0.7984
Epoch 00002 | Train Loss : 0.4128 | Eval Loss : 0.4053 | Train acc : 0.8041 | Eval Acc : 0.8051 | Eval Log. Respected : 0.9656
     Batch 000 | Loss : 0.3959 | Acc : 0.8128
     Batch 005 | Loss : 0.4443 | Acc : 0.7819
     Batch 010 | Loss : 0.3755 | Acc : 0.8234
     Batch 015 | Loss : 0.4072 | Acc : 0.8054
     Batch 020 | Loss : 0.3817 | Acc : 0.8171
     Batch 025 | Loss : 0.4199 | Acc : 0.7995
     Batch 030 | Loss : 0.3923 | Acc : 0.8181
     Batch 035 | Loss : 0.3619 | Acc : 0.8303
     Batch 040 | Loss : 0.3726 | Acc : 0.8271
     Batch 045 | Loss : 0.3629 | Acc : 0.8311
     Batch 050 | Loss : 0.3444 | Acc : 0.8400
     Batch 055 | Loss : 0.3586 | Acc : 0.8321
     Batch 060 | Loss : 0.3853 | Acc : 0.8219
Epoch 00003 | Train Loss : 0.3875 | Eval Loss : 0.3789 | Train acc : 0.8177 | Eval Acc : 0.8208 | Eval Log. Respected : 0.9867
     Batch 000 | Loss : 0.4010 | Acc : 0.8094
     Batch 005 | Loss : 0.3786 | Acc : 0.8210
     Batch 010 | Loss : 0.3855 | Acc : 0.8133
     Batch 015 | Loss : 0.3928 | Acc : 0.8104
     Batch 020 | Loss : 0.3693 | Acc : 0.8284
     Batch 025 | Loss : 0.3836 | Acc : 0.8162
     Batch 030 | Loss : 0.4564 | Acc : 0.7966
     Batch 035 | Loss : 0.3794 | Acc : 0.8198
     Batch 040 | Loss : 0.3860 | Acc : 0.8216
     Batch 045 | Loss : 0.3662 | Acc : 0.8293
     Batch 050 | Loss : 0.3412 | Acc : 0.8437
     Batch 055 | Loss : 0.3450 | Acc : 0.8410
     Batch 060 | Loss : 0.3618 | Acc : 0.8312
Epoch 00004 | Train Loss : 0.3751 | Eval Loss : 0.3751 | Train acc : 0.8249 | Eval Acc : 0.8201 | Eval Log. Respected : 0.9639
     Batch 000 | Loss : 0.3543 | Acc : 0.8358
     Batch 005 | Loss : 0.3638 | Acc : 0.8287
     Batch 010 | Loss : 0.3975 | Acc : 0.8102
     Batch 015 | Loss : 0.3767 | Acc : 0.8215
     Batch 020 | Loss : 0.3536 | Acc : 0.8343
     Batch 025 | Loss : 0.3238 | Acc : 0.8546
     Batch 030 | Loss : 0.3707 | Acc : 0.8291
     Batch 035 | Loss : 0.3752 | Acc : 0.8221
     Batch 040 | Loss : 0.3400 | Acc : 0.8437
     Batch 045 | Loss : 0.4033 | Acc : 0.8110
     Batch 050 | Loss : 0.3493 | Acc : 0.8380
     Batch 055 | Loss : 0.3690 | Acc : 0.8253
     Batch 060 | Loss : 0.3326 | Acc : 0.8456
Epoch 00005 | Train Loss : 0.3656 | Eval Loss : 0.3756 | Train acc : 0.8295 | Eval Acc : 0.8235 | Eval Log. Respected : 0.9606
     Batch 000 | Loss : 0.3475 | Acc : 0.8380
     Batch 005 | Loss : 0.3939 | Acc : 0.8098
     Batch 010 | Loss : 0.3859 | Acc : 0.8221
     Batch 015 | Loss : 0.3707 | Acc : 0.8256
     Batch 020 | Loss : 0.3562 | Acc : 0.8352
     Batch 025 | Loss : 0.3980 | Acc : 0.8116
     Batch 030 | Loss : 0.3833 | Acc : 0.8163
     Batch 035 | Loss : 0.3573 | Acc : 0.8326
     Batch 040 | Loss : 0.3356 | Acc : 0.8434
     Batch 045 | Loss : 0.4355 | Acc : 0.7898
     Batch 050 | Loss : 0.3505 | Acc : 0.8414
     Batch 055 | Loss : 0.4044 | Acc : 0.8136
     Batch 060 | Loss : 0.3793 | Acc : 0.8203
Epoch 00006 | Train Loss : 0.3632 | Eval Loss : 0.3670 | Train acc : 0.8314 | Eval Acc : 0.8253 | Eval Log. Respected : 0.9656
     Batch 000 | Loss : 0.3660 | Acc : 0.8274
     Batch 005 | Loss : 0.3228 | Acc : 0.8582
     Batch 010 | Loss : 0.3527 | Acc : 0.8424
     Batch 015 | Loss : 0.4024 | Acc : 0.8147
     Batch 020 | Loss : 0.3599 | Acc : 0.8315
     Batch 025 | Loss : 0.3649 | Acc : 0.8291
     Batch 030 | Loss : 0.3517 | Acc : 0.8362
     Batch 035 | Loss : 0.3546 | Acc : 0.8368
     Batch 040 | Loss : 0.3757 | Acc : 0.8275
     Batch 045 | Loss : 0.3559 | Acc : 0.8387
     Batch 050 | Loss : 0.3554 | Acc : 0.8347
     Batch 055 | Loss : 0.3461 | Acc : 0.8401
     Batch 060 | Loss : 0.3590 | Acc : 0.8312
Epoch 00007 | Train Loss : 0.3617 | Eval Loss : 0.3617 | Train acc : 0.8329 | Eval Acc : 0.8296 | Eval Log. Respected : 0.9710
     Batch 000 | Loss : 0.3400 | Acc : 0.8430
     Batch 005 | Loss : 0.3453 | Acc : 0.8437
     Batch 010 | Loss : 0.3641 | Acc : 0.8305
     Batch 015 | Loss : 0.3306 | Acc : 0.8485
     Batch 020 | Loss : 0.3663 | Acc : 0.8307
     Batch 025 | Loss : 0.3220 | Acc : 0.8538
     Batch 030 | Loss : 0.3117 | Acc : 0.8597
     Batch 035 | Loss : 0.3306 | Acc : 0.8485
     Batch 040 | Loss : 0.3908 | Acc : 0.8141
     Batch 045 | Loss : 0.3806 | Acc : 0.8186
     Batch 050 | Loss : 0.3310 | Acc : 0.8491
     Batch 055 | Loss : 0.3739 | Acc : 0.8244
     Batch 060 | Loss : 0.3612 | Acc : 0.8290
Epoch 00008 | Train Loss : 0.3553 | Eval Loss : 0.3596 | Train acc : 0.8357 | Eval Acc : 0.8309 | Eval Log. Respected : 0.9521
     Batch 000 | Loss : 0.3338 | Acc : 0.8501
     Batch 005 | Loss : 0.3313 | Acc : 0.8478
     Batch 010 | Loss : 0.3355 | Acc : 0.8445
     Batch 015 | Loss : 0.3671 | Acc : 0.8326
     Batch 020 | Loss : 0.3582 | Acc : 0.8313
     Batch 025 | Loss : 0.3648 | Acc : 0.8290
     Batch 030 | Loss : 0.3590 | Acc : 0.8333
     Batch 035 | Loss : 0.3505 | Acc : 0.8361
     Batch 040 | Loss : 0.3619 | Acc : 0.8304
     Batch 045 | Loss : 0.3629 | Acc : 0.8336
     Batch 050 | Loss : 0.3740 | Acc : 0.8289
     Batch 055 | Loss : 0.3673 | Acc : 0.8265
     Batch 060 | Loss : 0.3305 | Acc : 0.8525
Epoch 00009 | Train Loss : 0.3554 | Eval Loss : 0.3710 | Train acc : 0.8358 | Eval Acc : 0.8261 | Eval Log. Respected : 0.9458
     Batch 000 | Loss : 0.3496 | Acc : 0.8387
     Batch 005 | Loss : 0.3668 | Acc : 0.8272
     Batch 010 | Loss : 0.3574 | Acc : 0.8321
     Batch 015 | Loss : 0.3617 | Acc : 0.8374
     Batch 020 | Loss : 0.3458 | Acc : 0.8393
     Batch 025 | Loss : 0.3788 | Acc : 0.8227
     Batch 030 | Loss : 0.3520 | Acc : 0.8360
     Batch 035 | Loss : 0.3654 | Acc : 0.8312
     Batch 040 | Loss : 0.3494 | Acc : 0.8386
     Batch 045 | Loss : 0.3378 | Acc : 0.8449
     Batch 050 | Loss : 0.3367 | Acc : 0.8464
     Batch 055 | Loss : 0.3679 | Acc : 0.8278
     Batch 060 | Loss : 0.3915 | Acc : 0.8157
Epoch 00010 | Train Loss : 0.3552 | Eval Loss : 0.3568 | Train acc : 0.8357 | Eval Acc : 0.8336 | Eval Log. Respected : 0.9647
     Batch 000 | Loss : 0.3530 | Acc : 0.8371
     Batch 005 | Loss : 0.3261 | Acc : 0.8494
     Batch 010 | Loss : 0.3414 | Acc : 0.8437
     Batch 015 | Loss : 0.3337 | Acc : 0.8489
     Batch 020 | Loss : 0.3370 | Acc : 0.8465
     Batch 025 | Loss : 0.3883 | Acc : 0.8191
     Batch 030 | Loss : 0.3341 | Acc : 0.8473
     Batch 035 | Loss : 0.3334 | Acc : 0.8474
     Batch 040 | Loss : 0.3655 | Acc : 0.8312
     Batch 045 | Loss : 0.3293 | Acc : 0.8491
     Batch 050 | Loss : 0.3582 | Acc : 0.8313
     Batch 055 | Loss : 0.3926 | Acc : 0.8197
     Batch 060 | Loss : 0.3547 | Acc : 0.8366
Epoch 00011 | Train Loss : 0.3516 | Eval Loss : 0.3562 | Train acc : 0.8378 | Eval Acc : 0.8319 | Eval Log. Respected : 0.9691
     Batch 000 | Loss : 0.3459 | Acc : 0.8432
     Batch 005 | Loss : 0.3278 | Acc : 0.8492
     Batch 010 | Loss : 0.3155 | Acc : 0.8577
     Batch 015 | Loss : 0.3903 | Acc : 0.8183
     Batch 020 | Loss : 0.4126 | Acc : 0.8123
     Batch 025 | Loss : 0.3873 | Acc : 0.8154
     Batch 030 | Loss : 0.3355 | Acc : 0.8464
     Batch 035 | Loss : 0.3413 | Acc : 0.8451
     Batch 040 | Loss : 0.3559 | Acc : 0.8344
     Batch 045 | Loss : 0.3925 | Acc : 0.8097
     Batch 050 | Loss : 0.3864 | Acc : 0.8173
     Batch 055 | Loss : 0.3691 | Acc : 0.8276
     Batch 060 | Loss : 0.3103 | Acc : 0.8586
Epoch 00012 | Train Loss : 0.3521 | Eval Loss : 0.3556 | Train acc : 0.8374 | Eval Acc : 0.8316 | Eval Log. Respected : 0.9779
     Batch 000 | Loss : 0.3673 | Acc : 0.8250
     Batch 005 | Loss : 0.3354 | Acc : 0.8487
     Batch 010 | Loss : 0.3109 | Acc : 0.8585
     Batch 015 | Loss : 0.3245 | Acc : 0.8529
     Batch 020 | Loss : 0.3419 | Acc : 0.8410
     Batch 025 | Loss : 0.3898 | Acc : 0.8139
     Batch 030 | Loss : 0.3233 | Acc : 0.8552
     Batch 035 | Loss : 0.3409 | Acc : 0.8439
     Batch 040 | Loss : 0.3545 | Acc : 0.8369
     Batch 045 | Loss : 0.3679 | Acc : 0.8301
     Batch 050 | Loss : 0.3287 | Acc : 0.8514
     Batch 055 | Loss : 0.3425 | Acc : 0.8429
     Batch 060 | Loss : 0.3524 | Acc : 0.8385
Epoch 00013 | Train Loss : 0.3496 | Eval Loss : 0.3520 | Train acc : 0.8390 | Eval Acc : 0.8343 | Eval Log. Respected : 0.9607
     Batch 000 | Loss : 0.3313 | Acc : 0.8499
     Batch 005 | Loss : 0.3427 | Acc : 0.8443
     Batch 010 | Loss : 0.3466 | Acc : 0.8403
     Batch 015 | Loss : 0.3324 | Acc : 0.8476
     Batch 020 | Loss : 0.3546 | Acc : 0.8372
     Batch 025 | Loss : 0.3590 | Acc : 0.8324
     Batch 030 | Loss : 0.3880 | Acc : 0.8189
     Batch 035 | Loss : 0.3479 | Acc : 0.8419
     Batch 040 | Loss : 0.3347 | Acc : 0.8452
     Batch 045 | Loss : 0.3662 | Acc : 0.8291
     Batch 050 | Loss : 0.3734 | Acc : 0.8243
     Batch 055 | Loss : 0.3631 | Acc : 0.8298
     Batch 060 | Loss : 0.3406 | Acc : 0.8428
Epoch 00014 | Train Loss : 0.3486 | Eval Loss : 0.3524 | Train acc : 0.8395 | Eval Acc : 0.8348 | Eval Log. Respected : 0.9697
     Batch 000 | Loss : 0.3960 | Acc : 0.8157
     Batch 005 | Loss : 0.3374 | Acc : 0.8455
     Batch 010 | Loss : 0.3804 | Acc : 0.8250
     Batch 015 | Loss : 0.3359 | Acc : 0.8449
     Batch 020 | Loss : 0.3868 | Acc : 0.8215
     Batch 025 | Loss : 0.3328 | Acc : 0.8484
     Batch 030 | Loss : 0.3226 | Acc : 0.8509
     Batch 035 | Loss : 0.3409 | Acc : 0.8432
     Batch 040 | Loss : 0.3587 | Acc : 0.8322
     Batch 045 | Loss : 0.3359 | Acc : 0.8467
     Batch 050 | Loss : 0.3427 | Acc : 0.8401
     Batch 055 | Loss : 0.3767 | Acc : 0.8218
     Batch 060 | Loss : 0.3750 | Acc : 0.8274
Epoch 00015 | Train Loss : 0.3482 | Eval Loss : 0.3519 | Train acc : 0.8396 | Eval Acc : 0.8340 | Eval Log. Respected : 0.9552
     Batch 000 | Loss : 0.3388 | Acc : 0.8435
     Batch 005 | Loss : 0.3236 | Acc : 0.8511
     Batch 010 | Loss : 0.3588 | Acc : 0.8299
     Batch 015 | Loss : 0.3659 | Acc : 0.8273
     Batch 020 | Loss : 0.3474 | Acc : 0.8399
     Batch 025 | Loss : 0.3360 | Acc : 0.8465
     Batch 030 | Loss : 0.3519 | Acc : 0.8359
     Batch 035 | Loss : 0.3564 | Acc : 0.8349
     Batch 040 | Loss : 0.3284 | Acc : 0.8511
     Batch 045 | Loss : 0.3440 | Acc : 0.8397
     Batch 050 | Loss : 0.3712 | Acc : 0.8284
     Batch 055 | Loss : 0.3219 | Acc : 0.8543
     Batch 060 | Loss : 0.3711 | Acc : 0.8240
Epoch 00016 | Train Loss : 0.3456 | Eval Loss : 0.3507 | Train acc : 0.8407 | Eval Acc : 0.8352 | Eval Log. Respected : 0.9636
     Batch 000 | Loss : 0.3596 | Acc : 0.8313
     Batch 005 | Loss : 0.3625 | Acc : 0.8322
     Batch 010 | Loss : 0.3346 | Acc : 0.8465
     Batch 015 | Loss : 0.3266 | Acc : 0.8518
     Batch 020 | Loss : 0.3263 | Acc : 0.8504
     Batch 025 | Loss : 0.3848 | Acc : 0.8276
     Batch 030 | Loss : 0.3087 | Acc : 0.8602
     Batch 035 | Loss : 0.3420 | Acc : 0.8427
     Batch 040 | Loss : 0.3321 | Acc : 0.8479
     Batch 045 | Loss : 0.4041 | Acc : 0.8165
     Batch 050 | Loss : 0.3536 | Acc : 0.8385
     Batch 055 | Loss : 0.3398 | Acc : 0.8411
     Batch 060 | Loss : 0.3459 | Acc : 0.8418
Epoch 00017 | Train Loss : 0.3499 | Eval Loss : 0.3512 | Train acc : 0.8395 | Eval Acc : 0.8361 | Eval Log. Respected : 0.9627
     Batch 000 | Loss : 0.3572 | Acc : 0.8341
     Batch 005 | Loss : 0.3285 | Acc : 0.8508
     Batch 010 | Loss : 0.3564 | Acc : 0.8345
     Batch 015 | Loss : 0.3474 | Acc : 0.8389
     Batch 020 | Loss : 0.3269 | Acc : 0.8519
     Batch 025 | Loss : 0.3454 | Acc : 0.8421
     Batch 030 | Loss : 0.3570 | Acc : 0.8319
     Batch 035 | Loss : 0.3335 | Acc : 0.8480
     Batch 040 | Loss : 0.3560 | Acc : 0.8336
     Batch 045 | Loss : 0.3295 | Acc : 0.8494
     Batch 050 | Loss : 0.3493 | Acc : 0.8392
     Batch 055 | Loss : 0.3522 | Acc : 0.8365
     Batch 060 | Loss : 0.3240 | Acc : 0.8537
Epoch 00018 | Train Loss : 0.3464 | Eval Loss : 0.3539 | Train acc : 0.8405 | Eval Acc : 0.8330 | Eval Log. Respected : 0.9538
     Batch 000 | Loss : 0.3549 | Acc : 0.8359
     Batch 005 | Loss : 0.3336 | Acc : 0.8464
     Batch 010 | Loss : 0.3367 | Acc : 0.8468
     Batch 015 | Loss : 0.3633 | Acc : 0.8316
     Batch 020 | Loss : 0.3497 | Acc : 0.8383
     Batch 025 | Loss : 0.3278 | Acc : 0.8498
     Batch 030 | Loss : 0.3143 | Acc : 0.8569
     Batch 035 | Loss : 0.3511 | Acc : 0.8369
     Batch 040 | Loss : 0.4196 | Acc : 0.8082
     Batch 045 | Loss : 0.3249 | Acc : 0.8518
     Batch 050 | Loss : 0.3207 | Acc : 0.8524
     Batch 055 | Loss : 0.3622 | Acc : 0.8288
     Batch 060 | Loss : 0.3654 | Acc : 0.8296
Epoch 00019 | Train Loss : 0.3440 | Eval Loss : 0.3495 | Train acc : 0.8415 | Eval Acc : 0.8360 | Eval Log. Respected : 0.9527
     Batch 000 | Loss : 0.3527 | Acc : 0.8343
     Batch 005 | Loss : 0.3346 | Acc : 0.8487
     Batch 010 | Loss : 0.3505 | Acc : 0.8354
     Batch 015 | Loss : 0.3425 | Acc : 0.8443
     Batch 020 | Loss : 0.3233 | Acc : 0.8539
     Batch 025 | Loss : 0.3136 | Acc : 0.8552
     Batch 030 | Loss : 0.3680 | Acc : 0.8273
     Batch 035 | Loss : 0.3487 | Acc : 0.8402
     Batch 040 | Loss : 0.3492 | Acc : 0.8376
     Batch 045 | Loss : 0.3469 | Acc : 0.8409
     Batch 050 | Loss : 0.3166 | Acc : 0.8572
     Batch 055 | Loss : 0.3326 | Acc : 0.8452
     Batch 060 | Loss : 0.3304 | Acc : 0.8502
Epoch 00020 | Train Loss : 0.3432 | Eval Loss : 0.3502 | Train acc : 0.8420 | Eval Acc : 0.8355 | Eval Log. Respected : 0.9577
     Batch 000 | Loss : 0.3484 | Acc : 0.8387
     Batch 005 | Loss : 0.2946 | Acc : 0.8670
     Batch 010 | Loss : 0.3431 | Acc : 0.8413
     Batch 015 | Loss : 0.3479 | Acc : 0.8400
     Batch 020 | Loss : 0.4322 | Acc : 0.7953
     Batch 025 | Loss : 0.3248 | Acc : 0.8525
     Batch 030 | Loss : 0.3318 | Acc : 0.8505
     Batch 035 | Loss : 0.3464 | Acc : 0.8404
     Batch 040 | Loss : 0.3365 | Acc : 0.8432
     Batch 045 | Loss : 0.3697 | Acc : 0.8291
     Batch 050 | Loss : 0.3142 | Acc : 0.8556
     Batch 055 | Loss : 0.3357 | Acc : 0.8457
     Batch 060 | Loss : 0.3789 | Acc : 0.8241
Epoch 00021 | Train Loss : 0.3431 | Eval Loss : 0.3552 | Train acc : 0.8421 | Eval Acc : 0.8343 | Eval Log. Respected : 0.9574
     Batch 000 | Loss : 0.3247 | Acc : 0.8530
     Batch 005 | Loss : 0.3623 | Acc : 0.8324
     Batch 010 | Loss : 0.3440 | Acc : 0.8395
     Batch 015 | Loss : 0.3192 | Acc : 0.8563
     Batch 020 | Loss : 0.3437 | Acc : 0.8393
     Batch 025 | Loss : 0.3393 | Acc : 0.8440
     Batch 030 | Loss : 0.3552 | Acc : 0.8335
     Batch 035 | Loss : 0.3338 | Acc : 0.8452
     Batch 040 | Loss : 0.3102 | Acc : 0.8581
     Batch 045 | Loss : 0.3182 | Acc : 0.8578
     Batch 050 | Loss : 0.3423 | Acc : 0.8464
     Batch 055 | Loss : 0.3082 | Acc : 0.8617
     Batch 060 | Loss : 0.3523 | Acc : 0.8380
Epoch 00022 | Train Loss : 0.3457 | Eval Loss : 0.3484 | Train acc : 0.8407 | Eval Acc : 0.8358 | Eval Log. Respected : 0.9685
     Batch 000 | Loss : 0.3435 | Acc : 0.8404
     Batch 005 | Loss : 0.3130 | Acc : 0.8577
     Batch 010 | Loss : 0.3225 | Acc : 0.8534
     Batch 015 | Loss : 0.3658 | Acc : 0.8290
     Batch 020 | Loss : 0.3336 | Acc : 0.8470
     Batch 025 | Loss : 0.3161 | Acc : 0.8554
     Batch 030 | Loss : 0.3543 | Acc : 0.8369
     Batch 035 | Loss : 0.3276 | Acc : 0.8526
     Batch 040 | Loss : 0.3735 | Acc : 0.8228
     Batch 045 | Loss : 0.3021 | Acc : 0.8651
     Batch 050 | Loss : 0.3185 | Acc : 0.8545
     Batch 055 | Loss : 0.3181 | Acc : 0.8555
     Batch 060 | Loss : 0.3653 | Acc : 0.8316
Epoch 00023 | Train Loss : 0.3416 | Eval Loss : 0.3524 | Train acc : 0.8427 | Eval Acc : 0.8335 | Eval Log. Respected : 0.9495
     Batch 000 | Loss : 0.3492 | Acc : 0.8394
     Batch 005 | Loss : 0.3184 | Acc : 0.8539
     Batch 010 | Loss : 0.3384 | Acc : 0.8439
     Batch 015 | Loss : 0.3311 | Acc : 0.8488
     Batch 020 | Loss : 0.3431 | Acc : 0.8375
     Batch 025 | Loss : 0.3610 | Acc : 0.8294
     Batch 030 | Loss : 0.3547 | Acc : 0.8364
     Batch 035 | Loss : 0.3403 | Acc : 0.8425
     Batch 040 | Loss : 0.3770 | Acc : 0.8258
     Batch 045 | Loss : 0.3554 | Acc : 0.8342
     Batch 050 | Loss : 0.3366 | Acc : 0.8453
     Batch 055 | Loss : 0.3591 | Acc : 0.8323
     Batch 060 | Loss : 0.3517 | Acc : 0.8382
Epoch 00024 | Train Loss : 0.3428 | Eval Loss : 0.3509 | Train acc : 0.8419 | Eval Acc : 0.8345 | Eval Log. Respected : 0.9641
     Batch 000 | Loss : 0.3589 | Acc : 0.8291
     Batch 005 | Loss : 0.3285 | Acc : 0.8494
     Batch 010 | Loss : 0.3629 | Acc : 0.8321
     Batch 015 | Loss : 0.3629 | Acc : 0.8301
     Batch 020 | Loss : 0.3113 | Acc : 0.8576
     Batch 025 | Loss : 0.3478 | Acc : 0.8394
     Batch 030 | Loss : 0.3504 | Acc : 0.8399
     Batch 035 | Loss : 0.3067 | Acc : 0.8593
     Batch 040 | Loss : 0.3455 | Acc : 0.8382
     Batch 045 | Loss : 0.3241 | Acc : 0.8537
     Batch 050 | Loss : 0.3530 | Acc : 0.8359
     Batch 055 | Loss : 0.3522 | Acc : 0.8312
     Batch 060 | Loss : 0.3443 | Acc : 0.8399
Epoch 00025 | Train Loss : 0.3431 | Eval Loss : 0.3440 | Train acc : 0.8420 | Eval Acc : 0.8388 | Eval Log. Respected : 0.9498
     Batch 000 | Loss : 0.3365 | Acc : 0.8451
     Batch 005 | Loss : 0.3363 | Acc : 0.8429
     Batch 010 | Loss : 0.3412 | Acc : 0.8411
     Batch 015 | Loss : 0.2903 | Acc : 0.8717
     Batch 020 | Loss : 0.3659 | Acc : 0.8284
     Batch 025 | Loss : 0.3560 | Acc : 0.8360
     Batch 030 | Loss : 0.3058 | Acc : 0.8616
     Batch 035 | Loss : 0.3559 | Acc : 0.8332
     Batch 040 | Loss : 0.3431 | Acc : 0.8422
     Batch 045 | Loss : 0.3236 | Acc : 0.8493
     Batch 050 | Loss : 0.3463 | Acc : 0.8383
     Batch 055 | Loss : 0.3450 | Acc : 0.8387
     Batch 060 | Loss : 0.3323 | Acc : 0.8496
Epoch 00026 | Train Loss : 0.3392 | Eval Loss : 0.3420 | Train acc : 0.8437 | Eval Acc : 0.8398 | Eval Log. Respected : 0.9646
     Batch 000 | Loss : 0.3100 | Acc : 0.8602
     Batch 005 | Loss : 0.3108 | Acc : 0.8575
     Batch 010 | Loss : 0.3614 | Acc : 0.8302
     Batch 015 | Loss : 0.3847 | Acc : 0.8180
     Batch 020 | Loss : 0.3528 | Acc : 0.8395
     Batch 025 | Loss : 0.3534 | Acc : 0.8371
     Batch 030 | Loss : 0.3454 | Acc : 0.8404
     Batch 035 | Loss : 0.3419 | Acc : 0.8436
     Batch 040 | Loss : 0.3282 | Acc : 0.8503
     Batch 045 | Loss : 0.3213 | Acc : 0.8534
     Batch 050 | Loss : 0.3062 | Acc : 0.8599
     Batch 055 | Loss : 0.3368 | Acc : 0.8437
     Batch 060 | Loss : 0.3437 | Acc : 0.8406
Epoch 00027 | Train Loss : 0.3398 | Eval Loss : 0.3428 | Train acc : 0.8434 | Eval Acc : 0.8397 | Eval Log. Respected : 0.9576
     Batch 000 | Loss : 0.3263 | Acc : 0.8491
     Batch 005 | Loss : 0.3457 | Acc : 0.8406
     Batch 010 | Loss : 0.3330 | Acc : 0.8472
     Batch 015 | Loss : 0.3504 | Acc : 0.8393
     Batch 020 | Loss : 0.3449 | Acc : 0.8394
     Batch 025 | Loss : 0.3348 | Acc : 0.8455
     Batch 030 | Loss : 0.3536 | Acc : 0.8360
     Batch 035 | Loss : 0.3399 | Acc : 0.8421
     Batch 040 | Loss : 0.3526 | Acc : 0.8358
     Batch 045 | Loss : 0.3284 | Acc : 0.8516
     Batch 050 | Loss : 0.3530 | Acc : 0.8367
     Batch 055 | Loss : 0.3275 | Acc : 0.8490
     Batch 060 | Loss : 0.3419 | Acc : 0.8416
Epoch 00028 | Train Loss : 0.3404 | Eval Loss : 0.3454 | Train acc : 0.8432 | Eval Acc : 0.8379 | Eval Log. Respected : 0.9632
     Batch 000 | Loss : 0.3421 | Acc : 0.8389
     Batch 005 | Loss : 0.3833 | Acc : 0.8244
     Batch 010 | Loss : 0.3347 | Acc : 0.8444
     Batch 015 | Loss : 0.3318 | Acc : 0.8484
     Batch 020 | Loss : 0.3365 | Acc : 0.8443
     Batch 025 | Loss : 0.3197 | Acc : 0.8532
     Batch 030 | Loss : 0.3261 | Acc : 0.8507
     Batch 035 | Loss : 0.3384 | Acc : 0.8415
     Batch 040 | Loss : 0.3477 | Acc : 0.8401
     Batch 045 | Loss : 0.3098 | Acc : 0.8633
     Batch 050 | Loss : 0.3054 | Acc : 0.8606
     Batch 055 | Loss : 0.3074 | Acc : 0.8612
     Batch 060 | Loss : 0.3590 | Acc : 0.8310
Epoch 00029 | Train Loss : 0.3388 | Eval Loss : 0.3416 | Train acc : 0.8438 | Eval Acc : 0.8407 | Eval Log. Respected : 0.9492
     Batch 000 | Loss : 0.3036 | Acc : 0.8632
     Batch 005 | Loss : 0.3304 | Acc : 0.8483
     Batch 010 | Loss : 0.3270 | Acc : 0.8477
     Batch 015 | Loss : 0.3764 | Acc : 0.8267
     Batch 020 | Loss : 0.3184 | Acc : 0.8579
     Batch 025 | Loss : 0.3690 | Acc : 0.8307
     Batch 030 | Loss : 0.3597 | Acc : 0.8312
     Batch 035 | Loss : 0.3526 | Acc : 0.8390
     Batch 040 | Loss : 0.3286 | Acc : 0.8480
     Batch 045 | Loss : 0.3577 | Acc : 0.8311
     Batch 050 | Loss : 0.3243 | Acc : 0.8507
     Batch 055 | Loss : 0.3485 | Acc : 0.8390
     Batch 060 | Loss : 0.3516 | Acc : 0.8338
Epoch 00030 | Train Loss : 0.3418 | Eval Loss : 0.3493 | Train acc : 0.8428 | Eval Acc : 0.8349 | Eval Log. Respected : 0.9619
     Batch 000 | Loss : 0.3582 | Acc : 0.8347
     Batch 005 | Loss : 0.3231 | Acc : 0.8516
     Batch 010 | Loss : 0.3040 | Acc : 0.8609
     Batch 015 | Loss : 0.3225 | Acc : 0.8536
     Batch 020 | Loss : 0.3133 | Acc : 0.8599
     Batch 025 | Loss : 0.3486 | Acc : 0.8396
     Batch 030 | Loss : 0.3476 | Acc : 0.8362
     Batch 035 | Loss : 0.3780 | Acc : 0.8353
     Batch 040 | Loss : 0.3498 | Acc : 0.8380
     Batch 045 | Loss : 0.3190 | Acc : 0.8531
     Batch 050 | Loss : 0.3573 | Acc : 0.8310
     Batch 055 | Loss : 0.3495 | Acc : 0.8343
     Batch 060 | Loss : 0.3888 | Acc : 0.8230
Epoch 00031 | Train Loss : 0.3398 | Eval Loss : 0.3437 | Train acc : 0.8436 | Eval Acc : 0.8384 | Eval Log. Respected : 0.9635
     Batch 000 | Loss : 0.3635 | Acc : 0.8287
     Batch 005 | Loss : 0.3538 | Acc : 0.8350
     Batch 010 | Loss : 0.3178 | Acc : 0.8560
     Batch 015 | Loss : 0.3512 | Acc : 0.8372
     Batch 020 | Loss : 0.3385 | Acc : 0.8427
     Batch 025 | Loss : 0.3061 | Acc : 0.8605
     Batch 030 | Loss : 0.3649 | Acc : 0.8262
     Batch 035 | Loss : 0.3136 | Acc : 0.8613
     Batch 040 | Loss : 0.3706 | Acc : 0.8283
     Batch 045 | Loss : 0.3579 | Acc : 0.8306
     Batch 050 | Loss : 0.3226 | Acc : 0.8517
     Batch 055 | Loss : 0.3117 | Acc : 0.8582
     Batch 060 | Loss : 0.3017 | Acc : 0.8665
Epoch 00032 | Train Loss : 0.3373 | Eval Loss : 0.3420 | Train acc : 0.8445 | Eval Acc : 0.8396 | Eval Log. Respected : 0.9565
     Batch 000 | Loss : 0.3507 | Acc : 0.8369
     Batch 005 | Loss : 0.3334 | Acc : 0.8477
     Batch 010 | Loss : 0.3420 | Acc : 0.8422
     Batch 015 | Loss : 0.3526 | Acc : 0.8362
     Batch 020 | Loss : 0.3305 | Acc : 0.8496
     Batch 025 | Loss : 0.3353 | Acc : 0.8467
     Batch 030 | Loss : 0.3112 | Acc : 0.8587
     Batch 035 | Loss : 0.3216 | Acc : 0.8521
     Batch 040 | Loss : 0.3334 | Acc : 0.8452
     Batch 045 | Loss : 0.3203 | Acc : 0.8519
     Batch 050 | Loss : 0.3374 | Acc : 0.8445
     Batch 055 | Loss : 0.3277 | Acc : 0.8516
     Batch 060 | Loss : 0.3161 | Acc : 0.8562
Epoch 00033 | Train Loss : 0.3366 | Eval Loss : 0.3405 | Train acc : 0.8449 | Eval Acc : 0.8407 | Eval Log. Respected : 0.9620
     Batch 000 | Loss : 0.3426 | Acc : 0.8397
     Batch 005 | Loss : 0.3102 | Acc : 0.8572
     Batch 010 | Loss : 0.3046 | Acc : 0.8627
     Batch 015 | Loss : 0.3507 | Acc : 0.8354
     Batch 020 | Loss : 0.3454 | Acc : 0.8376
     Batch 025 | Loss : 0.3276 | Acc : 0.8475
     Batch 030 | Loss : 0.3680 | Acc : 0.8260
     Batch 035 | Loss : 0.3041 | Acc : 0.8611
     Batch 040 | Loss : 0.3045 | Acc : 0.8626
     Batch 045 | Loss : 0.3203 | Acc : 0.8544
     Batch 050 | Loss : 0.3280 | Acc : 0.8519
     Batch 055 | Loss : 0.3028 | Acc : 0.8637
     Batch 060 | Loss : 0.3784 | Acc : 0.8259
Epoch 00034 | Train Loss : 0.3359 | Eval Loss : 0.3447 | Train acc : 0.8452 | Eval Acc : 0.8363 | Eval Log. Respected : 0.9599
     Batch 000 | Loss : 0.3357 | Acc : 0.8464
     Batch 005 | Loss : 0.3784 | Acc : 0.8220
     Batch 010 | Loss : 0.3654 | Acc : 0.8261
     Batch 015 | Loss : 0.3549 | Acc : 0.8359
     Batch 020 | Loss : 0.3308 | Acc : 0.8485
     Batch 025 | Loss : 0.3566 | Acc : 0.8403
     Batch 030 | Loss : 0.3174 | Acc : 0.8574
     Batch 035 | Loss : 0.3147 | Acc : 0.8568
     Batch 040 | Loss : 0.3210 | Acc : 0.8513
     Batch 045 | Loss : 0.3524 | Acc : 0.8340
     Batch 050 | Loss : 0.3399 | Acc : 0.8447
     Batch 055 | Loss : 0.3307 | Acc : 0.8457
     Batch 060 | Loss : 0.3490 | Acc : 0.8366
Epoch 00035 | Train Loss : 0.3353 | Eval Loss : 0.3397 | Train acc : 0.8454 | Eval Acc : 0.8410 | Eval Log. Respected : 0.9623
     Batch 000 | Loss : 0.3692 | Acc : 0.8290
     Batch 005 | Loss : 0.3605 | Acc : 0.8310
     Batch 010 | Loss : 0.3491 | Acc : 0.8409
     Batch 015 | Loss : 0.3147 | Acc : 0.8573
     Batch 020 | Loss : 0.3503 | Acc : 0.8388
     Batch 025 | Loss : 0.3322 | Acc : 0.8457
     Batch 030 | Loss : 0.3093 | Acc : 0.8587
     Batch 035 | Loss : 0.3385 | Acc : 0.8428
     Batch 040 | Loss : 0.3421 | Acc : 0.8414
     Batch 045 | Loss : 0.3364 | Acc : 0.8443
     Batch 050 | Loss : 0.3557 | Acc : 0.8390
     Batch 055 | Loss : 0.3096 | Acc : 0.8572
     Batch 060 | Loss : 0.3018 | Acc : 0.8627
Epoch 00036 | Train Loss : 0.3343 | Eval Loss : 0.3481 | Train acc : 0.8459 | Eval Acc : 0.8373 | Eval Log. Respected : 0.9542
     Batch 000 | Loss : 0.3373 | Acc : 0.8456
     Batch 005 | Loss : 0.3191 | Acc : 0.8522
     Batch 010 | Loss : 0.3542 | Acc : 0.8370
     Batch 015 | Loss : 0.3176 | Acc : 0.8543
     Batch 020 | Loss : 0.3723 | Acc : 0.8282
     Batch 025 | Loss : 0.3372 | Acc : 0.8440
     Batch 030 | Loss : 0.3413 | Acc : 0.8425
     Batch 035 | Loss : 0.3585 | Acc : 0.8364
     Batch 040 | Loss : 0.3142 | Acc : 0.8578
     Batch 045 | Loss : 0.3348 | Acc : 0.8466
     Batch 050 | Loss : 0.3201 | Acc : 0.8504
     Batch 055 | Loss : 0.3607 | Acc : 0.8304
     Batch 060 | Loss : 0.3403 | Acc : 0.8434
Epoch 00037 | Train Loss : 0.3364 | Eval Loss : 0.3411 | Train acc : 0.8449 | Eval Acc : 0.8395 | Eval Log. Respected : 0.9489
     Batch 000 | Loss : 0.3282 | Acc : 0.8518
     Batch 005 | Loss : 0.3276 | Acc : 0.8487
     Batch 010 | Loss : 0.3477 | Acc : 0.8361
     Batch 015 | Loss : 0.3326 | Acc : 0.8446
     Batch 020 | Loss : 0.3325 | Acc : 0.8469
     Batch 025 | Loss : 0.2928 | Acc : 0.8675
     Batch 030 | Loss : 0.3332 | Acc : 0.8478
     Batch 035 | Loss : 0.3149 | Acc : 0.8531
     Batch 040 | Loss : 0.3457 | Acc : 0.8373
     Batch 045 | Loss : 0.3242 | Acc : 0.8529
     Batch 050 | Loss : 0.3673 | Acc : 0.8288
     Batch 055 | Loss : 0.3489 | Acc : 0.8364
     Batch 060 | Loss : 0.3283 | Acc : 0.8493
Epoch 00038 | Train Loss : 0.3348 | Eval Loss : 0.3496 | Train acc : 0.8455 | Eval Acc : 0.8371 | Eval Log. Respected : 0.9437
     Batch 000 | Loss : 0.2930 | Acc : 0.8676
     Batch 005 | Loss : 0.3205 | Acc : 0.8523
     Batch 010 | Loss : 0.3446 | Acc : 0.8395
     Batch 015 | Loss : 0.3148 | Acc : 0.8580
     Batch 020 | Loss : 0.3312 | Acc : 0.8457
     Batch 025 | Loss : 0.3298 | Acc : 0.8466
     Batch 030 | Loss : 0.3808 | Acc : 0.8265
     Batch 035 | Loss : 0.3431 | Acc : 0.8417
     Batch 040 | Loss : 0.3282 | Acc : 0.8488
     Batch 045 | Loss : 0.3255 | Acc : 0.8522
     Batch 050 | Loss : 0.3290 | Acc : 0.8475
     Batch 055 | Loss : 0.3792 | Acc : 0.8235
     Batch 060 | Loss : 0.3233 | Acc : 0.8504
Epoch 00039 | Train Loss : 0.3352 | Eval Loss : 0.3407 | Train acc : 0.8455 | Eval Acc : 0.8405 | Eval Log. Respected : 0.9561
     Batch 000 | Loss : 0.3713 | Acc : 0.8257
     Batch 005 | Loss : 0.3536 | Acc : 0.8338
     Batch 010 | Loss : 0.3120 | Acc : 0.8581
     Batch 015 | Loss : 0.3458 | Acc : 0.8399
     Batch 020 | Loss : 0.3350 | Acc : 0.8460
     Batch 025 | Loss : 0.2945 | Acc : 0.8666
     Batch 030 | Loss : 0.3211 | Acc : 0.8529
     Batch 035 | Loss : 0.3365 | Acc : 0.8450
     Batch 040 | Loss : 0.3228 | Acc : 0.8514
     Batch 045 | Loss : 0.3587 | Acc : 0.8356
     Batch 050 | Loss : 0.3158 | Acc : 0.8547
     Batch 055 | Loss : 0.3684 | Acc : 0.8290
     Batch 060 | Loss : 0.3535 | Acc : 0.8363
Epoch 00040 | Train Loss : 0.3329 | Eval Loss : 0.3375 | Train acc : 0.8466 | Eval Acc : 0.8416 | Eval Log. Respected : 0.9477
     Batch 000 | Loss : 0.3719 | Acc : 0.8261
     Batch 005 | Loss : 0.3282 | Acc : 0.8456
     Batch 010 | Loss : 0.3210 | Acc : 0.8543
     Batch 015 | Loss : 0.3179 | Acc : 0.8525
     Batch 020 | Loss : 0.3371 | Acc : 0.8495
     Batch 025 | Loss : 0.3122 | Acc : 0.8575
     Batch 030 | Loss : 0.3542 | Acc : 0.8316
     Batch 035 | Loss : 0.3630 | Acc : 0.8269
     Batch 040 | Loss : 0.3173 | Acc : 0.8551
     Batch 045 | Loss : 0.3248 | Acc : 0.8495
     Batch 050 | Loss : 0.3525 | Acc : 0.8382
     Batch 055 | Loss : 0.3501 | Acc : 0.8345
     Batch 060 | Loss : 0.3245 | Acc : 0.8518
Epoch 00041 | Train Loss : 0.3340 | Eval Loss : 0.3456 | Train acc : 0.8463 | Eval Acc : 0.8381 | Eval Log. Respected : 0.9367
     Batch 000 | Loss : 0.3311 | Acc : 0.8471
     Batch 005 | Loss : 0.3192 | Acc : 0.8534
     Batch 010 | Loss : 0.2916 | Acc : 0.8696
     Batch 015 | Loss : 0.3774 | Acc : 0.8254
     Batch 020 | Loss : 0.3235 | Acc : 0.8476
     Batch 025 | Loss : 0.3683 | Acc : 0.8300
     Batch 030 | Loss : 0.3371 | Acc : 0.8457
     Batch 035 | Loss : 0.3304 | Acc : 0.8476
     Batch 040 | Loss : 0.3020 | Acc : 0.8621
     Batch 045 | Loss : 0.3253 | Acc : 0.8486
     Batch 050 | Loss : 0.3658 | Acc : 0.8309
     Batch 055 | Loss : 0.3782 | Acc : 0.8256
     Batch 060 | Loss : 0.3189 | Acc : 0.8538
Epoch 00042 | Train Loss : 0.3348 | Eval Loss : 0.3387 | Train acc : 0.8458 | Eval Acc : 0.8410 | Eval Log. Respected : 0.9438
     Batch 000 | Loss : 0.3544 | Acc : 0.8346
     Batch 005 | Loss : 0.3415 | Acc : 0.8413
     Batch 010 | Loss : 0.3417 | Acc : 0.8405
     Batch 015 | Loss : 0.3290 | Acc : 0.8489
     Batch 020 | Loss : 0.3080 | Acc : 0.8594
     Batch 025 | Loss : 0.3329 | Acc : 0.8432
     Batch 030 | Loss : 0.3276 | Acc : 0.8494
     Batch 035 | Loss : 0.2976 | Acc : 0.8669
     Batch 040 | Loss : 0.3241 | Acc : 0.8529
     Batch 045 | Loss : 0.3516 | Acc : 0.8395
     Batch 050 | Loss : 0.3228 | Acc : 0.8509
     Batch 055 | Loss : 0.3296 | Acc : 0.8510
     Batch 060 | Loss : 0.3477 | Acc : 0.8383
Epoch 00043 | Train Loss : 0.3323 | Eval Loss : 0.3432 | Train acc : 0.8467 | Eval Acc : 0.8373 | Eval Log. Respected : 0.9562
     Batch 000 | Loss : 0.3268 | Acc : 0.8502
     Batch 005 | Loss : 0.3346 | Acc : 0.8425
     Batch 010 | Loss : 0.3406 | Acc : 0.8453
     Batch 015 | Loss : 0.3364 | Acc : 0.8430
     Batch 020 | Loss : 0.2901 | Acc : 0.8682
     Batch 025 | Loss : 0.3800 | Acc : 0.8280
     Batch 030 | Loss : 0.3358 | Acc : 0.8453
     Batch 035 | Loss : 0.3413 | Acc : 0.8413
     Batch 040 | Loss : 0.3274 | Acc : 0.8481
     Batch 045 | Loss : 0.2881 | Acc : 0.8740
     Batch 050 | Loss : 0.3042 | Acc : 0.8600
     Batch 055 | Loss : 0.3136 | Acc : 0.8547
     Batch 060 | Loss : 0.3327 | Acc : 0.8489
Epoch 00044 | Train Loss : 0.3339 | Eval Loss : 0.3396 | Train acc : 0.8462 | Eval Acc : 0.8411 | Eval Log. Respected : 0.9388
     Batch 000 | Loss : 0.3555 | Acc : 0.8358
     Batch 005 | Loss : 0.3442 | Acc : 0.8448
     Batch 010 | Loss : 0.3387 | Acc : 0.8428
     Batch 015 | Loss : 0.3511 | Acc : 0.8370
     Batch 020 | Loss : 0.3177 | Acc : 0.8518
     Batch 025 | Loss : 0.3220 | Acc : 0.8539
     Batch 030 | Loss : 0.3499 | Acc : 0.8378
     Batch 035 | Loss : 0.3524 | Acc : 0.8356
     Batch 040 | Loss : 0.3201 | Acc : 0.8533
     Batch 045 | Loss : 0.3303 | Acc : 0.8492
     Batch 050 | Loss : 0.3065 | Acc : 0.8590
     Batch 055 | Loss : 0.3331 | Acc : 0.8456
     Batch 060 | Loss : 0.3253 | Acc : 0.8502
Epoch 00045 | Train Loss : 0.3310 | Eval Loss : 0.3366 | Train acc : 0.8475 | Eval Acc : 0.8423 | Eval Log. Respected : 0.9378
     Batch 000 | Loss : 0.3289 | Acc : 0.8480
     Batch 005 | Loss : 0.3416 | Acc : 0.8425
     Batch 010 | Loss : 0.3436 | Acc : 0.8409
     Batch 015 | Loss : 0.3359 | Acc : 0.8484
     Batch 020 | Loss : 0.3314 | Acc : 0.8455
     Batch 025 | Loss : 0.3661 | Acc : 0.8279
     Batch 030 | Loss : 0.3328 | Acc : 0.8451
     Batch 035 | Loss : 0.3190 | Acc : 0.8532
     Batch 040 | Loss : 0.3119 | Acc : 0.8555
     Batch 045 | Loss : 0.3494 | Acc : 0.8360
     Batch 050 | Loss : 0.3329 | Acc : 0.8475
     Batch 055 | Loss : 0.3360 | Acc : 0.8429
     Batch 060 | Loss : 0.3449 | Acc : 0.8386
Epoch 00046 | Train Loss : 0.3311 | Eval Loss : 0.3449 | Train acc : 0.8475 | Eval Acc : 0.8377 | Eval Log. Respected : 0.9299
     Batch 000 | Loss : 0.3318 | Acc : 0.8456
     Batch 005 | Loss : 0.3669 | Acc : 0.8282
     Batch 010 | Loss : 0.3230 | Acc : 0.8501
     Batch 015 | Loss : 0.3470 | Acc : 0.8375
     Batch 020 | Loss : 0.3867 | Acc : 0.8197
     Batch 025 | Loss : 0.3124 | Acc : 0.8565
     Batch 030 | Loss : 0.3598 | Acc : 0.8328
     Batch 035 | Loss : 0.3360 | Acc : 0.8427
     Batch 040 | Loss : 0.3169 | Acc : 0.8557
     Batch 045 | Loss : 0.3348 | Acc : 0.8466
     Batch 050 | Loss : 0.3285 | Acc : 0.8480
     Batch 055 | Loss : 0.3044 | Acc : 0.8625
     Batch 060 | Loss : 0.3027 | Acc : 0.8618
Epoch 00047 | Train Loss : 0.3320 | Eval Loss : 0.3441 | Train acc : 0.8469 | Eval Acc : 0.8412 | Eval Log. Respected : 0.9467
     Batch 000 | Loss : 0.3403 | Acc : 0.8432
     Batch 005 | Loss : 0.3195 | Acc : 0.8543
     Batch 010 | Loss : 0.3197 | Acc : 0.8538
     Batch 015 | Loss : 0.3123 | Acc : 0.8539
     Batch 020 | Loss : 0.3545 | Acc : 0.8394
     Batch 025 | Loss : 0.3504 | Acc : 0.8364
     Batch 030 | Loss : 0.3301 | Acc : 0.8496
     Batch 035 | Loss : 0.3372 | Acc : 0.8427
     Batch 040 | Loss : 0.3341 | Acc : 0.8473
     Batch 045 | Loss : 0.3108 | Acc : 0.8565
     Batch 050 | Loss : 0.3024 | Acc : 0.8650
     Batch 055 | Loss : 0.3305 | Acc : 0.8448
     Batch 060 | Loss : 0.3504 | Acc : 0.8446
Epoch 00048 | Train Loss : 0.3309 | Eval Loss : 0.3350 | Train acc : 0.8477 | Eval Acc : 0.8431 | Eval Log. Respected : 0.9364
     Batch 000 | Loss : 0.3021 | Acc : 0.8655
     Batch 005 | Loss : 0.3099 | Acc : 0.8605
     Batch 010 | Loss : 0.3343 | Acc : 0.8459
     Batch 015 | Loss : 0.3218 | Acc : 0.8534
     Batch 020 | Loss : 0.3735 | Acc : 0.8280
     Batch 025 | Loss : 0.3223 | Acc : 0.8511
     Batch 030 | Loss : 0.3114 | Acc : 0.8597
     Batch 035 | Loss : 0.3255 | Acc : 0.8500
     Batch 040 | Loss : 0.3092 | Acc : 0.8564
     Batch 045 | Loss : 0.3428 | Acc : 0.8381
     Batch 050 | Loss : 0.3329 | Acc : 0.8451
     Batch 055 | Loss : 0.3581 | Acc : 0.8391
     Batch 060 | Loss : 0.3606 | Acc : 0.8325
Epoch 00049 | Train Loss : 0.3295 | Eval Loss : 0.3384 | Train acc : 0.8482 | Eval Acc : 0.8405 | Eval Log. Respected : 0.9429
     Batch 000 | Loss : 0.3286 | Acc : 0.8486
     Batch 005 | Loss : 0.3416 | Acc : 0.8406
     Batch 010 | Loss : 0.3267 | Acc : 0.8488
     Batch 015 | Loss : 0.3275 | Acc : 0.8491
     Batch 020 | Loss : 0.3330 | Acc : 0.8472
     Batch 025 | Loss : 0.2989 | Acc : 0.8641
     Batch 030 | Loss : 0.3191 | Acc : 0.8548
     Batch 035 | Loss : 0.2945 | Acc : 0.8633
     Batch 040 | Loss : 0.3248 | Acc : 0.8503
     Batch 045 | Loss : 0.3580 | Acc : 0.8317
     Batch 050 | Loss : 0.3228 | Acc : 0.8506
     Batch 055 | Loss : 0.3305 | Acc : 0.8470
     Batch 060 | Loss : 0.3332 | Acc : 0.8488
Epoch 00050 | Train Loss : 0.3300 | Eval Loss : 0.3393 | Train acc : 0.8478 | Eval Acc : 0.8411 | Eval Log. Respected : 0.9236
     Batch 000 | Loss : 0.3325 | Acc : 0.8476
     Batch 005 | Loss : 0.3489 | Acc : 0.8373
     Batch 010 | Loss : 0.3482 | Acc : 0.8356
     Batch 015 | Loss : 0.3353 | Acc : 0.8459
     Batch 020 | Loss : 0.3394 | Acc : 0.8457
     Batch 025 | Loss : 0.2918 | Acc : 0.8679
     Batch 030 | Loss : 0.3507 | Acc : 0.8400
     Batch 035 | Loss : 0.3320 | Acc : 0.8467
     Batch 040 | Loss : 0.2971 | Acc : 0.8649
     Batch 045 | Loss : 0.3172 | Acc : 0.8558
     Batch 050 | Loss : 0.3362 | Acc : 0.8439
     Batch 055 | Loss : 0.3246 | Acc : 0.8517
     Batch 060 | Loss : 0.3465 | Acc : 0.8374
Epoch 00051 | Train Loss : 0.3307 | Eval Loss : 0.3376 | Train acc : 0.8477 | Eval Acc : 0.8416 | Eval Log. Respected : 0.9313
     Batch 000 | Loss : 0.3296 | Acc : 0.8505
     Batch 005 | Loss : 0.3423 | Acc : 0.8389
     Batch 010 | Loss : 0.3181 | Acc : 0.8540
     Batch 015 | Loss : 0.2930 | Acc : 0.8682
     Batch 020 | Loss : 0.3305 | Acc : 0.8473
     Batch 025 | Loss : 0.3704 | Acc : 0.8271
     Batch 030 | Loss : 0.3467 | Acc : 0.8385
     Batch 035 | Loss : 0.3407 | Acc : 0.8408
     Batch 040 | Loss : 0.3369 | Acc : 0.8439
     Batch 045 | Loss : 0.3402 | Acc : 0.8439
     Batch 050 | Loss : 0.3197 | Acc : 0.8537
     Batch 055 | Loss : 0.2967 | Acc : 0.8645
     Batch 060 | Loss : 0.3246 | Acc : 0.8519
Epoch 00052 | Train Loss : 0.3307 | Eval Loss : 0.3493 | Train acc : 0.8475 | Eval Acc : 0.8344 | Eval Log. Respected : 0.9498
     Batch 000 | Loss : 0.3376 | Acc : 0.8422
     Batch 005 | Loss : 0.3298 | Acc : 0.8482
     Batch 010 | Loss : 0.3135 | Acc : 0.8576
     Batch 015 | Loss : 0.3010 | Acc : 0.8635
     Batch 020 | Loss : 0.3622 | Acc : 0.8301
     Batch 025 | Loss : 0.3148 | Acc : 0.8538
     Batch 030 | Loss : 0.3730 | Acc : 0.8317
     Batch 035 | Loss : 0.3489 | Acc : 0.8385
     Batch 040 | Loss : 0.3089 | Acc : 0.8578
     Batch 045 | Loss : 0.3289 | Acc : 0.8467
     Batch 050 | Loss : 0.3302 | Acc : 0.8446
     Batch 055 | Loss : 0.3462 | Acc : 0.8420
     Batch 060 | Loss : 0.3100 | Acc : 0.8612
Epoch 00053 | Train Loss : 0.3303 | Eval Loss : 0.3358 | Train acc : 0.8479 | Eval Acc : 0.8424 | Eval Log. Respected : 0.9246
     Batch 000 | Loss : 0.3038 | Acc : 0.8637
     Batch 005 | Loss : 0.2964 | Acc : 0.8634
     Batch 010 | Loss : 0.3662 | Acc : 0.8345
     Batch 015 | Loss : 0.3208 | Acc : 0.8542
     Batch 020 | Loss : 0.3361 | Acc : 0.8462
     Batch 025 | Loss : 0.3252 | Acc : 0.8469
     Batch 030 | Loss : 0.3334 | Acc : 0.8473
     Batch 035 | Loss : 0.3457 | Acc : 0.8396
     Batch 040 | Loss : 0.3187 | Acc : 0.8532
     Batch 045 | Loss : 0.3251 | Acc : 0.8507
     Batch 050 | Loss : 0.3688 | Acc : 0.8253
     Batch 055 | Loss : 0.3464 | Acc : 0.8449
     Batch 060 | Loss : 0.3208 | Acc : 0.8520
Epoch 00054 | Train Loss : 0.3306 | Eval Loss : 0.3353 | Train acc : 0.8478 | Eval Acc : 0.8422 | Eval Log. Respected : 0.9430
     Batch 000 | Loss : 0.3048 | Acc : 0.8638
     Batch 005 | Loss : 0.3430 | Acc : 0.8417
     Batch 010 | Loss : 0.3444 | Acc : 0.8378
     Batch 015 | Loss : 0.3052 | Acc : 0.8611
     Batch 020 | Loss : 0.3119 | Acc : 0.8578
     Batch 025 | Loss : 0.3033 | Acc : 0.8603
     Batch 030 | Loss : 0.2977 | Acc : 0.8654
     Batch 035 | Loss : 0.3034 | Acc : 0.8605
     Batch 040 | Loss : 0.3135 | Acc : 0.8587
     Batch 045 | Loss : 0.3341 | Acc : 0.8454
     Batch 050 | Loss : 0.3245 | Acc : 0.8515
     Batch 055 | Loss : 0.3223 | Acc : 0.8516
     Batch 060 | Loss : 0.3052 | Acc : 0.8620
Epoch 00055 | Train Loss : 0.3284 | Eval Loss : 0.3362 | Train acc : 0.8487 | Eval Acc : 0.8421 | Eval Log. Respected : 0.9247
     Batch 000 | Loss : 0.3607 | Acc : 0.8310
     Batch 005 | Loss : 0.3445 | Acc : 0.8443
     Batch 010 | Loss : 0.3540 | Acc : 0.8325
     Batch 015 | Loss : 0.3628 | Acc : 0.8322
     Batch 020 | Loss : 0.2983 | Acc : 0.8618
     Batch 025 | Loss : 0.3015 | Acc : 0.8651
     Batch 030 | Loss : 0.3305 | Acc : 0.8476
     Batch 035 | Loss : 0.3409 | Acc : 0.8397
     Batch 040 | Loss : 0.3624 | Acc : 0.8298
     Batch 045 | Loss : 0.3074 | Acc : 0.8601
     Batch 050 | Loss : 0.3388 | Acc : 0.8407
     Batch 055 | Loss : 0.3171 | Acc : 0.8533
     Batch 060 | Loss : 0.2944 | Acc : 0.8650
Epoch 00056 | Train Loss : 0.3286 | Eval Loss : 0.3379 | Train acc : 0.8486 | Eval Acc : 0.8416 | Eval Log. Respected : 0.9379
     Batch 000 | Loss : 0.3225 | Acc : 0.8537
     Batch 005 | Loss : 0.3156 | Acc : 0.8553
     Batch 010 | Loss : 0.3055 | Acc : 0.8599
     Batch 015 | Loss : 0.3152 | Acc : 0.8552
     Batch 020 | Loss : 0.3283 | Acc : 0.8502
     Batch 025 | Loss : 0.3181 | Acc : 0.8551
     Batch 030 | Loss : 0.3596 | Acc : 0.8327
     Batch 035 | Loss : 0.3418 | Acc : 0.8457
     Batch 040 | Loss : 0.3246 | Acc : 0.8488
     Batch 045 | Loss : 0.3295 | Acc : 0.8493
     Batch 050 | Loss : 0.3257 | Acc : 0.8517
     Batch 055 | Loss : 0.3326 | Acc : 0.8461
     Batch 060 | Loss : 0.3104 | Acc : 0.8571
Epoch 00057 | Train Loss : 0.3278 | Eval Loss : 0.3380 | Train acc : 0.8489 | Eval Acc : 0.8411 | Eval Log. Respected : 0.9246
     Batch 000 | Loss : 0.3465 | Acc : 0.8392
     Batch 005 | Loss : 0.3381 | Acc : 0.8425
     Batch 010 | Loss : 0.3179 | Acc : 0.8550
     Batch 015 | Loss : 0.3472 | Acc : 0.8393
     Batch 020 | Loss : 0.3409 | Acc : 0.8430
     Batch 025 | Loss : 0.3400 | Acc : 0.8421
     Batch 030 | Loss : 0.3475 | Acc : 0.8373
     Batch 035 | Loss : 0.2939 | Acc : 0.8670
     Batch 040 | Loss : 0.3213 | Acc : 0.8524
     Batch 045 | Loss : 0.3343 | Acc : 0.8438
     Batch 050 | Loss : 0.3221 | Acc : 0.8511
     Batch 055 | Loss : 0.3214 | Acc : 0.8527
     Batch 060 | Loss : 0.2990 | Acc : 0.8635
Epoch 00058 | Train Loss : 0.3275 | Eval Loss : 0.3381 | Train acc : 0.8490 | Eval Acc : 0.8419 | Eval Log. Respected : 0.9291
Early Stopping
Testing...
Test Loss 0.5854 | Test Acc 0.8467 | Test Log. Res. 0.9284
